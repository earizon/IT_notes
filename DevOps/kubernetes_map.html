<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>Kubernetes map(beta)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>

<div groupv>
<pre zoom>
<span xsmall>Summary</span>
- k8s orchestates pools of CPUs, storage and networks
- *Kubernet Master* is responsible for managing the cluster, coordinates
   all activities in your cluster, such as scheduling applications,
   maintaining applications' desired state, scaling applications, and rolling
   out new updates
- orchestation offers:
  {
  - Load balancing a set of containers with or without session affinity
  - Mounting persistent storage inside of the containers
  - Placement and scheduling of containers on the infrastructure
  - Rolling deployments and operational considerations that traditional
      developers are not experts on
  }

- A *node* is a (VM) computer serving as a worker machine. Each node has a Kubelet agent plus tools like Docker or rkt.
- A cluster handling *production traffic* should have a *minimum of three nodes*(3,5,..."odd" number) 
  since <a href="https://raft.github.io/">Raft("voting")</a> consensus is used.
</pre>
<pre zoom>
<span xsmall>External Links</span>
- <a href="https://kubernetes.io/">Kubernetes</a>
- <a href="https://kubernetes.io/docs/reference/">Source Generated Reference</a>
  API, API Client, CLI, Config,  Design Docs
- <a xsmall href="https://www.youtube.com/watch?v=PH-2FfFD2PU">Desired State management"</a>
- <a xsmall href="https://kubernetes.io/docs/reference/glossary/">Standardized Glossary</a>
- <a xsmall href="https://kubernetes.io/docs/reference/tools/">Tools</a>
- <a href="https://kubectl.docs.kubernetes.io/">Book</a>
- Most voted questions k8s on serverfault:
  <a href="https://serverfault.com/questions/tagged/kubernetes?sort=votes">[k8s]@severfault</a>
- Most voted questions k8s on stackoverflow:
- <a href="https://stackoverflow.com/questions/tagged/kubernetes?sort=votes">[k8s]@stackoverflow</a>
- <a href="https://github.com/kubernetes/examples">Real app Examples</a>
- <a TODO href="https://www.nomadproject.io/intro/vs/kubernetes.html">k8s vs Nomad</a>

*k8s Issues:*
  - <a href="https://kubernetes.io/docs/reference/issues-security/issues/">Kubernetes Issue Tracker</a>
  - <a href="https://kubernetes.io/docs/reference/issues-security/security/">Kubernetes Security and Disclosure Information</a>

*k8s Enhancements*
  - <a href="https://github.com/kubernetes/enhancements">Enhancement Tracking and Backlog</a>
</pre>

<pre zoom>
<span xsmall>Online training</span>
@[https://www.katacoda.com/courses/kubernetes/playground]
@[https://labs.play-with-k8s.com/]
</pre>

<pre zoom>
<span xsmall>cluster components</span>
ref:@[https://kubernetes.io/docs/concepts/overview/components/]
    @[https://kubernetes.io/docs/reference/#config-reference]

MASTER/S COMPONENTS:

    |*Etcd*                          |*(REST)API Server:*
    |- distributed key-value store   | - cluster main management entry point.
    |- used for Service Registration | - Implemented by the kube-apiserver
    |  and discovery                 |   component running on the master/s
    |- See ref1                      | - Offers APIs to configure:
    |                                |   - (many of)K8s workloads
    |                                |   - organizational units
    |                                | - validates and configures data for API objects

    |*Controller Manager Service*                |*Scheduler Service*                      | *cloud-controller-manager*
    | -*handles a number of controllers that:*   | - Assigns workloads to nodes            | - Kubernetes 1.6R*alpha feature*
    |   - regulate the state of the cluster      | - tracks resource utilization on hosts  | - runs controllers that interact
    |   - and perform routine tasks              | - tracks total resources available on   |   with the underlying cloud providers.
    |     - ensures number of replicas for       |   each server as well as the resources  | - affected controllers
    |       a service,                           |   allocated to existing workloads       |   - Node Controller
    |     - ...                                  |   assigned on each server.              |   - Route Controller
    | - Implemented by*kube-controller-manager*  | - Manages availability, performance and |   - Service Controller
    | - embeds k8s core control loops                capacity                              |   - Volume Controller


    |*federation-apiserveR*                  |*federation-controller-manager*
    |- API server for federated clusters.    |- embeds the core control loops shipped
                                             |  with Kubernetes federation.

*NODE/S COMPONENTS:*

    |*kubelet*                             |*Container Runtime Interface (CRI)*      |*kube-proxy*
    |- run on each node                    |- Docker, ...                            |- network service abstraction
    |- takes a set of PodSpecs and ensures |                                         |  enabling SDNs
    |  thate described containers are      |                                         |- Can do simple TCP/UDP stream forwarding 
    |  running and healthy.                |                                         |  or round-robin TCP/UDP forwarding across
                                           |                                         |  a set of back-ends.


ref1:
 @[https://kubernetes.io/docs/Architecture/architecture_map.html?query=service+registration+discovery]

- Nodes are created before k8s is running (hardware, virt.machine,...)
- A k8s node object just represents an existing node. k8s will remove
  the object if no pods can be instantiated.  
         ┌───────────┐
         │Node Status│
         ├───────────┤
         │Addresses  ←─(HostName, ExternalIP, InternalIP)
 ┌───────→Capacity   │                        ^^^^^^^^^^
 │ ┌─────→Condition  │                        Typically visible
 │ │ ┌───→Info       │                        within cluster
 │ │ │   └───────────┘
 │ │*Info*
 │ │ General information like
 │ │ kernel/k8s/docker version, OS name,...
 │ │  
 │*Condition*
 │  status of*all*Running nodes.
 │  Node               Description
 │  OutOfDisk          True → insufficient free space on the node for adding new pods
 │  Ready              True → node is healthy and ready to accept pods
 │                     if not True after *pod─eviction─timeout* (5 minutes by default)
 │                     an argument is passed to the kube─controller─manager and all Pods
 │  
 │  MemoryPressure     True → pressure exists on the node memory
 │  PIDPressure        True → pressure exists on the processes
 │  DiskPressure       True → pressure exists on the disk size
 │  NetworkUnavailable True → node network not correctly configured
 │ 
*Capacity*
─ Describes the resources available
  ─ CPU
  ─ memory 
  ─ max.number of pods supported
</pre>

<pre zoom bgorange>
<span xsmall>Network</span>
Network schema without Cloud-Controller:
REF:@[https://kubernetes.io/docs/concepts/architecture/cloud-controller/]

  o Cloud connector                             _                 _
                                            ___| | ___  _   _  __| |
             kube─controller─manager  o──→ / __| |/ _ \| | | |/ _` |
                        ^                 | (__| | (_) | |_| | (_| |
                        │                  \___|_|\___/ \__,_|\__,_|
                        │                           ^
                        │                           │
                        │                           │
                        v                           │  node
  etcd  ←───────→ kube─apiserver ←─────┬───┐    ┌───o──────┐
                                       │   └────→kubelet   │
                        ^              │        │          │
                        │              │        │          │
                        v              │        │          │
                  kube─scheduler       └────────→kube─proxy│
                                                └──────────┘


Network schema*with*   Cloud-Controller:

  o Cloud connector                             _                 _
                                            ___| | ___  _   _  __| |
            cloud-controller─manager  o──→ / __| |/ _ \| | | |/ _` |
                        ^                 | (__| | (_) | |_| | (_| |
kube─controller─manager │                  \___|_|\___/ \__,_|\__,_|
      ^                 │
      │                 │
      └─────────────────┤
                        │
                        v                              node
  etcd  ←───────→ kube─apiserver ←─────┬───┐    ┌──────────┐
                                       │   └────→kubelet   │
                        ^              │        │          │
                        │              │        │          │
                        v              │        │          │
                  kube─scheduler       └────────→kube─proxy│
                                                └──────────┘

</pre>


<pre zoom>
<span xsmall  bgorange>kubectl: Object Management</span>
- INSTALL   : @[https://kubernetes.io/docs/tasks/tools/install-kubectl/]
- Overview  : @[https://kubernetes.io/docs/concepts/overview/object-management-kubectl/overview/]
        _                _       _               _   
    ___| |__   ___  __ _| |_ ___| |__   ___  ___| |_ 
   / __| '_ \ / _ \/ _` | __/ __| '_ \ / _ \/ _ \ __|
  | (__| | | |  __/ (_| | |_\__ \ | | |  __/  __/ |_ 
   \___|_| |_|\___|\__,_|\__|___/_| |_|\___|\___|\__|
@[https://kubernetes.io/docs/reference/kubectl/cheatsheet/]*
 
*┌──────────┐*                                ┌───────────┐
*│k8s object│*                             ┌─→│metadata   │
*├──────────┤*                             │  ├───────────┤
*│kind      │* one of the G*Resource types*│  │name       │← maps to /api/v1/pods/name
*│metadata  │←─────────────────────────────┘  │UID        │← Distinguish between historical
*│spec      │← desired state*                 │namespace  │  occurrences of similar object
*│state     │← present state*                 │labels     │
*└──────────┘*                                │annotations│
                                              └───────────┘
                                         metadata is organized 
                                         around the concept of an     
                                         application. k8s does NOT    
                                         enforce a formal notion of   
                                         application. Apps are        
                                         described thorugh metadata   
                                         in a loose definition.             


etcd                 kubernetes                (/openapi/v2)       cli management:
(serialized  ←────── objects    ←────────────────→ API    ←───→  $*kubectl*'action'      G*'resource'*
 API resource    - Represent API resources        Server                    ^^^^^^
 states)           (persistent entities           (ref1)         *get     *: list resources 
                    in a cluster)                                *describe*: show details (and events for pods)
                 - The can describe:                             *logs    *: print container
                   - apps running on nodes                                   logs
                   - resources available to                      *exec    *: exec command on
                     a given app                                             container
                   - policies around apps                        *apply   *: creates and updates resources
                     (restart, upgrades, ...)                     ...
                                                                  common kubectl flags:
                                                                  --all-namespaces
                                                                  -o wide
                                                                  --include-uninitialized
                                                                  --sort-by=.metadata.name
                                                                  --sort-by='.status.containerStatuses[0].restartCount'
                                                                  --selector=app=cassandra



G*RESOURCE TYPES*
    clusters            │podtemplates               │statefulsets
(cs)componentstatuses   │(rs)replicasets            │(pvc)persistentvolumeclaims
(cm)configmaps          │(rc)replicationcontrollers │(pv) persistentvolumes
(ds)daemonsets          │(quota)resourcequotas      │(po) pods
(deploy)deployments     │cronjob                    │(psp)podsecuritypolicies
(ep)endpoints           │jobs                       │secrets
(ev)event               │(limits)limitranges        │(sa)serviceaccount
(hpa)horizon...oscalers │(ns)namespaces             │(svc)services
(ing)ingresses          │networkpolicies            │storageclasses
                        │(no)nodes                  │thirdpartyresources

<a xsmall href="https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/types.go">Full list APItypes</a> 

ref1: @[https://kubernetes.io/docs/conceptsverview/kubernetes-api/]
- External refs:
  - <a href="https://kubernetes.io/docs/reference/kubectl/jsonpath/">JSONPath Support</a>
  - <a href="https://kubernetes.io/docs/reference/kubectl/overview/">Overview of kubectl</a>
  - <a href="https://kubernetes.io/docs/reference/kubectl/kubectl/">kubectl</a>
  - <a href="https://kubernetes.io/docs/reference/kubectl/kubectl-cmds/">kubectl Commands</a>
  - <a href="https://kubernetes.io/docs/reference/kubectl/conventions/">kubectl Usage Conventions</a>
  - <a href="https://kubernetes.io/docs/reference/kubectl/docker-cli-to-kubectl/">kubectl for Docker Users</a>
</pre>

<pre zoom>
<span xsmall>Kubectl Autocomplete</span>
$ source ˂(kubectl completion bash)
$ source ˂(kubectl completion zsh)
# use multiple kubeconfig files
$ KUBECONFIG=~/.kube/config:~/.kube/kubconfig2 \
kubectl config view # Show Merged kubeconfig settings.

$ kubectl config current-context
$ kubectl config use-context my-cluster-name

$ kubectl run nginx --image=nginx
$ kubectl explain pods,svc
</pre>

<pre zoom >
<span xsmall>editing resources</span>
$ KUBE_EDITOR="nano" kubectl edit svc/my-service-1
</pre>

<pre TODO zoom>
<span xsmall>Organizing Cluster Access</span>
<span xsmall> Using kubeconfig Files</span>
@[https://kubernetes.io/docs/concepts/configuration/organize-cluster-access-kubeconfig/]
</pre>

<pre zoom>
<span xsmall>Core API types</span>
@[https://github.com/kubernetes/kubernetes/blob/master/pkg/apis/core/types.go]

001	Volume                               021	FlexPersistentVolumeSource           041	DownwardAPIProjection
002	VolumeSource                         022	FlexVolumeSource                     042	AzureFileVolumeSource
003	PersistentVolumeSource               023	AWSElasticBlockStoreVolumeSource     043	AzureFilePersistentVolumeSource
004	PersistentVolumeClaimVolumeSource    024	GitRepoVolumeSource                  044	VsphereVirtualDiskVolumeSource
005	PersistentVolume                     025	SecretVolumeSource                   045	PhotonPersistentDiskVolumeSource
006	PersistentVolumeSpec                 026	SecretProjection                     046	PortworxVolumeSource
007	VolumeNodeAffinity                   027	NFSVolumeSource                      047	AzureDiskVolumeSource
008	PersistentVolumeStatus               028	QuobyteVolumeSource                  048	ScaleIOVolumeSource
009	PersistentVolumeList                 029	GlusterfsVolumeSource                049	ScaleIOPersistentVolumeSource
010	PersistentVolumeClaim                030	GlusterfsPersistentVolumeSource      050	StorageOSVolumeSource
011	PersistentVolumeClaimList            031	RBDVolumeSource                      051	StorageOSPersistentVolumeSource
012	PersistentVolumeClaimSpec            032	RBDPersistentVolumeSource            052	ConfigMapVolumeSource
013	PersistentVolumeClaimCondition       033	CinderVolumeSource                   053	ConfigMapProjection
014	PersistentVolumeClaimStatus          034	CinderPersistentVolumeSource         054	ServiceAccountTokenProjection
015	HostPathVolumeSource                 035	CephFSVolumeSource                   055	ProjectedVolumeSource
016	EmptyDirVolumeSource                 036	SecretReference                      056	VolumeProjection
017	GCEPersistentDiskVolumeSource        037	CephFSPersistentVolumeSource         057	KeyToPath
018	ISCSIVolumeSource                    038	FlockerVolumeSource                  058	LocalVolumeSource
019	ISCSIPersistentVolumeSource          039	DownwardAPIVolumeSource              059	CSIPersistentVolumeSource
020	FCVolumeSource                       040	DownwardAPIVolumeFile                060	CSIVolumeSource


061	ContainerPort         081	Handler                           101	PreferredSchedulingTerm
062	VolumeMount           082	Lifecycle                         102	Taint
063	VolumeDevice          083	ContainerStateWaiting             103	Toleration
064	EnvVar                084	ContainerStateRunning             104	PodReadinessGate
065	EnvVarSource          085	ContainerStateTerminated          105	PodSpec
066	ObjectFieldSelector   086	ContainerState                    106	HostAlias
067	ResourceFieldSelector 087	ContainerStatus                   107	Sysctl
068	ConfigMapKeySelector  088	PodCondition                      108	PodSecurityContext
069	SecretKeySelector     089	PodList                           109	PodDNSConfig
070	EnvFromSource         090	NodeSelector                      110	PodDNSConfigOption
071	ConfigMapEnvSource    091	NodeSelectorTerm                  111	PodStatus
072	SecretEnvSource       092	NodeSelectorRequirement           112	PodStatusResult
073	HTTPHeader            093	TopologySelectorTerm              113	Pod
074	HTTPGetAction         094	TopologySelectorLabelRequirement  114	PodTemplateSpec
075	TCPSocketAction       095	Affinity                          115	PodTemplate
076	ExecAction            096	PodAffinity                       116	PodTemplateList
077	Probe                 097	PodAntiAffinity                   117	ReplicationControllerSpec
078	Capabilities          098	WeightedPodAffinityTerm           118	ReplicationControllerStatus
079	ResourceRequirements  099	PodAffinityTerm                   119	ReplicationControllerCondition
080	Container             100	NodeAffinity                      120	ReplicationController


121	ReplicationControllerList 141	DaemonEndpoint       161	Preconditions             181	ResourceQuotaSpec
122	ServiceList               142	NodeDaemonEndpoints  162	PodLogOptions             182	ScopeSelector
123	SessionAffinityConfig     143	NodeSystemInfo       163	PodAttachOptions          183	ScopedResourceSelerRequirement
124	ClientIPConfig            144	NodeConfigStatus     164	PodExecOptions            184	ResourceQuotaStatus
125	ServiceStatus             145	NodeStatus           165	PodPortForwardOptions     185	ResourceQuota
126	LoadBalancerStatus        146	AttachedVolume       166	PodProxyOptions           186	ResourceQuotaList
127	LoadBalancerIngress       147	AvoidPods            167	NodeProxyOptions          187	Secret
128	ServiceSpec               148	PreferAvoidPodsEntry 168	ServiceProxyOptions       188	SecretList
129	ServicePort               149	PodSignature         169	ObjectReference           189	ConfigMap
130	Service                   150	ContainerImage       170	LocalObjectReference      190	ConfigMapList
131	ServiceAccount            151	NodeCondition        171	TypedLocalObjectReference 191	ComponentCondition
132	ServiceAccountList        152	NodeAddress          172	SerializedReference       192	ComponentStatus
133	Endpoints                 153	NodeResources        173	EventSource               193	ComponentStatusList
134	EndpointSubset            154	Node                 174	Event                     194	SecurityContext
135	EndpointAddress           155	NodeList             175	EventSeries               195	SELinuxOptions
136	EndpointPort              156	NamespaceSpec        176	EventList                 196	WindowsSecurityContextOptions
137	EndpointsList             157	NamespaceStatus      177	LimitRangeItem            197	RangeAllocation
138	NodeSpec                  158	Namespace            178	LimitRangeSpec
139	NodeConfigSource          159	NamespaceList        179	LimitRange
140	ConfigMapNodeConfigSource 160	Binding              180	LimitRangeList
</pre>

<pre zoom>
<span xsmall>kubectrl Field Selectors</span>
- filter k8s objects based on the value of one or more object fields.
- the possible field depends on each object type/kind but 
  all resource-types support 'metadata.name' and 'metadata.namespace'
  Ex:
  $ kubectl get pods --field-selector       metadata.name==my-service,spec.restartPolicy=Always
  $ kubectl get pods --field-selector  metadata.namespace!=default,status.phase==Running
</pre>

<pre zoom>
<span TODO xsmall>Kustomization</span>
<a TODO href="https://kubernetes.io/docs/tasks/manage-kubernetes-objects/kustomization/">Object kustomization</a>
</pre>
</div>

<div groupv>
<span title>Building Blocks</span>

<pre zoom>
<span xsmall>Namespace</span>
@[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/]
@[https://kubernetes.io/docs/tasks/administer-cluster/namespaces/]
- Kubernetes supports multiple virtual clusters backed by the same physical
    cluster. These virtual clusters are called namespaces
- Provide a scope for names
- Namespaces are intended for use in environments with many users spread
  across multiple teams, or projects. For clusters with a few to tens of users,
  you should not need to create or think about namespaces at all. Start
  using namespaces when you need the features they provide.
- use labels, not namespaces, to distinguish resources within the same namespace
- Services are created with DNS entry
  "service-name"."namespace".svc.cluster.local

$ kubectl*get namespaces*
NAME          STATUS    AGE
default       Active    1d
kube-system   Active    1d

$ kubectl*create namespace*my-namespace // ← create namespace:

$ kubectl*--namespace=my-namespace* \ ← set NS for request 
    run nginx --image=nginx
$ kubectl*config set-context*\           ← permanently save the NS
    $(kubectl *config current-context*) \ ← shell syntax sugar: Exec command and take output
    --namespace=MyFavouriteNS                                   as input for next command
$ kubectl config view | \
    grep namespace: # Validate

<span TODO xsmall>Manage Mem./CPU/API Resrc.</span>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/">Configure Default Memory Requests and Limits for a Namespace</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/">Configure Default CPU Requests and Limits for a Namespace</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-constraint-namespace/">Configure Minimum and Maximum Memory Constraints for a Namespace</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-constraint-namespace/">Configure Minimum and Maximum CPU Constraints for a Namespace</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-memory-cpu-namespace/">Configure Memory and CPU Quotas for a Namespace</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/quota-pod-namespace/">Configure a Pod Quota for a Namespace</a>


<a href=https://kubernetes.io/docs/tasks/administer-cluster/namespaces-walkthrough/">Namespaces Walkthrough</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/namespaces/">Share a Cluster with Namespaces</a>
</pre>

<pre zoom>
<span xsmall>Secrets</span>
@[https://kubernetes.io/docs/concepts/configuration/secret/]

See also secret design:
@[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/auth/secrets.md]

- Used to store passwords, OAuth tokens, ssh keys, ...
  the Secrets k8s API object allows for more control over
  how they are used, reducing the risk of accidental exposure.

- Users (and system) create secrets
- Pods reference the secrets in three ways:
  - as files in a mounted volume
  - as ENVIRONMENT variables.
  - used by kubelet when pulling images for the pod

*Built-in Secrets*
- Service Accounts automatically create and attach secrets with API Credentials
- Kubernetes automatically creates secrets which contain credentials for 
  accessing the API and it automatically modifies your pods to use this type 
  of secret.

*Creating User Secrets*
│       ALTERNATIVE 1                           │ ALTERNATIVE 2                                    │ALTERNATIVE 3 
│ *STEP 1*: Create un-encrypted secrets locally:│*STEP 1:*Create Secret with data                  │*STEP 1*: Create Secret with stringData
│   $ echo -n 'admin'        ˃ ./username.txt   │   cat ˂˂ EOF ˃ secret.yaml                       │apiVersion: v1
│   $ echo -n '1f2d1e2e67df' ˃ ./password.txt   │   apiVersion: v1                                 │kind:*Secret*
│              ^^^^^^^^^^^^                     │   kind:*Secret*                                  │metadata:
│              special chars. must be           │   metadata:                                      │  name: mysecret
│              '\' escaped.                     │     name: mysecret                               │type: Opaque
│                                               │   type: Opaque                                   │stringData:
│                                               │   data:                                          │  config.yaml: |-
│                                               │     username: $(echo -n 'admin'        | base64) │    apiUrl: "https://my.api.com/api/v1"
│                                               │     password: $(echo -n '1f2d1e2e67df' | base64) │    username: admin
│                                               │   EOF                                            │    password: 1f2d1e2e67df
│                                               │                                                  │ 
│ *STEP 2:* Package into a Secret k8s object    │*STEP 2:* Apply                                   │*STEP 2:*Apply                   
│   $ kubectl*create secret*\                   │  $ kubectl apply -f ./secret.yaml                │  $ kubectl apply -f ./secret.yaml
│     genericG*db-user-pass* \                  │                                                  │                        
│     --from-file=./username.txt \              │                                                  │  
│     --from-file=./password.txt                │                                                  │                     

│*STEP 3:* Check that Secret object has been ┌────────────────────────────────────────────
│  properly created.                         │ *Using the Secrets in a Pod*
│  $ kubectl get secrets                     │                                   Control secret path with items:         Consume as ENV.VARS
│  → NAME           TYPE    DATA  AGE        │ apiVersion: v1                  │ apiVersion: v1                        | apiVersion: v1
│  → db-user-pass   Opaque  2     51s        │ kind:*Pod*                      │ kind:*Pod*                            | kind: Pod
│                                            │ metadata:                       │ metadata:                             | metadata:
│  $ kubectl describe secrets/db-user-pass   │   name: mypod                   │   name: mypod                         |   name: secret-env-pod
│  → Name:          G*db-user-pass*          │ spec:                           │ spec:                                 | spec:
│  → Namespace:       default                │   containers:                   │   containers:                         |  containers:
│  → Labels:          <none>                 │   ─ name: mypod                 │   - name: mypod                       |  - name: mycontainer
│  → Annotations:     <none>                 │     image: redis                │     image: redis                      |   image: redis
│  →                                         │     volumeMounts:               │     volumeMounts:                     |   env:
│  → Type:            Opaque                 │     ─ name:*foO*                │     - name:*foo*                      |    - name: SECRET_USERNAME
│  →                                         │       mountPath: "/etc/foo"     │       mountPath: "/etc/foo"           |     valueFrom:
│  → Data                                    │       readOnly:*true*           │       readOnly: true                  |      secretKeyRef:
│  → ====                                    │   volumes:                      │   volumes:                            |       name: G*db─user─pass*
│  →*password.txt:*   12 bytes               │   ─ name:*foO*                  │   - name:*foo*                        |       key: username
│  →*username.txt:*   5 bytes                │     secret:                     │     secret:                           |    - name: SECRET_PASSWORD
                                             │       secretName:G*db─user─pass*│       secretName:G*db─user─pass*      |     valueFrom:
                                             │       defaultMode: 256          │       items:                          |      secretKeyRef:
                                             │                     ^           │       - key: username                 |       name: G*db─user─pass*
                                             │                     |           │         path: my-group/my-username    |       key: password
                                                                   |                           ^^^^^^^^^^^^^^^^^^^^
                                                            JSON does NOT support     · username will be seen in container as:
                                                            octal notation.             /etc/foo/my-group/my-username
                                                                     256 = 0400       · password secret is not projected
     




Ex 2:
SECRET CREATION                                      | SECRET USSAGE
$ kubectl create secret generic \                    | apiVersion: v1
G*ssh-key-secret* \                                  | kind: Pod
  --from-file=ssh-privatekey=/path/to/.ssh/id_rsa \  | metadata:
  --from-file=ssh-publickey=/path/to/.ssh/id_rsa.pub |   name: secret-test-pod
                                                     |   labels:
                                                     |     name: secret-test
                                                     | spec:
                                                     |   volumes:
                                                     |   - name: secret-volume
                                                     |     secret:
                                                     |       secretName:G*ssh-key-secret*
                                                     |   containers:
                                                     |   - name: ssh-test-container
                                                     |     image: mySshImage
                                                     |     volumeMounts:
                                                     |     - name: secret-volume
                                                     |       readOnly: true
                                                     |       mountPath: "/etc/secret-volume"
                                                     |                  ^^^^^^^^^^^^^^^^^^^^
                                                     |          secret files available visible like:
                                                     |          /etc/secret-volume/ssh-publickey
                                                     |          /etc/secret-volume/ssh-privatekey


FEATURE STATE: Kubernetes v1.13 beta
You can enable encryption at rest for secret data, so that the secrets are not stored in the clear into etcd .
</pre>
<pre zoom>
<span xsmall>(Pod)ConfigMap</span>
@[https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-configmap/]

ConfigMaps: decouple configuration artifacts from image content

*Create a ConfigMap*


$ kubectl create configmap \
   'map-name' \
   'data-source'   ← data-source: directories, files or literal values
                                  translates to the key-value pair in the ConfigMap where
                                  key   = file_name or key               provided on the cli
                                  value = file_contents or literal_value provided on the cli

You can use kubectl describe or kubectl get to retrieve information about a ConfigMap.

ConfigMap*from config-file*:
┌───────────────────────────── ┌──────────────────────────────────────
│*STEP 0:* Input to config map │*STEP 1:* create ConfigMap object
│ ...configure─pod─container/  │ $ kubectl create configmap \             
│    └─ configmap/             │   game-config                            
│       ├─*game.properties*    │   --from-file=configmap/                 
│       └─*  ui.properties*    │               ^^^^^^^^^^                 
│                              │               combines the contents of   
│                              │               all files in the directory 
                                                                         
┌───────────────────────────────────────────────────────────────────────────────────────────────────────────
│*STEP 2:* check STEP 1
│$ kubectl*get configmaps*game-config -o yaml                     │$ kubectl*describe configmaps*game-config
│→ apiVersion: v1                                                 │→ 
│→ kind: ConfigMap                                                │→ Name:           game-config
│→ metadata:                                                      │→ Namespace:      default
│→   creationTimestamp: 2016-02-18T18:52:05Z                      │→ Labels:         <none>
│→   name: game-config                                            │→ Annotations:    <none>
│→   namespace: default                                           │→ 
│→   resourceVersion: "516"                                       │→ Data
│→   selfLink: /api/v1/namespaces/default/configmaps/game-config  │→ ====
│→   uid: b4952dc3-d670-11e5-8cd0-68f728db1985                    │→*game.properties:* 158 bytes
│→ data:                                                          │→*ui.properties:  *  83 bytes
│→  *game.properties:*|
│→     enemies=aliens
│→     lives=3                                  ┌───────────────────────────────────
│→     enemies.cheat=true                       │*STEP 3:*Use in Pod container*
│→     enemies.cheat.level=noGoodRotten         │ apiVersion: v1
│→     secret.code.passphrase=UUDDLRLRBABAS     │ kind: Pod
│→     secret.code.allowed=true                 │ metadata:
│→     secret.code.lives=30                     │   name: dapi-test-pod
│→  *ui.properties:*|                           │ spec:
│→     color.good=purple                        │  containers:
│→     color.bad=yellow                         │   - name: test-container
│→     allow.textmode=true                      │     ...
│→     how.nice.to.look=fairlyNice              │     env:
                                                │     *- name: ENEMIES_CHEAT     *
                                                │     *  valueFrom:              *
                                                │     *    configMapKeyRef:      *
                                                │     *      name: game-config   *
                                                │     *      key: enemies.cheat  *

 
ConfigMap*from env-file*:
│*STEP 0:* Input to config map       │*STEP 1:* create ConfigMap object
│ ...configure─pod─container/        │ $ kubectl create configmap \             
│    └─ configmap/                   │   game-config-env-file \
│       └─*game-env-file.properties* │   --from-env-file=game-env-file.properties
│          ^^^^^^^^^^^^^^^^^^^^^^^^
│          game-env-file.properties
│          enemies=aliens            │ *STEP 2:* Check STEP 1
│          lives=3                   │ $ kubectl get configmap game-config-env-file -o yaml
│          allowed="true"            │ → apiVersion: v1
                                     │ → kind: ConfigMap
                                     │ → metadata:
                                     │ →   creationTimestamp: 2017-12-27T18:36:28Z
                                     │ →   name: game-config-env-file
                                     │ →   namespace: default
                                     │ →   resourceVersion: "809965"
                                     │ →   selfLink: /api/v1/namespaces/default/configmaps/game-config-env-file
                                     │ →   uid: d9d1ca5b-eb34-11e7-887b-42010a8002b8
                                     │ → data:
                                     │ →   allowed: '"true"'
                                     │ →   enemies: aliens
                                     │ →   lives: "3"

NOTE: kubectl create configmap ... --from-file=B*'my-key-name'*='path-to-file'
      will create the data under:
      → ....
      → data:
      → B*my-key-name:*
      →     key1: value1
      →     ...

*ConfigMaps from literal values*
* STEP 1*                                     | * STEP 2*
$ kubectl create configmap special-config \   | → ...
   --from-literal=special.how=very \          | → data:
   --from-literal=special.type=charm          | →   special.how: very
                                              | →   special.type: charm

  restartPolicy: Never

</pre>

<span title>Containers</span>
<pre zoom>
<span xsmall>Containers</span>
Container: (lightweight and portable) executable image that contains software and
           all of its dependencies.
NOTE: 1 Pod = one or more*running*containers.

*Lifecycle Hooks*
 - PostStart Hook
 - PreStop   Hook

*type Container struct {*
    Name string
    Image string
    Command []string        
    Args []string           
    WorkingDir string       
    Ports []ContainerPort   
    EnvFrom []EnvFromSource 
    Env []EnvVar            
    Resources ResourceRequirements
    VolumeMounts []VolumeMount
    VolumeDevices []VolumeDevice
    LivenessProbe *Probe         ← kubelet exec the probes
    ReadinessProbe *Probe          (ExecAction, TCPSocketAction, HTTPGetAction)
    Lifecycle *Lifecycle
    TerminationMessagePath string
    TerminationMessagePolicy TerminationMessagePolicy
    ImagePullPolicy PullPolicy
    SecurityContext *SecurityContext
    Stdin bool
 }
</pre>

<pre zoom>
<span xsmall>Init Containers</span>
@[https://kubernetes.io/docs/concepts/workloads/pods/init-containers/]

- one or more specialized Containers that run before app Containers 
-*can contain utilities or setup scripts not present in an app image*
- exactly equal to regular Containers, except:
  - They always run to completion.
    k8s will restart it repeatedly until succeeds
    (unless restartPolicy == Never)
  - Each one must complete successfully before the next one is started.
  - status is returned in .status.initContainerStatuses
                                  ^^^^^^^^^^^^^^^^^^^^^
                       vs .status.containerStatuses 
  - readiness probes do not apply

Note: Init Containers can also be given access to Secrets that app Containers 
      are not able to access.

- Ex: Wait for 'myService' and then for 'mydb'.
  │ apiVersion: v1                  │apiVersion: v1        │apiVersion: v1
  │ kind:*Pod*                      │kind:*Service*        │kind:*Service*
  │ metadata:                       │metadata:             │metadata:
  │   name: myapp─pod               │  name:G*myservice*   │  name:B*mydb*
  │   labels:                       │spec:                 │spec:
  │     app: myapp                  │  ports:              │  ports:
  │ spec:                           │  ─ protocol: TCP     │  ─ protocol: TCP
  │   containers:                   │    port: 80          │    port: 80
  │   ─ name: myapp─container       │    targetPort: 9376  │    targetPort: 9377
  │     image: busybox:1.28         └────────────────────  └─────────────────────
  │     command: ['sh', '─c', 'echo The app is running! ⅋⅋ sleep 3600']
  │  *initContainers:*
  │   ─ name:O*init─myservice*
  │     image: busybox:1.28
  │     command: ['sh', '─c', 'until nslookup G*myservice*; sleep 2; done;']
  │   ─ name:O*init─mydb*
  │     image: busybox:1.28
  │     command: ['sh', '─c', 'until nslookup B*mydb*     ; sleep 2; done;']

  Inspect init Containers like:
  $ kubectl logs myapp-pod -c O*init-myservice*
  $ kubectl logs myapp-pod -c O*init-mydb     *

R*WARN:*
- Use activeDeadlineSeconds on the Pod and livenessProbe on the Container
  to prevent Init Containers from failing forever. 
</pre>
<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-init-containers/">Debug Init Containers</a>
</pre>

<pre zoom>
<span xsmall>Containers Resources</span>
@[https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/]
- TODO: difference between requests and limits, see Resource QoS.
      @[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/node/resource-qos.md]

A Container:
-   *can exceed* its resource*request*if the Node has memory available, 
- is*not allowed*to use more than its resource *limit*.


- spec.containers[].resources.limits.cpu              ← can NOT exceed resource limit → it will be scheduled 
- spec.containers[].resources.limits.memory                                             for termination
- spec.containers[].resources.requests.cpu            ← can     exceed resource request 
- spec.containers[].resources.requests.memory
- spec.containers[].resources.limits.ephemeral-storage     k8s 1.8+
- spec.containers[].resources.requests.ephemeral-storage   k8s 1.8+

Note: resource quota feature can be configured to limit the total amount of resources
      that can be consumed. In conjunction namespaces, it can prevent one team from
      hogging all the resources.

ContaineR*Compute*Resource types:
- CPU    : units of cores
           - total amount of CPU time that a container can use every 100ms. 
             (minimum resolution can be setup to 1ms)
           - 1 cpu is equivalent to: 1 (AWS vCPU|GCP Core|Azure vCore,IBM vCPU,Hyperthread bare-metal )
           - 0.1 cpu == 100m
- memory : units of bytes:
           - Ex: 128974848, 129e6, 129M, 123Mi
- Local ephemeral storage(k8s 1.8+):
           - No QoS can be applied.
           - Ex: 128974848, 129e6, 129M, 123Mi

When using Docker:
 greater of this number or 2 is used as the value of the --cpu-shares flag in the docker run command.


*Extended resources*
- fully-qualified resource names outside the kubernetes.io domain.

Ussage:
- STEP 1: cluster operator must advertise an Extended Resource.
- STEP 2: users must request the Extended Resource in Pods.
(See official doc for more info)
 
It is planned to add new resource types, including a node disk space resource, and a framework for adding custom resource types.
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/">Get a Shell to a Running Container</a>
</pre>

<span title>Pods</span>
<pre zoom>
<span xsmall>Pod</span>
<span xsmall>basic execution</span>
<span xsmall>unit of k8s apps</span>
@[https://kubernetes.io/docs/concepts/workloads/pods/pod-overview/]
Pod: Minimal*schedulable-for-execution unit*in a k8s application (or k8s service)
 ┌───────┐            ┌─────┐                 ┌───────────┐
 │k8s app│1←───→(1..N)│pod/s│ 1 ←───→ (1,2,..)│container/s│
 └───────┘            └─────┘                 └───────────┘
           ^^^^^                ^^^^^ 
  ex: - 1 DDBB pod              Best practice:
      - 2 FrontEnd pods         only a single container or
      - 1 ESB pod               a few tightly-coupled ones.
      - ...

- one or more application containers (such as Docker or rkt) plus
  shared resources for those containers:
  - Volumes   : (Shared storage)
  - Networking: (Shared unique cluster IP)
  - Metadata  : (Container image version, ports to use, ...)

- Represent a k8s app. "logical host"

- Each Pod is tied to the Node where it is scheduled, and remains there 
  until termination (according to restart policy) or deletion.

- In case of a Node failure, identical Pods are scheduled on other 
  available Nodes in the cluster.

- The containers in a Pod are automatically co-located and co-scheduled
  on the same physical or virtual machine in the cluster.             

- Pods*do not, by themselves, self-heal*
  Using a k8s Controller is used for Pod scaling or healing (restart).

*Pod Preset*
@[https://kubernetes.io/docs/concepts/workloads/pods/podpreset/]
- objects used to inject information into pods at creation time
  like secrets, volumes, volume mounts, and environment variables.

- label selectors are used to specify Pods affected.

*Disable Pod Preset for a Specific Pod*
Add next annotation to the Pod Spec:
-  podpreset.admission.kubernetes.io/exclude: "true".


</pre>
<pre zoom>
<span xsmall>Pod Life Cicle</span>
 DEFINITION                                               END
  defined   → scheduled in node → run → alt1: run until their container(s) exit
                                        alt2: pod removed (for any other reason)
                                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                                  depending on policy and exit code,
                                                  may be removed after exiting, or
                                                  may be retained in order to enable
                                                  access containers logs.

@[https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/]
Pod.PodStatus.phase := Pending|Running|Succeeded|Failed
</pre>
<pre zoom>
<span xsmall>Pod QoS</span>
@[https://kubernetes.io/docs/tasks/configure-pod-container/quality-service-pod/]

Pod.spec.qosClass := 
Guaranteed   Assigned to pod when:
             - All Pod-Containers have a defined cpu/mem. limit.
             - All Pod-Containers have same cpu/mem.limit and same cpu/mem.request

Burstable    Assigned to pod when: 
             - criteria for QoS class Guaranteed is*NOT*met.
             - 1+ Pod-Containers have memory or CPU request.

BestEffort   Assigned to pod when:
             - Pod-Containers do NOT have any memory/CPU request/limit
</pre>

<pre zoom >
<span xsmall>Ex. Pod YAML Definition:</span>
apiVersion: v1
kind: Pod
metadata:
name: test-pd
spec:
containers:
  - image: gcr.io/google_containers/test-webserver
    name: test-container
    volumeMounts:
    - mountPath: /cache    ← where to mount
      name:B*cache-volume*  ← volume (name) to mount
    - mountPath: /test-pd
      name:G*test-volume*
  volumes:
  - name:B*cache-volume*
    emptyDir: {}
  - name:G*test-volume*
    hostPath:
      # directory location on host
      path: /data
      # this field is optional
      type: Directory
</pre>

<pre zoom>
<a xsmall TODO href="https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle/#restart-policy">RestartPolicy</a>
</pre>


<pre zoom>
<span xsmall>Mng. running pod</span>
<span xsmall>run,attach</span>
<span xsmall>exec,top</span>
<span xsmall>Mng. running pod:</span>
  $ kubectl logs my-pod (-c my-container) (-f)
                                           ^"tail -f"
  $ kubectl run -i --tty busybox --image=busybox -- sh
  $ kubectl attach my-pod -i
  $ kubectl port-forward my-pod 5000:6000  # Forward port 6000 of Pod → 5000 local machine
  $ kubectl exec my-pod (-c my-container) -- ls / # Run command in existing pod
  $ kubectl top pod POD_NAME --containers
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-pod-replication-controller/">Debug Pods and ReplicationControllers</a>
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/determine-reason-pod-failure/">Determine the Reason for Pod Failure</a>
</pre>

<pre zoom>
<span xsmall>SecurityContext</span>
@[https://kubernetes.io/docs/tasks/configure-pod-container/security-context/]
@[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/auth/security_context.md]
Pod.spec.securityContext Ex:
Ex:
apiVersion: v1                            Exec a terminal into a running container adnd execute:
kind: Pod                                 $ id
metadata:                                *uid=1000 gid=3000 groups=2000*
  name: security-context-demo                 ^^^^     ^^^^ 
spec:                                         uid/gid 0/0 if not specified
 *securityContext:  *
 *  runAsUser: 1000 *
 *  runAsGroup: 3000*
 *  fsGroup: 2000* ← owned/writable by this GID when supported by volume
  volumes:           
  - name: sec-ctx-vol   ← Volumes will be relabeled with provided seLinuxOptions values
    emptyDir: {}
  containers:
  - name: sec-ctx-demo
    ...
    volumeMounts:
    - name: sec-ctx-vol
      mountPath: /data/demo
   *securityContext:*
   *  allowPrivilegeEscalation: false*
   *  capabilities:*
   *    add: ["NET_ADMIN", "SYS_TIME"]* ← Provides a subset of 'root' capabilities:
                                        @[https://github.com/torvalds/linux/blob/master/include/uapi/linux/capability.h]
    * seLinuxOptions:*
    *   level: "s0:c123,c456"*

SecurityContext holds security configuration that will be applied to a container.
Some fields are present in both SecurityContext and PodSecurityContext.  When both
are set, the values in SecurityContext take precedence.
type*SecurityContext*struct {
    // The capabilities to add/drop when running containers.
    // Defaults to the default set of capabilities granted by the container runtime.
    // +optional
    Capabilities *Capabilities
    // Run container in privileged mode.
    // Processes in privileged containers are essentially equivalent to root on the host.
    // Defaults to false.
    // +optional
    Privileged *bool
    // The SELinux context to be applied to the container.
    // If unspecified, the container runtime will allocate a random SELinux context for each
    // container.  May also be set in PodSecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence.
    // +optional
    SELinuxOptions *SELinuxOptions
    // Windows security options.
    // +optional
    WindowsOptions *WindowsSecurityContextOptions
    // The UID to run the entrypoint of the container process.
    // Defaults to user specified in image metadata if unspecified.
    // May also be set in PodSecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence.
    // +optional
    RunAsUser *int64
    // The GID to run the entrypoint of the container process.
    // Uses runtime default if unset.
    // May also be set in PodSecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence.
    // +optional
    RunAsGroup *int64
    // Indicates that the container must run as a non-root user.
    // If true, the Kubelet will validate the image at runtime to ensure that it
    // does not run as UID 0 (root) and fail to start the container if it does.
    // If unset or false, no such validation will be performed.
    // May also be set in PodSecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence.
    // +optional
    RunAsNonRoot *bool
    // The read-only root filesystem allows you to restrict the locations that an application can write
    // files to, ensuring the persistent data can only be written to mounts.
    // +optional
    ReadOnlyRootFilesystem *bool
    // AllowPrivilegeEscalation controls whether a process can gain more
    // privileges than its parent process. This bool directly controls if
    // the no_new_privs flag will be set on the container process.
    // +optional
    AllowPrivilegeEscalation *bool
    // ProcMount denotes the type of proc mount to use for the containers.
    // The default is DefaultProcMount which uses the container runtime defaults for
    // readonly paths and masked paths.
    // +optional
    ProcMount *ProcMountType
}
</pre>

<pre zoom>
<span xsmall>PodSecurityContext</span>
// PodSecurityContext holds pod-level security attributes and common container settings
// Some fields are also present in container.securityContext.  Field values of
// container.securityContext take precedence over field values of PodSecurityContext.
type*PodSecurityContext*struct {
    // Use the host's network namespace.  If this option is set, the ports that will be
    // used must be specified.
    // Optional: Default to false
    // +k8s:conversion-gen=false
    // +optional
    HostNetwork bool
    // Use the host's pid namespace.
    // Optional: Default to false.
    // +k8s:conversion-gen=false
    // +optional
    HostPID bool
    // Use the host's ipc namespace.
    // Optional: Default to false.
    // +k8s:conversion-gen=false
    // +optional
    HostIPC bool
    // Share a single process namespace between all of the containers in a pod.
    // When this is set containers will be able to view and signal processes from other conts
    // in the same pod, and the first process in each container will not be assigned PID 1.
    // HostPID and ShareProcessNamespace cannot both be set.
    // Optional: Default to false.
    // This field is beta-level and may be disabled with the PodShareProcessNamespace feature.
    // +k8s:conversion-gen=false
    // +optional
    ShareProcessNamespace *bool
    // The SELinux context to be applied to all containers.
    // If unspecified, the container runtime will allocate a random SELinux context for each
    // container.  May also be set in SecurityContext.  If set in
    // both SecurityContext and PodSecurityContext, the value specified in SecurityContext
    // takes precedence for that container.
    // +optional
    SELinuxOptions *SELinuxOptions
    // Windows security options.
    // +optional
    WindowsOptions *WindowsSecurityContextOptions
    // The UID to run the entrypoint of the container process.
    // Defaults to user specified in image metadata if unspecified.
    // May also be set in SecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence
    // for that container.
    // +optional
    RunAsUser *int64
    // The GID to run the entrypoint of the container process.
    // Uses runtime default if unset.
    // May also be set in SecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence
    // for that container.
    // +optional
    RunAsGroup *int64
    // Indicates that the container must run as a non-root user.
    // If true, the Kubelet will validate the image at runtime to ensure that it
    // does not run as UID 0 (root) and fail to start the container if it does.
    // If unset or false, no such validation will be performed.
    // May also be set in SecurityContext.  If set in both SecurityContext and
    // PodSecurityContext, the value specified in SecurityContext takes precedence
    // for that container.
    // +optional
    RunAsNonRoot *bool
    // A list of groups applied to the first process run in each container, in addition
    // to the container's primary GID.  If unspecified, no groups will be added to
    // any container.
    // +optional
    SupplementalGroups []int64
    // A special supplemental group that applies to all containers in a pod.
    // Some volume types allow the Kubelet to change the ownership of that volume
    // to be owned by the pod:
    //
    // 1. The owning GID will be the FSGroup
    // 2. The setgid bit is set (new files created in the volume will be owned by FSGroup)
    // 3. The permission bits are OR'd with rw-rw----
    //
    // If unset, the Kubelet will not modify the ownership and permissions of any volume.
    // +optional
    FSGroup *int64
    // Sysctls hold a list of namespaced sysctls used for the pod. Pods with unsupported
    // sysctls (by the container runtime) might fail to launch.
    // +optional
    Sysctls []Sysctl
}
</pre>

<pre zoom>
<span xsmall>ServiceAccount</span>
@[https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/]
@[https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/]

- Processes in containers inside pods can also contact the apiserver.
  When they do, they are authenticated as a particular Service Account 
- provides an identity for processes that run in a Pod.
- to access the API from inside a pod a mounted service account with
  a 1-hour-expiration-token is provided.
  (can be disabled with automountServiceAccountToken)
- User accounts are for humans. Service accounts are for processes running on pods.
- Service accounts are namespaced.
- cluster users can create service accounts for specific tasks (i.e. principle of least privilege).
- Allows to split auditing between humans and services.

- Three separate components cooperate to implement the automation around service accounts:
  - A*Service account admission controller*, part of the apiserver, thatsynchronously modify 
    pods as they are created or updated:
    - sets the ServiceAccount to default when not specified by pod.
    - abort if ServiceAccount in the pod spec does NOT exists.
    - ImagePullSecrets of ServiceAccount are added to the pod if none is specified by pod.
    - Adds a volume to the pod which contains a token for API access.
      (service account token expires after 1 hour or on pod deletion)
    - Adds a volumeSource to each container of the pod mounted at
      */var/run/secrets/kubernetes.io/serviceaccount*
  - A*Token controller*, running as as part of controller-manager. It acts asynchronously and
    - observes serviceAccount creation and creates a corresponding Secret to allow API access.
    - observes serviceAccount deletion and deletes all corresponding ServiceAccountToken Secrets.
    - observes secret addition, and ensures the referenced ServiceAccount exists, and adds a
      token to the secret if needed.
    - observes secret deletion and removes a reference from the corresponding ServiceAccount if needed.
    NOTE: 
      controller-manageR*'--service-account-private-key-file'* indicates the priv.key signing tokens
     - kube-apiserver   *'--service-account-key-file'*         indicates the matching pub.key verifying signatures
    - A controller loop ensures a secret with an API token exists for each service account.

    To create additional API tokens for a service account:
    - create a new secret of type ServiceAccountToken like: 
      (controller will update it with a generated token)
      {
        "kind":*"Secret"*,
        "type":*"kubernetes.io/service-account-token"*
        "apiVersion": "v1",
        "metadata": {
          "name": "mysecretname",
          "annotations": {
            "kubernetes.io/service-account.name" :
              "myserviceaccount"  ← reference to service account
          }
        },
      }
  - A*Service account controller:*
    - manages ServiceAccount inside namespaces,
    - ensures a ServiceAccount named “default” exists in every active namespace.
      $ kubectl get serviceAccounts
      NAME      SECRETS    AGE
      default   1          1d   ← Exists for each namespace
      ...
    - Additional ServiceAccount's can be created like:
    - 
    - $ kubectl apply -f - ˂˂EOF
    - apiVersion: v1
    - kind: ServiceAccount
    - metadata:
    -   name: build-robot
    - EOF
</pre>

<pre zoom>
<span xsmall>Liveness  Probes</span>
<span xsmall>Readiness Probes</span>
@[https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-probes/]
@[https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.14/#probe-v1-core]

.
- liveness probes  : used by kubelet to know when to restart a Container.
- readiness probes : used by kubelet to know whether a Container can accept new requests.
                     - A Pod is considered ready when all of its Containers are ready. 
                     - The Pod is not restarted in this case but no new requests are
                       forwarded.
                     - Readiness probes continue to execute for the lifetime of the Pod
                       (not only at startup)

*liveness COMMAND*            │ *liveness HTTP request*         │ *TCP liveness probe*
                              │                                 │ 
apiVersion: v1                │ apiVersion: v1                  │ apiVersion: v1
kind:*Pod*                    │ kind:*Pod*                      │ kind:*Pod*
metadata:                     │ metadata:                       │ metadata:
  ...                         │   ...                           │   ...
spec:                         │ spec:                           │ spec:
 *containers:*                │  *containers:*                  │  *containers:*
  - name: ...                 │   - name: ...                   │   - name: ...
    ...                       │     ...                         │    *readinessProbe:          *
   *livenessProbe:          * │    *livenessProbe:           *  │    *  tcpSocket:             *
   *  exec:                 * │    *  httpGet:               *  │    *    port: 8080           *
   *    command:            * │    *    path: /healthz       *  │    *  initialDelaySeconds: 5 *
   *    - cat               * │    *    port: 8080           *  │    *  periodSeconds: 10      *
   *    - /tmp/healthy      * │    *    httpHeaders:         *  │    *livenessProbe:           *
   *  initialDelaySeconds: 5* │    *    - name: Custom-HeadeR*  │    *  tcpSocket:             *
   *  periodSeconds: 5      * │    *      value: Awesome     *  │    *    port: 8080           *
                              │    *  initialDelaySeconds: 3 *  │    *  initialDelaySeconds: 15*
                              │    *  periodSeconds: 3       *  │    *  periodSeconds: 20      *
                                HTTP proxy ENV.VARS is ignored 
                                in liveness probes (k8s v1.13+) 

                                      ↑                                ↑
                                      │                                │
*named*ContainerPort can also   ──────┴────────────────────────────────┘ 
be used for HTTP and TCP probes
like:
 │ ports:
 │ - name: liveness-port
 │   containerPort: 8080
 │   hostPort: 8080
 │ 
 │ livenessProbe:
 │   httpGet:
 │     path: /healthz
 │     port: liveness-port

*Monitor liveness test result*
$ kubectl describe pod liveness-exec

Other optional parameters:
- timeoutSeconds  : secs after which the probe times out. (1 sec by default)
- successThreshold: Minimum consecutive successes for the probe to be
                    considered successful after having failed. (1 by default)
- failureThreshold: number of failures before "giving up": Pod will be marked Unready.(Def 3)

HTTP optional parameters: 
- host  : for example 127.0.0.1 (defaults to pod IP)
- port  : Name or number(1-65535)
- scheme: HTTP or HTTPS(skiping cert.validation)
- path  : 
- httpHeaders: Custom headers 
</pre>

<pre zoom>
<span TODO xsmall>Config Pods/Cont.</span>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/configure-pod-container/assign-pods-nodes/">Assign Pods to Nodes</a>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/configure-pod-container/configure-pod-initialization/">Config.Pod I12n</a>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/configure-pod-container/attach-handler-lifecycle-event/">Attach Cont.Lifecycle Handlers</a>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/configure-pod-container/share-process-namespace/">Share Process Namespace between Containers in a Pod</a>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/configure-pod-container/translate-compose-kubernetes/">Translate Docker-Compose to k8s Resources</a>

<a TODO href="https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/">Pod Priority and Preemption</a>
<a TODO href="https://kubernetes.io/docs/concepts/configuration/assign-pod-node/">Assigning Pods to Nodes</a>
<a TODO href="https://kubernetes.io/docs/concepts/policy/pod-security-policy/">Pod Security Policies</a>
<a TODO href="https://kubernetes.io/docs/concepts/policy/resource-quotas/">Resource Quotas</a>
</pre>

<span xsmall>Services</span>
<pre zoom>
<span xsmall>Service</span>
@[https://kubernetes.io/docs/concepts/services-networking/service/]
  ^^^ contains -many more info- about kube-proxy + iptables +... advanced config settings


- Service:*Defines a logical set of Pods (Label Selected) and *a policy by which to access them*

- Kubernetes-native applications: K8s offers a simple Endpoints API
- non-native        applications: K8s offers virtual-IP-based-2-Service bridge

kind:*Service*                  ServiceTypes:
apiVersion: v1                  ┌─────────────┬─────────────────────────────────┬──────────────────────
metadata:                       │Type         │ Description                     │ External Access
  name: my-service              ├─────────────┼─────────────────────────────────┼──────────────────────
spec:                           │O*ClusterIP* │ (default) Exposes the service   │ (None)
  selector:                     │(virtual IP) │ on a cluster─internal IP.       │
    app: MyApp                  ├─────────────┼─────────────────────────────────┼──────────────────────
  ports:                        │NodePort     │                                 │ (each)NodeIP:NodePort
  - protocol: TCP               ├─────────────┼─────────────────────────────────┼──────────────────────
    port: 80                    │LoadBalancer │ uses a cloud provider           │ ClusterIP:NodePort
    targetPort: 9376            │             │ load balancer                   │ ^spec.clusterIP
  - name: https                 │             │ (round robin by default)        │ 
    protocol: TCP               ├─────────────┼─────────────────────────────────┼──────────────────────
    port: 443                   │ExternalName │ Maps the service to the contents│ foo.bar.example.com
    targetPort: 9377            │             │ of the "externalName" field     │
  sessionAffinity: "ClientIP"   │             │ by returning a CNAME record.    │
  ^                             └─────────────┴─────────────────────────────────┴──────────────────────
  |
  sessionAffinityConfig.clientIP.timeoutSeconds: 100 (defaults to 10800)

-*Service.spec.clusterIP can be used to force a given IP*

│   USER SPACE proxy─mode                     │   IPTABLES proxy─mode 
│                                             │   (or ipvs)
│     NODE                                    │     NODE
│   ┌─────────────────────────┐               │   ┌─────────────────────────┐
│   │ ┌──────┐                │               │   │ ┌──────┐  ┌──────────┐  │
│   │ │Client│                │               │   │ │Client│  │kube─proxy│←─── API─Server
│   │ └──────┘                │               │   │ └┬─────┘  └────────┬─┘  │
│   │    ↓                    │               │   │  │                 │    │   
│   │ O*ClusterIP*            │               │   │  │                 │    │   
│   │ (iptables)              │               │   │  │   ┌───────────┐ │    │
│   │    │      ┌──────────┐  │               │   │  └──→│O*cluserIP*│←┘    │
│   │    └─────→│kube─proxy│←──API─Server     │   │      │(iptables) │      │
│   │           └────┬─────┘  │               │   │      └────┬──────┘      │
│   └────────────────│────────┘               │   └───────────│─────────────┘
│      ┌─────────────┴─────────┐              │      ┌────────┴──────────────┐
│      │                       │              │      │                       │
│┌─────↓───────────┐  ┌────────↓────────┐     │┌─────↓───────────┐  ┌────────↓────────┐
││Backend Pod 1    │  │Backend Pod 2    │     ││Backend Pod 1    │  │Backend Pod 2    │
││lables: app=MyApp│  │lables: app=MyApp│     ││lables: app=MyApp│  │lables: app=MyApp│
││IP:10.244.3.4    │  │IP:10.244.2.5    │     ││IP:10.244.3.4    │  │IP:10.244.2.5    │
││port:9113   ↑    │  │port:9113   ↑    │     ││port:9113        │  │port:9113        │
│└────────────│────┘  └────────────│────┘     │└─────────────────┘  └─────────────────┘
              │                    │
   Note that the Pod IP dies when the pod dies,
   while the ClusterIP persists for the 
   application (Deployment) life.

┌─────────────────┐
│SERVICE DISCOVERY│
├─────────────────┴─────────────────────────────────────────────────────────────────────────────────────────
│  ALT.1, using ENV.VARS                          │  ALT.2, using the k8s DNS add─on
│                                                 │  (strongly recommended).
├─────────────────────────────────────────────────┼──────────────────────────────────────────────────────────
│Ex: given service "redis─master" with            │  Updates dynamically the set of DNS records for services.
│  O*ClusterIP* = 10.0.0.11:6379                  │  The entry will be similar to:  "my─service.my─namespace"
│  next ENV.VARs are created:                     │
│  REDIS_MASTER_SERVICE_HOST=10.0.0.11            │
│  REDIS_MASTER_SERVICE_PORT=6379                 │
│  REDIS_MASTER_PORT=tcp://10.0.0.11:6379         │
│  REDIS_MASTER_PORT_6379_TCP=tcp://10.0.0.11:6379│
│  REDIS_MASTER_PORT_6379_TCP_PROTO=tcp           │
│  REDIS_MASTER_PORT_6379_TCP_PORT=6379           │
│  REDIS_MASTER_PORT_6379_TCP_ADDR=10.0.0.11      │
└─────────────────────────────────────────────────┴──────────────────────────────────────────────────────────
 
  
*Quick App(Deployment) service creation*
REF: @[https://kubernetes.io/docs/concepts/services-networking/connect-applications-service/]
$ kubectl expose deployment/my-nginx

is equivalent to:

$ kubectl apply -f service.yaml
                   ^^^^^^^^^^^^                  
                   apiVersion: v1
                   kind: Service
                   metadata:
                     name: my-nginx
                     labels:
                       run: my-nginx
                   spec:
                     ports:
                     - port: 80
                       protocol: TCP
                     selector:
                       run: my-nginx

@[https://kubernetes.io/docs/concepts/services-networking/add-entries-to-pod-etc-hosts-with-host-aliases/]
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/">Debug Services</a>
</pre>

<span title>Labels</span>
<pre zoom>
<span xsmall>Labels</span>
@[https://kubernetes.io/docs/concepts/overview/working-with-objects/labels/#label-selectors/]
- Labels = key/value pairs attached to objects.
- labels*do not provide uniqueness* 
  - we expect many objects to carry the same label(s)
- used to specify *identifying attributes of objects*
 *meaningful and relevant to users*, (vs k8s core system)
- Normally used to organize and to select subsets of objects.
- Label format:
  ("prefix"/)name
  name : [a-z0-9A-Z\-_.]{1,63}
  prefix : must be a DNS subdomain no longer than 253 chars

- Example labels:
  "release"    : "stable" # "canary" ...
  "environment": "dev" # "qa", "pre",  "production"
  "tier"       : "frontend" # "backend" "cache"
  "partition"  : "customerA", "partition" : "customerB"
  "track"      : "daily" # "weekly" "monthly" ...


*Recommended Labels*
@[https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/]
Key                           Description                      Example             Type
app.kubernetes.io/name        The name of the application      mysql               string
app.kubernetes.io/instance    A unique name identifying        wordpress-abcxzy    string
                              the instance of an application
app.kubernetes.io/version     current version of the app       5.7.21              string
app.kubernetes.io/component   component within the arch        database            string
app.kubernetes.io/part-of     name of higher level app         wordpress           string
app.kubernetes.io/managed-by  tool used to manage the          helm                string
                              operation of an application

Ex.1 use in an StatefulSet object:

apiVersion: apps/v1                           |apiVersion: apps/v1             |apiVersion: v1
kind: StatefulSet                             |kind: Deployment                |kind: Service
metadata:                                     |metadata:                       |metadata:
 labels:                                      | labels:                        | labels:
  app.kubernetes.io/name      : mysql         |  .../name      : wordpress     |  .../name      : wordpress
  app.kubernetes.io/instance  : wordpress-abc |  .../instance  : wordpress-abc |  .../instance  : wordpress-abcxzy
  app.kubernetes.io/version   : "5.7.21"      |  .../version   : "4.9.4"       |  .../version   : "4.9.4"
  app.kubernetes.io/component : database      |  .../component : server        |  .../component : server
 *app.kubernetes.io/part-of   :*wordpress*    |  .../part-of   :*wordpress*    |  .../part-of   : wordpress*
  app.kubernetes.io/managed-by: helm          |  .../managed-by: helm          |  .../managed-by: helm
                                              |...                             |...
                                                



-*Well-Known Labels, Annotations and Taints*
@[https://kubernetes.io/docs/reference/kubernetes-api/labels-annotations-taints/]
    kubernetes.io/arch
    kubernetes.io/os
    beta.kubernetes.io/arch (deprecated)
    beta.kubernetes.io/os (deprecated)
    kubernetes.io/hostname
    beta.kubernetes.io/instance-type
    failure-domain.beta.kubernetes.io/region
    failure-domain.beta.kubernetes.io/zone
</pre>

<pre zoom>
<span xsmall>Label selectors</span>
- core (object) grouping primitive.
- two types of selectors: 
  - equality-based   
    Ex. environment=production,tier!=frontend
                              ^
                            "AND" represented with ','

  - set-based
    - filter keys according to a set of values.
      (in, notin and exists ) 
    Ex.'*key*equal to environment and*value*equal to production or qa'

    Ex.'environment in (production, qa)'

    Ex.'tier notin (frontend, backend)'
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^ 
        all resources with 
            (key == "tier" AND
             values != frontend or backend )  AND
        all resources with 
            (key != "tier")

    Ex:'partition'                    !partition
        ^^^^^^^^^                     ^^^^^^^^^
        all resources including       all resources without
        a label with key 'partition'  a label with key 'partition'
        
- LIST and WATCH operations may specify label selectors to filter the sets
  of objects returned using a query parameter. 
  Ex.:
  $ kubectl get pods -l environment=production,tier=frontend             # equality-based
  $ kubectl get pods -l 'environment in (production),tier in (frontend)' # set-based
  $ kubectl get pods -l 'environment in (production, qa)'                # "OR" only set-based
  $ kubectl get pods -l 'environment,environment notin (frontend)'       # "NOTIN"
</pre>

<span title>Storage</span>

<pre zoom>
<span xsmall>Volume</span>
@[https://kubernetes.io/docs/concepts/storage/volumes/]
TODO: @[https://github.com/kubernetes/community/blob/master/contributors/design-proposals/storage/volume-ownership-management.md]
<span xsmall>lifetime of pod (vs docker container)</span>
- many types supported, and a Pod can use any number
   of them simultaneously.
- a pod specifies what volumes to provide for the pod (spec.volumes) and
    where to mount those into each container(spec.containers.volumeMounts).

- Sometimes, it is useful to share one volume for
  multiple uses in a single pod. The volumeMounts.subPath
  property can be used to specify a sub-path inside the
  referenced volume instead of its root.
</pre>

<pre zoom>
<span xsmall>Persistent Volumes</span>
@[https://kubernetes.io/docs/concepts/storage/persistent-volumes/]
  (provisioned by cluster admin)
- "hardware" resource applying to the whole cluster (just like a node)
- While Volumes lifecycle is dependent of pod's lifecyles,
  Persistent Volumes lifecycle is independent of any individual pod.
- The k8s API/object captures the details of the implementation of
  underlying storage (NFS, iSCSI, cloud-provided, ...)
- Pods use it through a volume of type "PersistentVolumeClaim" like:
  Similar to how Pods can request specific levels of resources
  (CPU and Memory), Claims can request specific size and access modes
  (e.g., can be mounted once read/write or many times read-only)
  - PV contains max size, PVC contains min size where
    PVC min.size must be ˂ PV max.size

Ex:
Ex. PV using O*local storage*:
apiVersion: v1
kind: *PersistentVolume*
metadata:
  name: example-pv
  annotations:
    "volume.alpha.kubernetes.io/node-affinity": '{
      "requiredDuringSchedulingIgnoredDuringExecution": {
        "nodeSelectorTerms": [
          { "matchExpressions": [
            { "key": "kubernetes.io/hostname",
              "operator": "In",
              "values": ["example-node"]
            }
          ]}
         ]}
        }'
*spec:*
  capacity:
    storage: 100Gi
  accessModes:
  - ReadWriteOnce
  persistentVolumeReclaimPolicy: Delete
O*storageClassName: local-storage*
O*local:                         *
O*  path: /mnt/disks/ssd1        *
</pre>

<pre zoom>
<a TODO xsmall href="https://kubernetes.io/docs/concepts/storage/storage-classes/">Storage Classes</a>
</pre>
<pre zoom>
<a TODO xsmall href="https://kubernetes.io/docs/concepts/storage/dynamic-provisioning/">Dynamic Volume Provisioning</a>
</pre>
<pre zoom>
<a TODO xsmall href="https://kubernetes.io/docs/concepts/storage/storage-limits/">Node-specific Volume Limits</a>
</pre>
</div>

<div groupv>
<span title>Controllers</span>
<pre zoom>
<span xsmall>ReplicationController</span>
@[https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller/]
Deprecated in favor of Deployment Controllers configuring a Replica Set
</pre>
<pre zoom>
<a xsmall href="https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/">ReplicaSet</a>
- ensures that a specified number of pod replicas are running at any given time.
- While ReplicaSets can be used independently, today it's mainly used by
  Deployments as a mechanism to orchestrate pod creation, deletion and updates.
- "we recommend using Deployments instead of directly using ReplicaSets, unless
   you require custom update orchestration or don't require updates at all."
- Unlike the case where a user directly created pods, a ReplicaSet replaces
  pods that are deleted or terminated for any reason, such as in the case of
  node failure or disruptive node maintenance, such as a kernel upgrade. For
  this reason, we recommend that you use a ReplicaSet even if your
  application requires only a single pod.
- Use a Job instead of a ReplicaSet for pods that are expected to terminate on their own.
- Use a DaemonSet instead of a ReplicaSet for pods that provide a machine-level
  function, such as machine monitoring or machine logging. These pods have a
  lifetime that is tied to a machine lifetime: the pod needs to be running on
  the machine before other pods start, and are safe to terminate when the
  machine is otherwise ready to be rebooted/shutdown.

|controllers/frontend.yaml
|
|apiVersion: apps/v1
|kind: ReplicaSet
|metadata:
|  name: frontend
|  labels:
|    app: guestbook
|    tier: frontend
|spec:
|  # modify replicas according to your case
|  replicas: 3   ← Default to 1
|  selector:
|    matchLabels:
|      tier: frontend  ← Matches all pods with labels matching the selector
|    matchExpressions:
|      - {key: tier, operator: In, values: [frontend]}
|  template:  ← Pod template (same schema as a pod, except that it is nested and does not have an apiVersion or kind)
|    metadata:
|      labels:             ← Needed in pod-template, not in isolated pod
|        app: guestbook     .spec.template.metadata.labels must match the .spec.selector
|        tier: frontend
|    spec:
|      restartPolicy: Always ← It's the only allowed value, can be ommited (default).
|      containers:
|      - name: php-redis
|        image: gcr.io/google_samples/gb-frontend:v3
|        resources:
|          requests:
|            cpu: 100m
|            memory: 100Mi
|        env:
|        - name: GET_HOSTS_FROM
|          value: dns
|          # If your cluster config does not include a dns service, then to
|          # instead access environment variables to find service host
|          # info, comment out the 'value: dns' line above, and uncomment the
|          # line below.
|          # value: env
|        ports:
|        - containerPort: 80


$ kubectl create -f (http://k8s.io/examples/controllers/frontend.yaml
→ replicaset.apps/frontend created
→ $ kubectl describe rs/frontend
→ Name:       frontend
→ Namespace:  default
→ Selector:   tier=frontend,tier in (frontend)
→ Labels:     app=guestbook
→         tier=frontend
→ Annotations:    ˂none˃
→ Replicas:   3 current / 3 desired
→ Pods Status:    3 Running / 0 Waiting / 0 Succeeded / 0 Failed
→ Pod Template:
→   Labels:       app=guestbook
→                 tier=frontend
→   Containers:
→    php-redis:
→     Image:      gcr.io/google_samples/gb-frontend:v3
→     Port:       80/TCP
→     Requests:
→       cpu:      100m
→       memory:   100Mi
→     Environment:
→       GET_HOSTS_FROM:   dns
→     Mounts:             ˂none˃
→   Volumes:              ˂none˃
→ Events:
→   FirstSeen    LastSeen    Count    From                SubobjectPath    Type        Reason            Message
→   ---------    --------    -----    ----                -------------    --------    ------            -------
→   1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-qhloh
→   1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-dnjpy
→   1m           1m          1        {replicaset-controller }             Normal      SuccessfulCreate  Created pod: frontend-9si5l


$ kubectl get pods
→ NAME             READY     STATUS    RESTARTS   AGE
→ frontend-9si5l   1/1       Running   0          1m
→ frontend-dnjpy   1/1       Running   0          1m
→ frontend-qhloh   1/1       Running   0          1m
</pre>

<pre zoom>
<span bgorange xsmall>Deployment(=="aplication")</span>

@[https://kubernetes.io/docs/concepts/workloads/controllers/deployment/]
  Declarative updates for Pods and ReplicaSets
  Ex. Deployment:
# for versions before 1.7.0 use apps/v1beta1
apiVersion: apps/v1beta2
kind: Deployment
metadata:
  name: nginx-deployment
  labels:
    app: nginx
# namespace: production
spec:
  replicas: 3                  ← 3 replicated Pods
  strategy:
   - type : Recreate           ← Recreate | RollingUpdate*
                               # Alt. strategy example
                               # strategy:
                               #   rollingUpdate:
                               #     maxSurge: 2
                               #     maxUnavailable: 0
                               #   type: RollingUpdate
  selector:
    matchLabels:
      app: nginx
  template:                    ← pod template
    metadata:
      labels:
        app: nginx
    spec:                      ← template pod spec
      containers:                change triggers new rollout
      - name: nginx
        image: nginx:1.7.9
        ports:
        - containerPort: 80
        livenessProbe:
          httpGet:
          path: /heartbeat
          port: 80
          scheme: HTTP

$ kubectl create -f nginx-deployment.yaml

$ kubectl get deployments
NAME             DESIRED CURRENT  ...
nginx-deployment 3       0

$ kubectl rollout status deployment/nginx-deployment
  Waiting for rollout to finish: 2 out of 3 new replicas
  have been updated...
  deployment "nginx-deployment" successfully rolled out

$ kubectl get deployments
NAME             DESIRED CURRENT ...
nginx-deployment 3       3


*To see the ReplicaSet (rs) created by the deployment:*
$ kubectl get rs
NAME                     DESIRED ...
nginx-deployment-...4211 3
^*1

*1:format [deployment-name]-[pod-template-hash-value]

*To see the labels automatically generated for each pod*
$ kubectl get pods --show-labels
NAME          ... LABELS
nginx-..7ci7o ... app=nginx,...,
nginx-..kzszj ... app=nginx,...,
nginx-..qqcnn ... app=nginx,...,

*Ex: Update nginx Pods from nginx:1.7.9 to nginx:1.9.1:
$ kubectl set image deployment/nginx-deployment \
   nginx=nginx:1.9.1

*Check the revisions of deployment:*
$ kubectl rollout history deployment/nginx-deployment
deployments "nginx-deployment"
R CHANGE-CAUSE
1 kubectl create -f nginx-deployment.yaml ---record
2 kubectl set image deployment/nginx-deployment \
                    nginx=nginx:1.9.1
3 kubectl set image deployment/nginx-deployment \
                    nginx=nginx:1.91

$ kubectl rollout undo deployment/nginx-deployment \
   --to-revision=2

*Scale Deployment:*
$ kubectl scale deployment \
  nginx-deployment --replicas=10

$ kubectl autoscale deployment nginx-deployment \
  --min=10 --max=15 --cpu-percent=80
</pre>

<pre zoom>
<span TODO xsmall>Inject Data Into Apps</span>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/define-command-argument-container/">Define a Command and Arguments for a Container</a>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/">Define Environment Variables for a Container</a>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/environment-variable-expose-pod-information/">Expose Pod Information to Containers Through Environment Variables</a>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/downward-api-volume-expose-pod-information/">Expose Pod Information to Containers Through Files</a>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/">Distribute Credentials Securely Using Secrets</a>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/podpreset/">Inject Information into Pods Using a PodPreset</a>
</pre>
<pre zoom>
<span TODO xsmall>Run Applications</span>
<a href=https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/">Run a Stateless Application Using a Deployment</a>
<a href=https://kubernetes.io/docs/tasks/run-application/run-single-instance-stateful-application/">Run a Single-Instance Stateful Application</a>
<a href=https://kubernetes.io/docs/tasks/run-application/run-replicated-stateful-application/">Run a Replicated Stateful Application</a>
<a href=https://kubernetes.io/docs/tasks/run-application/update-api-object-kubectl-patch/">Update API Objects in Place Using kubectl patch</a>
<a href=https://kubernetes.io/docs/tasks/run-application/scale-stateful-set/">Scale a StatefulSet</a>
<a href=https://kubernetes.io/docs/tasks/run-application/delete-stateful-set/">Delete a StatefulSet</a>
<a href=https://kubernetes.io/docs/tasks/run-application/force-delete-stateful-set-pod/">Force Delete StatefulSet Pods</a>
<a href=https://kubernetes.io/docs/tasks/run-application/rolling-update-replication-controller/">Perform Rolling Update Using a Replication Controller</a>
<a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">Horizontal Pod Autoscaler</a>
<a href=https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale-walkthrough/">Horizontal Pod Autoscaler Walkthrough</a>
<a href=https://kubernetes.io/docs/tasks/run-application/configure-pdb/">Specifying a Disruption Budget for your Application</a>
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application/">Troubleshoot Applications</a>
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-application-introspection/">App Introspection and Debugging</a>
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/local-debugging/">Developing and debugging services locally</a>
</pre>

<pre zoom>
<span TODO xsmall>Access Applications in a Cluster</span>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/web-ui-dashboard/">Web UI (Dashboard)</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/access-cluster/">Accessing Clusters</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/configure-access-multiple-clusters/">Configure Access to Multiple Clusters</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/">Use Port Forwarding to Access Applications in a Cluster</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/load-balance-access-application-cluster/">Provide Load-Balanced Access to an Application in a Cluster</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/service-access-application-cluster/">Use a Service to Access an Application in a Cluster</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/connecting-frontend-backend/">Connect a Front End to a Back End Using a Service</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/create-external-load-balancer/">Create an External Load Balancer</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/configure-cloud-provider-firewall/">Configure Your Cloud Provider's Firewalls</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/list-all-running-container-images/">List All Container Images Running in a Cluster</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/communicate-containers-same-pod-shared-volume/">Communicate Between Containers in the Same Pod Using a Shared Volume</a>
<a href=https://kubernetes.io/docs/tasks/access-application-cluster/configure-dns-cluster/">Configure DNS for a Cluster</a>
</pre>

<pre zoom>
<span xsmall>StatefulSet (v:1.9+)</span>
@[https://kubernetes.io/docs/concepts/workloads/controllers/statefulset/]

- Manages stateful apps:
  - Useful for apps requiring one+ of:
    {
    - Stable, unique network identifiers.
    - persistent storage across Pod (re)scheduling
    - Ordered, graceful deployment and scaling.
    - Ordered, graceful deletion and termination.
    - Ordered, automated rolling updates.
    }

  - Manages the deploy+scaling of Pods providing
      guarantees about ordering and uniqueness
  - Unlike Deployments, a StatefulSet maintains a sticky identity for each
    of their Pods. These pods are created from the same spec, but are not
    interchangeable: each has a persistent identifier that it maintains
    across any rescheduling
  - Pod Identity: StatefulSet Pods have a unique identity that is comprised
    of [ordinal, stable network identity, stable storage] that sticks even
    if Pods is rescheduled on another node.
    <ol>
    - Ordinal:Each Pod will be assigned an unique integer ordinal,
       from 0 up through N-1, where N = number of replicas
    -  Stable Network Pod host-name = $(statefulset name)-$(ordinal)
     Ex. full DNS using Stateless service:

Pod full DNS  (web == StatefullSet.name)
O*← pod-host →* B*← service ns    →* Q*←clusterDoma→*
O*web-{0..N-1}*.B*nginx.default.svc*.Q*cluster.local*
O*web-{0..N-1}*.B*nginx.foo    .svc*.Q*cluster.local*
O*web-{0..N-1}*.B*nginx.foo    .svc*.Q*kube.local   *

*1: Cluster Domain defaults to cluster.local

    - Pod Name Label: When the controller creates a Pod,
      it adds a label statefulset.kubernetes.io/"pod-name" set to
      the name of the pod, allowing to attach a Service to an unique Pod


<span xsmall>*Limitations*</span>
  - The storage for a given Pod must either be provisioned by a
    PersistentVolume Provisioner based on the requested storage class, or
    pre-provisioned by an admin.
  - Deleting and/or scaling a StatefulSet down will not delete the
      volumes associated with the StatefulSet in order to ensure data safety,
      which is generally more valuable than an automatic purge of all related
      StatefulSet resources.
  - StatefulSets currently require a <span TODO>Headless Service</span> to be
    responsible for the network identity of the Pods. You are responsible for
    creating this Service.

*Example StatefulSet*
The B*headless Service (named nginx)*, is used to control the network domain
The <span orange>StatefulSet(named web)</orange>, has a Spec that indicates
that 3 replicas of the nginx container will be launched in unique Pods.
The G*volumeClaimTemplates* will provide stable storage
using PersistentVolumes provisioned by a PersistentVolume Provisioner.

<span blue>apiVersion: v1
kind: Service
metadata:
  name: nginx
  labels:
    app: nginx
spec:
  ports:
  - port: 80
    name: web
  clusterIP: None
  selector:
    app: nginx</span>
---
<span orange>apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: web
spec:
  replicas: 3 # by default is 1

  selector:
    matchLabels:
      Q*app: nginx # has to match .spec.template.metadata.labels*
  serviceName: "nginx"
  template:
    metadata:
      labels:
        <span>app: nginx # has to match .spec.selector.matchLabels</span>
</span>spec:
      terminationGracePeriodSeconds: 10
      containers:
      - name: nginx
        image: k8s.gcr.io/nginx-slim:0.8
        ports:
        - containerPort: 80
          name: web
        volumeMounts:
        - name: www
          mountPath: /usr/share/nginx/html
<span green>*volumeClaimTemplates*:
  # Kubernetes creates one PersistentVolume for each VolumeClaimTemplate
  - metadata:
      name: www
    spec:
      accessModes: [ "ReadWriteOnce" ]
      # each Pod will receive a single PersistentVolume
      # When a Pod is (re)scheduled onto a node, its volumeMounts mount
      # the PersistentVolumes associated with its PersistentVolume Claims.
      storageClassName: "my-storage-class"
      resources:
        requests:
          storage: 1Gi</span>
</span>
</pre>
<pre zoom>
<span xsmall>Deployment and Scaling Guarantees</span>
  - Pods are deployed sequentially in order from {0..N-1}.
  - When Pods are deleted they are terminated in reverse order, from {N-1..0}
  - Before a scaling operation is applied to a Pod, all of its predecessors must be Running and Ready
  - Before a Pod is terminated, all of its successors must be completely shutdown
  -*Ordering Policies guarantees can be relaxed via  .spec.podManagementPolicy(K8s 1.7+)*
    <ol>
    - OrderedReady: Defaults, implements the behavior described above
    - Parallel Pod Management: launch/terminate all Pods in parallel, not
       waiting for Pods to become Running and Ready or completely terminated
    -
    </ol>

  -*.spec.updateStrategy* allows to configure and disable
    automated rolling updates for containers, labels, resource request/limits,
    and annotations for the Pods in a StatefulSet.
    <ol>
    - "OnDelete" implements the legacy (1.6 and prior) behavior.
      StatefulSet controller will not automatically update the Pods in a StatefulSet.
      Users must manually delete Pods to cause the controller to create new Pods
      that reflect modifications made to a StatefulSet’s .spec.template.
    - "RollingUpdate" (default 1.7+) implements automated, rolling update for Pods.
       The StatefulSet controller will delete and recreate each Pod proceeding
       in the same order as Pod termination (largest to smallest ordinal), updating
       each Pod one at a time waiting until an updated Pod is Running and Ready
       prior to updating its predecessor.
       Partitions
       {
       - RollingUpdate strategy can be partitioned, by specifying a
         .spec.updateStrategy.rollingUpdate.partition
       - If specified all Pods with an ordinal greater than or equal to
          the partition will be updated when the StatefulSet’s .spec.template
          is updated. All Pods with an ordinal that is less than the partition
          will not be updated, and, even if they are deleted, they will be
          recreated at the previous version.
       - If it is greater than its .spec.replicas, updates to its
          .spec.template will not be propagated to its Pods.

       - In most cases you will not need to use a partition, but they
         are useful if you want to stage an update, roll out a canary,
         or perform a phased roll out.
       }
    </ol>


  }
</pre>
<pre zoom>
<span xsmall>DONT'S</span>
  - The StatefulSet should NOT specify a pod.Spec.TerminationGracePeriodSeconds of 0.
      Unsafe and *strongly discouraged*
</pre>
<pre zoom>
<a xsmall TODO href=https://kubernetes.io/docs/tasks/debug-application-cluster/debug-stateful-set/">Debug a StatefulSet</a>
</pre>

<pre zoom>
<a xsmall href="https://kubernetes.io/docs/concepts/workloads/controllers/daemonset/">DaemonSet</a>
- ensures "N" Nodes run a Pod instance
- typical uses: cluster storage, log collection or monitoring
  - As nodes are added to the cluster, Pods are added to them. As nodes are 
    removed from the cluster, those Pods are garbage collected. Deleting a 
    DaemonSet will clean up the Pods it created.
  - Ex (simple case): one DaemonSet, covering all nodes, would be used
    for each type of daemon. A more complex setup might use multiple DaemonSets
    for a single type of daemon, but with different flags and/or different memory
    and cpu requests for different hardware types.

Ex.  DaemonSet for B*fluentd-elasticsearch*:
$ cat daemonset.yaml
apiVersion: apps/v1
kind: *DaemonSet*
metadata:
  name: fluentd-elasticsearch
  namespace: kube-system
  labels:
    k8s-app: fluentd-logging
spec:
  selector:
    matchLabels:
      name: fluentd-elasticsearch
  template: # Pod template
    # Pod Template must have RestartPolicy equal to Always (default if un-specified)
    metadata:
      labels:
        name: fluentd-elasticsearch
    spec:
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      containers:
      - name: fluentd-elasticsearch
        image: B*k8s.gcr.io/fluentd-elasticsearch:1.20*
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
      terminationGracePeriodSeconds: 30
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
<!--
TODO
The .spec.selector field is a pod selector. It works the same as the .spec.selector of a Job.

As of Kubernetes 1.8, you must specify a pod selector that matches the labels of the .spec.template. The pod selector will no longer be defaulted when left empty. Selector defaulting was not compatible with kubectl apply. Also, once a DaemonSet is created, its spec.selector can not be mutated. Mutating the pod selector can lead to the unintentional orphaning of Pods, and it was found to be confusing to users.

The spec.selector is an object consisting of two fields:

    matchLabels - works the same as the .spec.selector of a ReplicationController.
    matchExpressions - allows to build more sophisticated selectors by specifying key, list of values and an operator that relates the key and values.

When the two are specified the result is ANDed.

If the .spec.selector is specified, it must match the .spec.template.metadata.labels. Config with these not matching will be rejected by the API.

Also you should not normally create any Pods whose labels match this selector, either directly, via another DaemonSet, or via other controller such as ReplicaSet. Otherwise, the DaemonSet controller will think that those Pods were created by it. Kubernetes will not stop you from doing this. One case where you might want to do this is manually create a Pod with a different value on a node for testing.
Running Pods on Only Some Nodes

If you specify a .spec.template.spec.nodeSelector, then the DaemonSet controller will create Pods on nodes which match that node selector. Likewise if you specify a .spec.template.spec.affinity, then DaemonSet controller will create Pods on nodes which match that node affinity. If you do not specify either, then the DaemonSet controller will create Pods on all nodes.
How Daemon Pods are Scheduled

Normally, the machine that a Pod runs on is selected by the Kubernetes scheduler. However, Pods created by the DaemonSet controller have the machine already selected (.spec.nodeName is specified when the Pod is created, so it is ignored by the scheduler). Therefore:

    The unschedulable field of a node is not respected by the DaemonSet controller.
    The DaemonSet controller can make Pods even when the scheduler has not been started, which can help cluster bootstrap.

Daemon Pods do respect taints and tolerations, but they are created with NoExecute tolerations for the following taints with no tolerationSeconds:

    node.kubernetes.io/not-ready
    node.alpha.kubernetes.io/unreachable

This ensures that when the TaintBasedEvictions alpha feature is enabled, they will not be evicted when there are node problems such as a network partition. (When the TaintBasedEvictions feature is not enabled, they are also not evicted in these scenarios, but due to hard-coded behavior of the NodeController rather than due to tolerations).

They also tolerate following NoSchedule taints:

    node.kubernetes.io/memory-pressure
    node.kubernetes.io/disk-pressure

When the support to critical pods is enabled and the pods in a DaemonSet are labeled as critical, the Daemon pods are created with an additional NoSchedule toleration for the node.kubernetes.io/out-of-disk taint.

Note that all above NoSchedule taints above are created only in version 1.8 or later if the alpha feature TaintNodesByCondition is enabled.

Also note that the node-role.kubernetes.io/master NoSchedule toleration specified in the above example is needed on 1.6 or later to schedule on master nodes as this is not a default toleration.
Communicating with Daemon Pods

Some possible patterns for communicating with Pods in a DaemonSet are:

    Push: Pods in the DaemonSet are configured to send updates to another service, such as a stats database. They do not have clients.
    NodeIP and Known Port: Pods in the DaemonSet can use a hostPort, so that the pods are reachable via the node IPs. Clients know the list of node IPs somehow, and know the port by convention.
    DNS: Create a headless service with the same pod selector, and then discover DaemonSets using the endpoints resource or retrieve multiple A records from DNS.
    Service: Create a service with the same Pod selector, and use the service to reach a daemon on a random node. (No way to reach specific node.)

Updating a DaemonSet

If node labels are changed, the DaemonSet will promptly add Pods to newly matching nodes and delete Pods from newly not-matching nodes.

You can modify the Pods that a DaemonSet creates. However, Pods do not allow all fields to be updated. Also, the DaemonSet controller will use the original template the next time a node (even with the same name) is created.

You can delete a DaemonSet. If you specify ──cascade=false with kubectl, then the Pods will be left on the nodes. You can then create a new DaemonSet with a different template. The new DaemonSet with the different template will recognize all the existing Pods as having matching labels. It will not modify or delete them despite a mismatch in the Pod template. You will need to force new Pod creation by deleting the Pod or deleting the node.

In Kubernetes version 1.6 and later, you can perform a rolling update on a DaemonSet.
Alternatives to DaemonSet
Init Scripts

It is certainly possible to run daemon processes by directly starting them on a node (e.g. using init, upstartd, or systemd). This is perfectly fine. However, there are several advantages to running such processes via a DaemonSet:

    Ability to monitor and manage logs for daemons in the same way as applications.
    Same config language and tools (e.g. Pod templates, kubectl) for daemons and applications.
    Running daemons in containers with resource limits increases isolation between daemons from app containers. However, this can also be accomplished by running the daemons in a container but not in a Pod (e.g. start directly via Docker).

Bare Pods

It is possible to create Pods directly which specify a particular node to run on. However, a DaemonSet replaces Pods that are deleted or terminated for any reason, such as in the case of node failure or disruptive node maintenance, such as a kernel upgrade. For this reason, you should use a DaemonSet rather than creating individual Pods.
Static Pods

It is possible to create Pods by writing a file to a certain directory watched by Kubelet. These are called static pods. Unlike DaemonSet, static Pods cannot be managed with kubectl or other Kubernetes API clients. Static Pods do not depend on the apiserver, making them useful in cluster bootstrapping cases. Also, static Pods may be deprecated in the future.
Deployments

DaemonSets are similar to Deployments in that they both create Pods, and those Pods have processes which are not expected to terminate (e.g. web servers, storage servers).

Use a Deployment for stateless services, like frontends, where scaling up and down the number of replicas and rolling out updates are more important than controlling exactly which host the Pod runs on. Use a DaemonSet when it is important that a copy of a Pod always run on all or certain hosts, and when it needs to start before other Pods.
-->
</pre>

<pre zoom>
<a xsmall TODO href="https://kubernetes.io/docs/concepts/workloads/controllers/garbage-collection/">Garbage Collection</a>
</pre>

<pre zoom>
<a xsmall TODO href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Jobs - Run to Completion</a>
</pre>
<pre zoom>
<span TODO xsmall>Run Jobs</span>
<a href=https://kubernetes.io/docs/tasks/job/automated-tasks-with-cron-jobs/">Running Automated Tasks with a CronJob</a>
<a href=https://kubernetes.io/docs/tasks/job/parallel-processing-expansion/">Parallel Processing using Expansions</a>
<a href=https://kubernetes.io/docs/tasks/job/coarse-parallel-processing-work-queue/">Coarse Parallel Processing Using a Work Queue</a>
<a href=https://kubernetes.io/docs/tasks/job/fine-parallel-processing-work-queue/">Fine Parallel Processing Using a Work Queue</a>
</pre>

<pre zoom>
<a xsmall TODO href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">CronJob</a>
</pre>

<pre zoom>
<a xsmall href="https://kubernetes.io/docs/concepts/workloads/controllers/jobs-run-to-completion/">Job</a>
- reliably run one+ Pod to "N" completions
  - creates one+ pods and ensures that a specified number of them successfully terminate
  - Jobs are complementary to Deployment Controllers. A Deployment Controller
    manages pods which are not expected to terminate (e.g. web servers), and
    a Job manages pods that are expected to terminate (e.g. batch jobs).
  - As pods successfully complete, the job tracks the successful completions. When a specified number of successful completions is reached, the job itself is complete. Deleting a Job will cleanup the pods it created.
  - Pod Backoff failure policy: ifyou want to fail a Job after N retries set
      .spec.backoffLimit (defaults to 6).
  - Pods are not deleted on completion in order to allow view logs/output/errors for completed pods. They will show up with kubectl get pods *-a*.
    Neither the job object in order to allow viewing its status.
  -  Another way to terminate a Job is by setting an active deadline
    in .spec.activeDeadlineSeconds or .specs.template.specs.activeDeadlineSeconds

  }
  example. Compute  2000 digits of "pi"
$ cat job.yaml

apiVersion: batch/v1
kind: *Job*
metadata:
  name: pi
spec:
  template: # Required (== Pod template - apiVersion - kind)
    spec:
      containers:
      - name: pi
        *image: perl*
        *command: ["perl",  "-Mbignum=bpi", "-wle", "print bpi(2000)"]*
      O*restartPolicy: Never* # Only Never/OnFailure allowed
  backoffLimit: 4

# *Run job using:*
# $ kubectl create -f ./job.yaml

# *Check job current status like:*
# $ kubectl describe jobs/pi
# output will be similar to:
# Name:             pi
# Namespace:        default
# Selector:         controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
# Labels:           controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
#                   job-name=pi
# Annotations:      ˂none˃
# Parallelism:      1
# Completions:      1
# Start Time:       Tue, 07 Jun 2016 10:56:16 +0200
# Pods Statuses:    0 Running / 1 Succeeded / 0 Failed
# Pod Template:
#   Labels:       controller-uid=b1db589a-2c8d-11e6-b324-0209dc45a495
#                 job-name=pi
#   Containers:
#    pi:
#     Image:      perl
#     Port:
#     Command:
#       perl
#       -Mbignum=bpi
#       -wle
#       print bpi(2000)
#     Environment:        ˂none˃
#     Mounts:             ˂none˃
#   Volumes:              ˂none˃
# Events:
#   FirstSeen LastSeen  Count From            SubobjectPath  Type    Reason            Message
#   --------- --------  ----- ----            -------------  ------- ------            -------
#   1m        1m        1     {job-controller}               Normal  SuccessfulCreate  Created pod: pi-dtn4q
#
# *To view completed pods of a job, use *
# $ kubectl get pods

# *To list all pods belonging to job in machine-readable-form*:
#
# $ pods=$(kubectl get pods --selector=*job-name=pi* --output=*jsonpath={.items..metadata.name}*)
# $ echo $pods
pi-aiw0a


# *View the standard output of one of the pods:*
# $ kubectl logs $pods
# 3.1415926535897a....9
</pre>
<pre zoom>
<span xsmall>Parallel Jobs</span>
  - Parallel Jobs with a fixed completion count 
    (.spec.completions greater than zero). 
    the job is complete when there is one successful pod for
    each value in the range 1 to .spec.completions.

  - Parallel Jobs with a work queue: do not specify .spec.completions:
     pods must coordinate with themselves or external service to determine
     what each should work on.
     each pod is independently capable of determining whether or not all its peers
     are done, thus the entire Job is done.

  - For Non-parallel job, leave both .spec.completions and
      .spec.parallelism unset.
  - Actual parallelism (number of pods running at any instant) may be more or less than requested parallelism, for a variety of reasons
  - (read official K8s docs for Job Patterns ussages)
  }
  <a href="https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/">Cron Jobs (1.8+)</a>
  - written in Cron format (question mark (?) has the same meaning as an asterisk *)
  - Concurrency Policy
    - Allow (default): allows concurrently running jobs
    - Forbid: forbids concurrent runs, skipping next
      if previous still running
    - Replace: cancels currently running job and replaces with new one
</pre>
<pre zoom>
<span xsmall>Ex. cronjob:</span>
$ cat cronjob.yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: hello
spec:
  schedule: O*"*/1 * * * *"*
  jobTemplate:
    spec:
      template:
        spec:
          containers:
        O*- name: hello    *
        O*  image: busybox *
        O*  args:          *
        O*  - /bin/sh      *
        O*  - -c           *
        O*  - date; echo 'Hi from K8s'*
          restartPolicy: OnFailure

# Alternatively:
$ kubectl run hello \
    --schedule="*/1 * * * *"  \
    --restart=OnFailure \
    --image=busybox \
    -- /bin/sh -c "date; echo Hello from the Kubernetes cluster"

# get status:

$ kubectl get cronjob hello
NAME      SCHEDULE      SUSPEND   ACTIVE    LAST-SCHEDULE
hello     */1 * * * *   False     0         <none>

# Watch for the job to be created:
$ kubectl get jobs --watch
NAME               DESIRED   SUCCESSFUL   AGE
hello-4111706356   1         1         2s
</pre>
</div>

<div groupv>
<span title>Node Monitoring</span>
<pre zoom>
<span TODO xsmall>Node Health</span>
@[https://kubernetes.io/docs/tasks/debug-application-cluster/monitor-node-health/]
</pre>

<pre zoom>
<a xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/crictl/">Debugging Kubernetes nodes with crictl</a>
</pre>

<span title>Monitoring the cluster</span>
<pre zoom>
<span TODO xsmall>Monitor, Log, and Debug</span>
</pre>
<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/audit/">Auditing</a>
</pre>
<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/core-metrics-pipeline/">Core metrics pipeline</a>
</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/events-stackdriver/">Events in Stackdriver</a>
</pre>

<pre zoom>
<span TODO xsmall>Metrics API+Pipeline</span>
@[https://kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/]
@[https://github.com/kubernetes-incubator/metrics-server]

Extracted from @[https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/]
"""If you are running Minikube, run the following command to enable the metrics-server:
   $ minikube addons enable metrics-server

... to see whether the metrics-server is running, or another provider of the resource metrics
API (metrics.k8s.io), run the following command:

   $ kubectl get apiservices

   output must include a reference to metrics.k8s.io.
   → ...
   → v1beta1.metrics.k8s.io

"""

</pre>

<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/logging-elasticsearch-kibana/">Logging Using Elasticsearch and Kibana</a>
</pre>
<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/logging-stackdriver/">Logging Using Stackdriver</a>
</pre>
<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/resource-usage-monitoring/">Tools for Monitoring Resources</a>
</pre>
<pre zoom>
<a TODO xsmall href=https://kubernetes.io/docs/tasks/debug-application-cluster/troubleshooting/">Troubleshooting</a>
</pre>
<pre zoom labels="">
<span xsmall>Troubleshooting</span>
<span xsmall>Debuggin the Cluster</span>
@[https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster/]
</pre>
</div>

<div groupv>
<span title>Unordered notes</span>

<pre zoom >
<span xsmall>Get External</span>
<span xsmall>IPs of all</span>
<span xsmall>nodes</span>

$ kubectl get nodes -o jsonpath=\
    '{.items[*].status.addresses[?(@.type=="ExternalIP")].address}'
</pre>

<pre zoom>
<span xsmall>Run stateless app. deployments</span>
@[https://kubernetes.io/docs/tasks/run-application/run-stateless-application-deployment/]
</pre>


<pre zoom>
<span xsmall>GPU and Kubernetes</span>
@[https://www.infoq.com/news/2018/01/gpu-workflows-kubernetes]
@[https://kubernetes.io/docs/tasks/manage-gpus/scheduling-gpus/]
</pre>


<pre zoom>
<span xsmall>Running</span>
<span xsmall>storage</span>
<span xsmall>services</span>
<span xsmall>(GlusterFS,iSCSI,...)</span>
@[https://opensource.com/article/17/12/storage-services-kubernetes?elqTrackId=6b7560db7aa24ea28978ac2a7e110c4c&elq=b9bc524fe6af48e38828609c30c593bc&elqaid=45485&elqat=1&elqCampaignId=148699]
</pre>


<pre zoom labels="devops,setup" bgorange>
<span xsmall>KOPS - KUBERNETES OPERATIONS</span>
<span xsmall>The easiest way</span>
<span xsmall>to get a production</span>
<span xsmall>grade k8s cluster</span>
<span xsmall>up and running</span>
@[https://github.com/kubernetes/kops/blob/master/README.md]
@[https://github.com/kubernetes/kops/blob/master/README.md]
  - What is kops?  """We like to think of it as kubectl for clusters."""
  -  kops helps you create, destroy, upgrade and maintain production-grade,
    highly available, Kubernetes clusters from the command line.
    AWS is currently officially supported, with GCE in beta support,
    and VMware vSphere in alpha, and other platforms planned.
</pre>

<pre zoom>
<span xsmall>HELM</span>
<span xsmall>Package</span>
<span xsmall>manager</span>
@[https://HELM.sh]

<span xsmall>Kubeapps</span>
@[https://kubeapps.com/]
The Easiest Way to Deploy Applications in Your Kubernetes Cluster
</pre>

<pre zoom>
<span xsmall>SIG-Apps</span>
@[https://github.com/kubernetes/community/blob/master/sig-apps/README.md]
  Special Interest Group for deploying and operating apps in Kubernetes.
  - They meet each week to demo and discuss tools and projects.
  <li cite>Covers deploying and operating applications in Kubernetes. We focus
    on the developer and devops experience of running applications in
    Kubernetes. We discuss how to define and run apps in Kubernetes, demo
    relevant tools and projects, and discuss areas of friction that can lead
    to suggesting improvements or feature requests
</pre>

<pre zoom>
<span xsmall TODO>Skaffold</span>
@[https://www.infoq.com/news/2018/03/skaffold-kubernetes]
  Tool to facilitate Continuous Development with Kubernetes
</pre>

<pre zoom>
<span xsmall>6 Tips for Running</span>
<span xsmall>Scalable Workloads</span>
@[https://www.infoq.com/articles/tips-running-scalable-workloads-kubernetes]
</pre>

<pre zoom>
<span xsmall>Kanifo</span>
@[https://github.com/GoogleContainerTools/kaniko]
- tool to build container images inside an unprivileged container or
  Kubernetes cluster.
- Although kaniko builds the image from a supplied Dockerfile, it does
  not depend on a Docker daemon, and instead executes each command completely
  in userspace and snapshots the resulting filesystem changes.
- The majority of Dockerfile commands can be executed with kaniko, with
  the current exception of SHELL, HEALTHCHECK, STOPSIGNAL, and ARG.
  Multi-Stage Dockerfiles are also unsupported currently. The kaniko team
  have stated that work is underway on both of these current limitations.
</pre>

<pre zoom>
<span xsmall>K&A with K8s...</span>
@[https://www.infoq.com/news/2018/02/dist-system-patterns-burns]
Distributed Systems programming is not for the faint of heart, and despite
the evolution of platforms and tools from COM, CORBA, RMI, Java EE, Web
Services, Services Oriented Architecture (SOA) and so on, it's more of an art
than a science.

Brendan Burns outlined many of the patterns that enables distributed systems
programming in the blog he wrote in 2015. He and David Oppenheimer, both
original contributors for Kubernetes, presented a paper at Usenix based
around design patterns and containers shortly after.
InfoQ caught up with Burns, who recently authored an ebook titled Designing
Distributed Systems, Patterns and Paradigms for Scaleable Microservices. He
talks about distributed systems patterns and how containers enable it.
</pre>

<pre zoom>
<span xsmall>Container Network Iface (CNI)</span>
@[https://github.com/containernetworking/cni]
  - specification and libraries for writing plugins to configure network interfaces
in Linux containers, along with a number of supported plugins.
  - CNI concerns itself only with network connectivity of containers and removing allocated resources when the container is deleted.
  - <a href="https://github.com/containernetworking/cni/blob/master/SPEC.md">CNI Spec</a>
</pre>

<pre TODO zoom>
<span xsmall>K8s Networking Explained</span>
@[https://www.slideshare.net/CJCullen/kubernetes-networking-55835829]
</pre>

<pre TODO zoom>
<span xsmall>Best Practices</span>
<span xsmall>(Rolling, Blue/Green,</span>
<span xsmall> Canary, BigBan,...)</span>
@[https://dzone.com/articles/best-practices-for-advanced-deployment-patterns]
</pre>

<pre TODO zoom>
<span xsmall>kubecfg</span>
<span xsmall>k8s as code</span>
@[https://github.com/ksonnet/kubecfg]
@[https://www.youtube.com/watch?v=zpgp3yCmXok] Writing less yaml

A tool for managing Kubernetes resources as code.

kubecfg allows you to express the patterns across your infrastructure and reuse
these powerful "templates" across many services, and then manage those templates
as files in version control. The more complex your infrastructure is, the more
you will gain from using kubecfg.

The idea is to describe as much as possible about your configuration as files
in version control (eg: git).
</pre>

<pre zoom>
<span xsmall>Atlassian escalator</span>
@[https://www.infoq.com/news/2018/05/atlassian-kubernetes-autoscaler]
In Kubernetes, scaling can mean different things to different users. We
distinguish between two cases:

- Cluster scaling, sometimes called infrastructure-level scaling, refers to
  the (auto\u2010 mated) process of adding or removing worker nodes based on cluster utilization.
- Application-level scaling, sometimes called pod scaling, refers to the (automated) process
  of manipulating pod characteristics based on a variety of metrics, from low-level signals
  such as CPU utilization to higher-level ones, such as HTTP requests served per
  second, for a given pod. Two kinds of pod-level scalers exist:
- Horizontal Pod Autoscalers (HPAs), which increase or decrease the number
  of pod replicas depending on certain metrics.
- Vertical Pod Autoscalers (VPAs), which increase or decrease the resource
  requirements of containers running in a pod.

Atlassian <a href="https://developers.atlassian.com/blog/2018/05/introducing-escalator/">released</a>
 their in-house tool <a href="https://github.com/atlassian/escalator/" >Escalator</a> as an open source
 project. It provides configuration-driven preemptive scale-up and faster scale-down
for <a href="https://www.infoq.com/kubernetes/" target="_blank">Kubernetes</a> nodes.

<p>Atlassian adopted containers and built their own
<a href="https://www.atlassian.com/company/events/summit-us/watch-sessions/2016/build-deploy/scaling-your-first-1000-containers-with-docker">

<p>Kubernetes has two autoscalers
- <a href="https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">horizontal pod autoscaler</a>
  scales pods: an abstraction over a container or a set of related containers - up and down, and thus depends upon the
   availability of underlying compute (usually VM) resources.
  pods can scale down very quickly,

- <a href="https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler">cluster autoscaler</a>.
  scales the compute infrastructure itself. Understandably, it takes a longer time to scale up and down due to
  the higher provisioning time of virtual machines. Any delays in the cluster autoscaler would translate to delays
  in the pod autoscaler.

 Atlassian’s problem was very specific to batch workloads, with a low tolerance for delay in scaling up and down.
They decided to write their own autoscaling functionality to solve these problems on top of Kubernetes.

   Escalator <a href="https://github.com/atlassian/escalator/blob/master/docs/configuration/advanced-configuration.md">configurable thresholds</a>
for upper and lower capacity of the compute VMs. Some of the configuration properties work by modifying a Kubernetes feature called
<a href="https://kubernetes.io/docs/concepts/configuration/taint-and-toleration/">taint</a>’. A VM node can be ‘tainted’ (marked) with a
certain value so that pods with a related marker are not scheduled onto it. Unused nodes would be
brought down faster by the Kubernetes standard cluster autoscaler when they are marked. The
scale-up configuration parameter is a threshold expressed as a percentage of utilization, usually
less than 100 so that there is a buffer.
 Escalator autoscales the compute VMs when utilization reaches the threshold, thus making room for
containers that might come up later, and allowing them to boot up fast.
</pre>

<pre TODO zoom>
<span xsmall>Job patterns</span>
One example of this pattern would be a Job which starts a Pod which runs a script
that in turn starts a Spark master controller (see spark example), runs a spark
driver, and then cleans up.
</pre>
</div>

<div groupv>
<span title>Install a new cluster</span>
<pre zoom>
<span xsmall TODO>Minikube</span>
@[https://kubernetes.io/docs/tasks/tools/install-minikube/]
</pre>

<pre zoom>
<span TODO xsmall>kubespray</span>
@[https://github.com/kubernetes-sigs/kubespray/blob/master/docs/comparisons.md]
</pre>

<pre zoom>
<span TODO small>Kubeadm</span>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm/">Overview of kubeadm</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-init/">kubeadm init</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-join/">kubeadm join</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-upgrade/">kubeadm upgrade</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-config/">kubeadm config</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-reset/">kubeadm reset</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-token/">kubeadm token</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-version/">kubeadm version</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/kubeadm-alpha/">kubeadm alpha</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubeadm/implementation-details/">Implementation details</a>

Tasks:
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-ha/">Upgrading kubeadm HA clusters from 1.9.x to 1.9.y</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-8/">Upgrading kubeadm clusters from 1.7 to 1.8</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-11/">Upgrading kubeadm clusters from v1.10 to v1.11</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade-1-9/">Upgrading/downgrading kubeadm clusters between v1.8 to v1.9</a>
</pre>


<pre zoom>
<span xsmall>(default) TCP Ports</span>
REF @[https://kubernetes.io/docs/tasks/tools/install-kubeadm/]
                                       ┌───────┬───────┐
                                       │Master │Worker │
                                       │node(s)│node(s)│
┌──────────────────────────────────────┼───────┼───────┤
│Port Range   │ Purpose                │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│6443*        │ Kubernetes API server  │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│2379─2380    │ etcd server client API │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│10250        │ Kubelet API            │   X   │  X    │
├──────────────────────────────────────┼───────┼───────┤
│10251        │ kube─scheduler         │   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│10252        │ kube─controller─manager│   X   │       │
├──────────────────────────────────────┼───────┼───────┤
│10255        │ Read─only Kubelet API  │   X   │  X    │
├──────────────────────────────────────┼───────┼───────┤
│ 30000─32767 │ NodePort Services      │       │  X    │
└──────────────────────────────────────┼───────┼───────┘
</pre>
<pre TODO zoom>
<span xsmall>kubelet TLS bootstrap</span>
<span xsmall>authentication</span>
<span xsmall>authorization</span>
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/]
@[https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/]
</pre>
<pre zoom>
<span xsmall>private (image) registries</span>
@[https://kubernetes.io/docs/concepts/containers/images/#using-a-private-registry]

<span xsmall>imagePullSecrets</span>
@[https://kubernetes.io/docs/concepts/configuration/secret/]
- method to pass a secret that contains a Docker image registry password 
 to the Kubelet so it can pull a private image on behalf of your Pod.



<a TODO href=https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry/">Pull Image from a Private Registry</a>
</pre>

<pre zoom>
<span TODO xsmall>TLS</span>
<a href=https://kubernetes.io/docs/tasks/tls/certificate-rotation/">Certificate Rotation</a>
<a href=https://kubernetes.io/docs/tasks/tls/managing-tls-in-a-cluster/">Manage TLS Certificates in a Cluster</a>
</pre>

<pre zoom>
<span TODO xsmall>Manage Cluster Daemons</span>
<a href=https://kubernetes.io/docs/tasks/manage-daemon/rollback-daemon-set/">Perform a Rollback on a DaemonSet</a>
<a href=https://kubernetes.io/docs/tasks/manage-daemon/update-daemon-set/">Perform a Rolling Update on a DaemonSet</a>
</pre>
<pre zoom>
<span TODO xsmall>Install Service Catalog</span>
<a href=https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-helm/">Install Service Catalog using Helm</a>
<a href=https://kubernetes.io/docs/tasks/service-catalog/install-service-catalog-using-sc/">Install Service Catalog using SC</a>
</pre>

<pre zoom>
<span xsmall>Un-ordered</span>
@[https://stackoverflow.com/questions/46891273/kubernetes-what-is-the-maximum-distance-latency-supported-between-kubernetes-no]
....There is no latency limitation between nodes in kubernetes cluster. They are configurable parameters.
- For kubelet on worker node it is:
   --node-status-update-frequency duration    Specifies how often kubelet posts node status to master. 
                                              Note: be cautious when changing the constant, it must work
                                                    with nodeMonitorGracePeriod in nodecontroller. (default 10s)
- For controller-manager on master node they are:
  --node-monitor-grace-period duration    Amount of time which we allow running Node to be unresponsive
                                          before marking it unhealthy. Must be N times more than kubelet's
                                          nodeStatusUpdateFrequency, where N means number of retries allowed 
                                          for kubelet to post node status. (default 40s)
  --node-monitor-period duration          The period for syncing NodeStatus in NodeController. (default 5s)
  --node-startup-grace-period duration    Amount of time which we allow starting Node to be unresponsive before
                                          marking it unhealthy. (default 1m0s)


</pre>

</div>

<div groupv>
<span title>Cluster Admin</span>

<pre zoom>
<a href="https://kubernetes.io/docs/reference/#config-reference">Config Ref()</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/cluster-administration-overview/">Overview</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/certificates/">Certificates</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/cloud-providers/">Cloud Providers</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/manage-deployment/">Managing Resources</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/networking/">Cluster Networking</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/logging/">Logging Architecture</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/kubelet-garbage-collection/">Configuring kubelet Garbage Collection</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/proxies/">Proxies in Kubernetes</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/controller-metrics/">Controller manager metrics</a>
<a xsmall href="https://kubernetes.io/docs/concepts/cluster-administration/addons/">Installing Addons</a>
</pre>
<pre zoom>
<span TODO xsmall>Extending Kubernetes</span>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/extend-cluster/">Extending your Kubernetes Cluster</a>
</pre>
<pre zoom>
<span TODO xsmall>Extending the Kubernetes API</span>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/apiserver-aggregation/">Extending the Kubernetes API with the aggregation layer</a>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/">Custom Resources</a>
</pre>
<pre zoom>
<span TODO xsmall>Config. files</span>
(documented in the Reference section of the online documentation, under each binary:)
<a href="https://kubernetes.io/docs/admin/kubelet/">kubelet</a>
<a href="https://kubernetes.io/docs/admin/kube-apiserver/">kube-apiserver</a>
<a href="https://kubernetes.io/docs/admin/kube-controller-manager/">kube-controller-manager</a>
<a href="https://kubernetes.io/docs/admin/kube-scheduler/">kube-scheduler</a>.
</pre>
<pre zoom>
<span TODO xsmall>Compute, Storage, and Networking Extensions</span>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/">Network Plugins</a>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/device-plugins/">Device Plugins</a>
<a href="https://kubernetes.io/docs/concepts/extend-kubernetes/service-catalog/">Service Catalog</a>
</pre>

<span title>Cluster Federation </span>
<pre zoom>
<span xsmall>Cluster Federation</span>
<span xsmall>Run an App on </span>
<span xsmall>Multiple Clusters</span>
@[https://kubernetes.io/docs/concepts/cluster-administration/federation/]

*Federation API*:
 - @[https://kubernetes.io/docs/reference/federation/extensions/v1beta1/definitions/]
 - @[https://kubernetes.io/docs/reference/federation/extensions/v1beta1/operations/]
 - @[https://kubernetes.io/docs/reference/federation/v1/definitions/]
 - @[https://kubernetes.io/docs/reference/federation/v1/operations/]

*External references:*
- @[https://kubernetes.io/docs/reference/command-line-tools-reference/federation-apiserver/]
- @[https://kubernetes.io/docs/reference/command-line-tools-reference/federation-controller-manager/]
- <a href=https://kubernetes.io/docs/tasks/federation/federation-service-discovery/">Cross-cluster Service Discovery using Federated Services</a>
- <a href=https://kubernetes.io/docs/tasks/federation/set-up-cluster-federation-kubefed/">Set up Cluster Federation with Kubefed</a>
- <a href=https://kubernetes.io/docs/tasks/federation/set-up-coredns-provider-federation/">Set up CoreDNS as DNS provider for Cluster Federation</a>
- <a href=https://kubernetes.io/docs/tasks/federation/set-up-placement-policies-federation/">Set up placement policies in Federation</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/cluster/">Federated Cluster</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/configmap/">Federated ConfigMap</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/daemonset/">Federated DaemonSet</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/deployment/">Federated Deployment</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/events/">Federated Events</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/hpa/">Federated Horizontal Pod Autoscalers (HPA)</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/ingress/">Federated Ingress</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/job/">Federated Jobs</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/namespaces/">Federated Namespaces</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/replicaset/">Federated ReplicaSets</a>
- <a href=https://kubernetes.io/docs/tasks/administer-federation/secret/">Federated Secrets</a>
- <a href=https://kubernetes.io/docs/tasks/extend-kubectl/kubectl-plugins/">Extend kubectl with plugins</a>
- <a href=https://kubernetes.io/docs/tasks/manage-hugepages/scheduling-hugepages/">Manage HugePages</a>

*Secrets*
- <a TODO href=https://kubernetes.io/docs/tasks/administer-federation/secret/">Federated Secrets</a>
</pre>

<pre zoom>
<span TODO xsmall>kubefed</span>
<span TODO xsmall>Controls cluster</span>
<span TODO xsmall>federation</span>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed/">kubefed</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-options/">kubefed options</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-init/">kubefed init</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-join/">kubefed join</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-unjoin/">kubefed unjoin</a>
<a href="https://kubernetes.io/docs/reference/setup-tools/kubefed/kubefed-version/">kubefed version</a>
</pre>

<pre zoom>
<span xsmall>Federated Services</span>
@[https://kubernetes.io/docs/tasks/federation/federation-service-discovery/]
</pre>

<pre zoom>
<span TODO xsmall>Using the k8s API</span>
<a href="https://kubernetes.io/docs/reference/using-api/api-overview/">Kubernetes API Overview</a>
</pre>

<pre zoom>
<span TODO xsmall>Accessing the API</span>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/controlling-access/">Controlling Access to the Kubernetes API</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authentication/">Authenticating</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/bootstrap-tokens/">Authenticating with Bootstrap Tokens</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/">Using Admission Controllers</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/extensible-admission-controllers/">Dynamic Admission Control</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/service-accounts-admin/">Managing Service Accounts</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/authorization/">Authorization Overview</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">Using RBAC Authorization</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/abac/">Using ABAC Authorization</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Using Node Authorization</a>
<a href="https://kubernetes.io/docs/reference/access-authn-authz/webhook/">Webhook Mode</a>
</pre>
<pre zoom>
<span TODO xsmall>Extend Kubernetes</span>
<span TODO xsmall>Use Custom Resources</span>
<a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definitions/">Extend the Kubernetes API with CustomResourceDefinitions</a>
<a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/custom-resource-definition-versioning/">Versions of CustomResourceDefinitions</a>
<a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/custom-resources/migrate-third-party-resource/">Migrate a ThirdPartyResource to CustomResourceDefinition</a>
<a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/configure-aggregation-layer/">Configure the Aggregation Layer</a>
<a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/setup-extension-api-server/">Setup an Extension API Server</a>
<a href=https://kubernetes.io/docs/tasks/access-kubernetes-api/http-proxy-access-api/">Use an HTTP Proxy to Access the Kubernetes API</a>
</pre>
</div>

<div groupv>
<span title>Understanding K8S Code</span>
<pre zoom>
<a xsmall href="https://jvns.ca/blog/2017/06/04/learning-about-kubernetes/">K8s Implementation Summary</a>
- <a href="https://jvns.ca/blog/2017/06/04/learning-about-kubernetes/">Julia Evans "A few things I've learned about Kubernetes"</a>

- """... you can run the kubelet by itself! And if you have a kubelet, you
can add the API server and just run those two things by themselves! Okay,
awesome, now let’s add the scheduler!"""

- the “kubelet” is in charge of running containers on nodes
- If you tell the API server to run a container on a node, it will tell the kubelet to get it done (indirectly)
- The scheduler translates "run a container" to "run a container on node X"

*etcd is Kubernetes’ brain*
- Every component in Kubernetes (API server, scheduler, kubelets, controller manager, ...) is stateless.
   All of the state is stored in the (key-value store) etcd database.
- Communication between components (often) happens via etcd.
-O*basically everything in Kubernetes works by watching etcd for stuff it has to do,*
 O*doing it, and then writing the new state back into etcd *

  Ex 1: Run a container on Machine "X":
   Wrong way: ask kubelet@Machine"X" to run the container.
   Right way: kubectl*1 →(API Server)→ etcd: "This pod should run on Machine X"
              kubelet@Machine"X"     → etcd: check work to do
              kubelet@Machine"X"     ← etcd: "This pod should run on Machine X"
              kubelet@Machine"X"     ← kubelet@Machine"X": Run pod

  Ex 2: Run a container anywhere on the k8s cluster
    kubectl*1 → (API Server) → etcd: "This pod should run somewhere"
    scheduler                → etcd: Something to run?
    scheduler                ← etcd: "This pod should run somewhere"
    scheduler                → kuberlet@Machine"Y":  Run pod

 *1 The kubectl is used from the command line.
    In the sequence diagram it can be replaced for any
    of the existing controllers (ReplicaSet, Deployment, DaemonSet, Job,...)

*API server roles in cluster:*
API Server is responsible for:
1.- putting stuff into etcd

    kubectl    → API Server : put "stuff" in etcd
    API Server → API Server : check "stuff"
    alt 1:
       kubectl ← API Server : error: "stuff" is wrong
    alt 2:
       API Server → etcd    : set/update "stuff"

2.- Managing authentication:
    ("who is allowed to put what stuff into etcd")
    The normal way is through X509 client certs.


*controller manager does a bunch of stuff*
Responsible for:
- Inspect etcd for pending to schedule pods.
- daemon-set-controllers will inspect etcd for
  pending daemonsets and will call the scheduler
  to run them on every machine with the given
  pod configuration.
- The "replica set controller" will inspect etcd for
  pending replicasets and will create 5 pods that
  the scheduler will then schedule.
- "deployment controller" ...

*Troubleshooting:*
something isn’t working? figure out which controller is
responsible and look at its logs

*Core K8s components run inside of k8s*
- Only 5 things needs to be running before k8s starts up:
  - the scheduler
  - the API server
  - etcd
  - kubelets on every node (to actually execute containers)
  - the controller manager (because to set up daemonsets you
                            need the controller manager)

  Any other core system (DNS, overlay network,... ) can
  be scheduled by k8s inside k8s
</pre>
<pre zoom>
<span TODO xsmall>API Conventions</span>
@[https://github.com/kubernetes/community/blob/master/contributors/devel/sig-architecture/api-conventions.md]
</pre>


<pre zoom>
<span xsmall>Source Code Layout</span>
Note: Main k8s are placed in <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg">kubernetes/pkg/</a>
      (API, kubectl, kubelet, controller, ...)

- <a href="https://developer.ibm.com/opentech/2017/06/21/tour-kubernetes-source-code-part-one-kubectl-api-server/">REF: A Tour of the Kubernetes Source Code Part One: From kubectl to API Server</a>

*Examining kubectl source*

Locating the implementation of kubectl commands in the Kubernetes source code

- <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd">kubectl entry point for all commands</a>
  - Inside there is a name of a go directory that matches the kubectl command:
    Example:
    <a href="https://github.com/kubernetes/kubernetes/tree/master/pkg/kubectl/cmd/create">kubectl create/create.go</a>

*K8s loves the Cobra Command Framework*
- k8s commands are implemented using the Cobra command framework.
- Cobra provides lot of features for building command line interfaces
  amongst them, Cobra puts the command usage message and command
  descriptions adjacent to the code that runs the command.
  Ex.:
  | // NewCmdCreate returns new initialized instance of create sub command
  | func NewCmdCreate(f cmdutil.Factory, ioStreams genericclioptions.IOStreams) *cobra.Command {
  |     o := NewCreateOptions(ioStreams)
  |
  |     cmd := ⅋cobra.Command{
  |         Use:                   "create -f FILENAME",
  |         DisableFlagsInUseLine: true,
  |         Short:                 i18n.T("Create a resource from a file or from stdin."),
  |         Long:                  createLong,
  |         Example:               createExample,
  |         Run: func(cmd *cobra.Command, args []string) {
  |             if cmdutil.IsFilenameSliceEmpty(o.FilenameOptions.Filenames) {
  |                 defaultRunFunc := cmdutil.DefaultSubCommandRun(ioStreams.ErrOut)
  |                 defaultRunFunc(cmd, args)
  |                 return
  |             }
  |             cmdutil.CheckErr(o.Complete(f, cmd))
  |             cmdutil.CheckErr(o.ValidateArgs(cmd, args))
  |             cmdutil.CheckErr(o.RunCreate(f, cmd))
  |         },
  |     }
  |
  |     // bind flag structs
  |     o.RecordFlags.AddFlags(cmd)
  |
  |     usage := "to use to create the resource"
  |     cmdutil.AddFilenameOptionFlags(cmd, ⅋o.FilenameOptions, usage)
  |     ...
  |     o.PrintFlags.AddFlags(cmd)
  |
  |     // create subcommands
  |     cmd.AddCommand(NewCmdCreateNamespace(f, ioStreams))
  |     ...
  |     return cmd
  | }

*Builders and Visitors Abound in Kubernetes*
  Ex. code:
  | r := f.NewBuilder().
  |     Unstructured().
  |     Schema(schema).
  |     ContinueOnError().
  |     NamespaceParam(cmdNamespace).DefaultNamespace().
  |     FilenameParam(enforceNamespace, ⅋o.FilenameOptions).
  |     LabelSelectorParam(o.Selector).
  |     Flatten().
  |     Do()
  The functions Unstructured, Schema, ContinueOnError,...  Flatten
  all take in a pointer to a Builder struct, perform some form of
  modification on the Builder struct, and then return the pointer to
  the Builder struct for the next method in the chain to use when it
  performs its modifications defined at:
  <a href="https://github.com/kubernetes/cli-runtime/blob/master/pkg/genericclioptions/resource/builder.go">https://github.com/kubernetes/cli-runtime/blob/master/pkg/genericclioptions/resource/builder.go</a>:

  | ...
  | func (b *Builder) Schema(schema validation.Schema) *Builder {
  |     b.schema = schema
  |     return b
  | }
  | ...
  | func (b *Builder) ContinueOnError() *Builder {
  |     b.continueOnError = true
  |     return b
  | }

 The Do function finally returns a Result object that will be used to drive
the creation of our resource. It also creates a Visitor object that can be
used to traverse the list of resources that were associated with this
invocation of resource.NewBuilder. The Do function implementation is shown below.


  a new DecoratedVisitor is created and stored as part of the Result object
that is returned by the Builder Do function. The DecoratedVisitor has a Visit
function that will call the Visitor function that is passed into it.
  |// Visit implements Visitor
  |func (v DecoratedVisitor) Visit(fn VisitorFunc) error {
  |    return v.visitor.Visit(func(info *Info, err error) error {
  |        if err != nil {
  |            return err
  |        }
  |        for i := range v.decorators {
  |            if err := v.decorators[i](info, nil); err != nil {
  |                return err
  |            }
  |        }
  |        return fn(info, nil)
  |    })
  |}


  Create eventually will call the anonymous function that contains the
  createAndRefresh function that will lead to the code making a REST call
  to the API server.

  The createAndRefresh function invokes the Resource NewHelper
  function found in ...helper.go returning a new Helper object:
  | func NewHelper(client RESTClient, mapping *meta.RESTMapping) *Helper {
  |     return ⅋Helper{
  |         Resource:        mapping.Resource,
  |         RESTClient:      client,
  |         Versioner:       mapping.MetadataAccessor,
  |         NamespaceScoped: mapping.Scope.Name() == meta.RESTScopeNameNamespace,
  |     }
  | }

  Finally the Create function iwill invoke a createResource function of the
  Helper Create function.
  The Helper createResource function, will performs the actual REST call to
  the API server to create the resource we defined in our YAML file.


*Compiling and Running Kubernetes*
- we are going to use a special option that informs the Kubernetes build process

$ make WHAT='cmd/kubectl'  # ← compile only kubectl
Test it like:
On terminal 1 boot up local test hack cluster:
$ PATH=$PATH KUBERNETES_PROVIDER=local hack/local-up-cluster.sh
On terminal 2 execute the compiled kubectl:
$ cluster/kubectl.sh create -f nginx_replica_pod.yaml

*Code Learning Tools*
Tools and techniques that can really help accelerate your ability to learn the k8s src:
- Chrome Sourcegraph Plugin:
  provides several advanced IDE features that make it dramatically
  easier to understand Kubernetes Go code when browsing GitHub repositories.
  Ussage:
  - start by looking at an absolutely depressing snippet of code,
    with ton of functions.
  - Hover over each code function with Chrome browser + Sourcegraph extension
    installed:
    It will popup a description of the function, what is passed into it
    and what it returns.
  - It also provides advanced view with the ability to peek into the function
    being invoked.
- Properly formatted print statements:
  fmt.Println("\n createAndRefresh Info = %#v", info)
- Use of a go panic to get desperately needed stack traces:
  | func createAndRefresh(info *resource.Info) error {
  |     fmt.Println("\n createAndRefresh Info = %#v", info)
  |     *panic("Want Stack Trace")*
  |     obj, err := resource.NewHelper(info.Client, info.Mapping).Create(info.Namespace, true, info.Object)
  |     if err != nil {
  |         return err
  |     }
  |     info.Refresh(obj, true)
  |     return nil
  | }
- GitHub Blame to travel back in time:
  "What was the person thinking when they committed those lines of code?"
  - GitHub browser interface has a blame option available as a button on the user interface:
      It returns a view of the code that has the commits responsible for each line of code
    in the source file. This allows you to go back in time and look at the commit that added
    a particular line of code and determine what the developer was trying to accomplish when
    that line of code was added.
</pre>
</div>

<div groupv>
<span title>SDNs</span>

<pre zoom>
<span xsmall>getting started</span>
@[https://www.redhat.com/sysadmin/getting-started-sdn]
- provide sdns: allows admin teams to control network traffic in
      complex networking topologies through a centralized panel, 
      rather than handling each network device, such as routers
      and switches, manually ("hierarchical" topology)

*container network interface (cni):*
- library definition and a set of tools to configure network 
  interfaces in linux containers through many supported plugins.
- multiple plugins can run at the same time in a container
  that participates in a network driven by different plugins.
- networks use json configuration files and instantiated as
  new namespaces when the cni plugin is invoked.

- common cni plugins include:
  -*calico*:
    - high scalability.
    @[https://docs.projectcalico.org/v3.7/getting-started/kubernetes/]
    @[https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/calico-network-policy/]
  -*cilium*:
    - provides network connectivity and load balancing 
      between application workloads, such as application 
      containers and processes, and ensures transparent security.
  -*contiv*:
    - integrates containers, virtualization, and physical servers
      based on the container network using a single networking fabric.
  -*contrail*:
    - provides overlay networking for multi-cloud and
      hybrid cloud through network policy enforcement.
  -*flannel*:
    - makes it easier for developers to configure a layer 3
      network fabric for kubernetes.
  -*multus*:
    - supports multiple network interfaces in a single pod on
      kubernetes for sriov, sriov-dpdk, ovs-dpdk, and vpp workloads.
  -*open vswitch (ovs)*:
    - production-grade cni platform with a standard management
      interface on openshift and openstack.
  -*ovn-kubernetes*:
    - enables virtual networks for multiple containers on different
      hosts using an overlay function.
  -*romana*:
    - makes cloud network functions less expensive to build, 
      easier to operate, and better performing than traditional
      cloud networks.

- in addition to network namespaces, an sdn should increase security by 
offering isolation between multiple namespaces with the multi-tenant plugin: 
  packets from one namespace, by default, will not be visible to 
  other namespaces, so containers from different namespaces cannot 
  send packets to or receive packets from pods and services of a
  different namespace.
</pre>

<pre zoom>
<span todo xsmall>network policy providers</span>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/cilium-network-policy/">use cilium for networkpolicy</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/kube-router-network-policy/">use kube-router for networkpolicy</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/romana-network-policy/">romana for networkpolicy</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/network-policy-provider/weave-network-policy/">weave net for networkpolicy</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-api/">access clusters using the kubernetes api</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/access-cluster-services/">access services running on clusters</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/extended-resource-node/">advertise extended resources for a node</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/dns-horizontal-autoscaling/">autoscale the dns service in a cluster</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/change-pv-reclaim-policy/">change the reclaim policy of a persistentvolume</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/change-default-storage-class/">change the default storageclass</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/cluster-management/">cluster management</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-multiple-schedulers/">configure multiple schedulers</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/out-of-resource/">configure out of resource handling</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/quota-api-object/">configure quotas for api objects</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies/">control cpu management policies on the node</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/dns-custom-nameservers/">customizing dns service</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/">debugging dns resolution</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/declare-network-policy/">declare network policy</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/developing-cloud-controller-manager/">developing cloud controller manager</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/encrypt-data/">encrypting secret data at rest</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/">guaranteed scheduling for critical add-on pods</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/ip-masq-agent/">ip masquerade agent user guide</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/running-cloud-controller/">kubernetes cloud controller manager</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/limit-storage-consumption/">limit storage consumption</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/configure-upgrade-etcd/">operating etcd clusters for kubernetes</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/">reconfigure a node's kubelet in a live cluster</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/reserve-compute-resources/">reserve compute resources for system daemons</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/safely-drain-node/">safely drain a node while respecting application slos</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/securing-a-cluster/">securing a cluster</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/">set kubelet parameters via a config file</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/highly-available-master/">set up high-availability kubernetes masters</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/static-pod/">static pods</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/storage-object-in-use-protection/">storage object in use protection</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/coredns/">using coredns for service discovery</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/">using a kms provider for data encryption</a>
<a href=https://kubernetes.io/docs/tasks/administer-cluster/sysctl-cluster/">using sysctls in a kubernetes cluster</a>
</pre>

<span title>service mesh</span>
<pre zoom labels="">
<span xsmall TODO>istio.io</span>
</pre>
<pre zoom labels="">
<span xsmall>linkerd</span>
</pre>
  
</div>
</body>
</html>
<!--
todo_start
__________________
<a href="https://banzaicloud.com/blog/kafka-on-kubernetes/">ref</a>

local persistent volume (k8s 1.10+) leverage local disks and it enables to use
them with persistent volume claims, pvc.
*this type of storage is suitable for applications which handles the data*
* replication themself. (kafka, p2p, hdfs, cassandra, ...)*
"""... there are some systems we use or deploy for our customers which are
already handles replications - like hdfs, cassandra, etc - and local persistent
volume is a good fit for that.

to use this as a persistent volume, we have some manual steps to do:

    Pre-partition, format and mount disks to nodes

    Create Persistent Volumes
        Manually
        Using a DaemonSet which handles the creation.

    Create Storage Class

    In later Kubernetes release these steps will be omitted.

You may ask, Kubernetes already has a feature hostPath which allows to use local disk as a storage for Pods, why should I use Local Persistent Volume instead? In case of hostPath the storage path has to be set inside the Pod descriptor. On the other hand when a Local Persistent Volume is used, a storage can be preserved through Persistent Volume Claim, so the storage path is not encoded directly in the Pod spec.
______________________
https://docs.containership.io/kubernetes/overview/kubernetes-api
_____________________
https://banzaicloud.com/blog/multi-cloud-k8s/
___________________
https://github.com/banzaicloud/pipeline
___________________
https://www.infoq.com/news/2018/10/kubernetes-1-12

Kubernetes 1.12 Brings Volume Snapshots, TLS Improvements, and More
___________________________
k8s on a Pine64 ARM (4GB RAM)
https://itnext.io/building-an-arm-kubernetes-cluster-ef31032636f9
_________________
https://github.com/ovh/svfs
__________________
Secrets:
<a TODO href="https://kubernetes.io/docs/concepts/configuration/secret/">Secrets</a>
<a href=https://kubernetes.io/docs/tasks/inject-data-application/distribute-credentials-secure/">Distribute Credentials Securely Using Secrets</a>
    Deployments
__________________
CRDs (Custom Resource Definition)
https://kubernetes.io/blog/page/17/
KubeDirector: The easy way to run complex stateful applications on Kubernetes
 open source project designed to make it easy to run complex stateful scale-out application clusters on Kubernetes. KubeDirector is built using the custom resource definition (CRD) framework and leverages the native Kubernetes API extensions and design philosophy. This enables transparent integration with Kubernetes user/resource management as well as existing clients and tools.
_____________________
https://kubernetes.io/blog/page/12/
Topology-Aware Volume Provisioning in Kubernetes

The multi-zone cluster experience with persistent volumes is improving in Kubernetes 1.12 with the topology-aware dynamic provisioning beta feature. This feature allows Kubernetes to make intelligent decisions when dynamically provisioning volumes by getting scheduler input on the best place to provision a volume for a pod. In multi-zone clusters, this means that volumes will get provisioned in an appropriate zone that can run your pod, allowing you to easily deploy and scale your stateful workloads across failure domains to provide high availability and fault tolerance.
_____________________
https://kubernetes.io/blog/2018/07/09/ipvs-based-in-cluster-load-balancing-deep-dive/
IPVS-Based In-Cluster Load Balancing Deep Dive

What Is IPVS?

IPVS (IP Virtual Server) is built on top of the Netfilter and implements transport-layer load balancing as part of the Linux kernel.

IPVS is incorporated into the LVS (Linux Virtual Server), where it runs on a host and acts as a load balancer in front of a cluster of real servers. IPVS can direct requests for TCP- and UDP-based services to the real servers, and make services of the real servers appear as virtual services on a single IP address. Therefore, IPVS naturally supports Kubernetes Service.
_____________________________
https://www.infoq.com/news/2019/01/kubecon-cloudnativecon-2018
_________________________
udits log events (1.11+)
_________________________________
Related k8s must-known
 - Overlay networks (I wrote a container networking overview about this last year)
 - Network namespaces (understanding namespaces in general is really helpful for working with containers)
 - DNS (because Kubernetes has a DNS server)
 - Route tables, how to run ip route list and ip link list
 - Network interfaces
 - Encapsulation (vxlan / UDP)
 - Basics about how to use iptables & read iptables configuration
 - TLS, server certs, client certs, certificate authorities
_____________________
https://sysdig.com/blog/kubernetes-security-rbac-tls/
__________________________
https://www.cncf.io/blog/2018/11/05/34097/
_____________________
https://es.slideshare.net/try_except_/kubernetes-on-aws-at-zalando-failures-learnings-devops-nrw
________________________
k8s plugin for IntelliJ:
https://blog.jetbrains.com/idea/2018/03/intellij-idea-2018-1-kubernetes-support/
_____________
https://www.zdnet.com/article/red-hat-introduces-first-kubernetes-native-ide/?ftag=TRE-03-10aaa6b&bhid=28374205867001011904732094012637
______________________
Azure Kubernetes AKS automatic deployment with Terraform:
https://github.com/jdeiviz/aks-terraform-module
_______________________
https://www.infoq.com/articles/ambassador-api-gateway-kubernetes
_______________________

Advanced Kubernetes in Practice (O'Reilly Course:, Viktor Farcic)

Break : 10 mins

Section 3: Dividing A Cluster Into Namespaces (50 mins)

Applications and corresponding objects often need to be separated from each other to avoid conflicts and other undesired effects.

Break : 10 mins

Section 4: Securing Kubernetes Clusters (1 hour)

Security implementation is a game between a team with a total lock-down strategy and a team that plans to win by providing complete freedom to everyone. You can think of it as a battle between anarchists and totalitarians. The only way the game can be won is if both blend into something new. The only viable strategy is freedom without sacrificing security (too much).

DAY 2

Section 5: Managing Resources (1 hour)

Without an indication how much CPU and memory a container needs, Kubernetes has no other option than to treat all containers equally. That often produces a very uneven distribution of resource usage. Asking Kubernetes to schedule containers without resource specifications is like entering a taxi driven by a blind person.

Break : 10 mins

Section 6: Persisting State (1 hour)

Having fault-tolerance and high-availability is of no use if we lose application state during rescheduling. Having state is unavoidable, and we need to preserve it no matter what happens to our applications, servers, or even a whole datacenter.

Break : 10 mins

Section 7: Deploying Stateful Applications At Scale (1 hour)

Stateless and stateful application are quite different in their architecture. Those differences need to be reflected in Kubernetes as well. The fact that we can use Deployments with PersistentVolumes does not mean that is the best way to run stateful applications.
_____________________________
Hybric Cloud+On-Premise k8s
https://cloud.google.com/blog/products/gcp/going-hybrid-with-kubernetes-on-google-cloud-platform-and-nutanix
____________________
https://cloud.google.com/knative/
Kubernetes-based platform to build, deploy, and manage modern serverless workloads.
Essential base primitives for all

Knative provides a set of middleware components that are essential to build modern, source-centric, and container-based applications that can run anywhere: on premises, in the cloud, or even in a third-party data center. Knative components are built on Kubernetes and codify the best practices shared by successful real-world Kubernetes-based frameworks. It enables developers to focus just on writing interesting code, without worrying about the “boring but difficult” parts of building, deploying, and managing an application.
Developer-friendly software

Knative offers a set of reusable components that focuses on solving many mundane but difficult tasks such as orchestrating source-to-container workflows, routing and managing traffic during deployment, auto-scaling your workloads, or binding running services to eventing ecosystems. Developers can even use familiar idioms, languages, and frameworks to deploy any workload: functions, applications, or containers.
Supports popular development patterns

Knative focuses on an idiomatic developer experience. It supports common development patterns such as GitOps, DockerOps, ManualOps, as well as tools and frameworks such as Django, Ruby on Rails, Spring, and many more.
Best of both worlds: Flexibility and control

Knative is designed to plug easily into existing build and CI/CD toolchains. By focusing on open-source-first technologies that run anywhere, on any cloud, on any infrastructure supported by Kubernetes, enterprises are free to move their workloads wherever they run best. This offers the flexibility and control customers need to adapt the system to their own unique requirements.
Operator-friendly

Knative is designed to be run as a service by all major cloud providers. Google currently works with industry leaders such as Pivotal, SAP, Red Hat, IBM, and many others to create the building blocks that will best suit the needs of developers. Knative powers real-world workloads and is also compatible with other cutting-edge technologies such as Kubernetes and Istio.
Run your serverless workloads on Kubernetes Engine

You can now run your serverless workloads on Google Kubernetes Engine (GKE) by enabling the serverless add-on. Powered by Knative, the serverless add-on helps developers orchestrate builds, serving, and events with a single click, enabling the benefits of an idiomatic developer experience with the flexibility and control of GKE.

______________________________
https://www.itprotoday.com/containers/rancher-labs-k3s-shrinks-kubernetes-edge

Rancher Labs' K3s Shrinks Kubernetes for the Edge
For running containers at the edge, Rancher Labs has created K3s, a Kubernetes distribution that weighs-in at 40MB and needs only 512MB RAM to run.
_____________________________
https://www.datacenterknowledge.com/open-source/aws-google-microsoft-red-hats-new-registry-act-clearing-house-kubernetes-operators
AWS, Google, Microsoft, Red Hat's New Registry to Act as Clearing House for Kubernetes Operators
Operators make life easier for Kubernetes users, but they're so popular that finding good ones is not easy. Operatorhub.io is an attempt to fix that.
_____________________________________
https://github.com/gluster/gluster-kubernetes
gluster-kubernetes is a project to provide Kubernetes administrators a
mechanism to easily deploy GlusterFS as a native storage service onto an
existing Kubernetes cluster. Here, GlusterFS is managed and orchestrated like
any other app in Kubernetes. This is a convenient way to unlock the power of
dynamically provisioned, persistent GlusterFS volumes in Kubernetes.
________________________________
https://www.infoq.com/news/2019/03/airbnb-kubernetes-workflow

________________________________
Kubernetes top questions by votes in ServerFault:
https://serverfault.com/questions/tagged/kubernetes?sort=votes&pageSize=15

Kubernetes top questions by votes in DevOps.stackexchange.com:
https://devops.stackexchange.com/questions/tagged/kubernetes?sort=votes&pageSize=15
_______________________________
https://www.infoq.com/news/2019/03/rancher-submariner-multicluster
_________________________
https://www.infoq.com/news/2019/03/redhat-release-quarkus
_________________________
proper shutdown k8s cluster
https://serverfault.com/questions/893886/proper-shutdown-of-a-kubernetes-cluster
_________________________
The K8s Bible for Beginners and developers:
https://docs.google.com/document/d/1O-BwDTuE4qI0ASE7iFp6qFpTj8uIVrl9F0HUrC4u_GQ/edit
__________________
https://opensource.com/article/19/3/getting-started-jaeger
_____________________
https://www.infoq.com/news/2019/04/kubernetes-pci-dss-compliance
_____________________
C&P from JBCNConf 2019:
Zero Downtime Migrations in Istio era
(talk)

You joined the DevOps movement and want to release software even faster and safer. You started reading about Advanced deployment techniques like Blue-Green Deployment, Canary Releases or Dark Shadow Technique. But how do you implement them without disrupting your users in production? With Zero Downtime! This is easy with your code, but what about ephemeral and persistent states? Most of the examples out there does not tackle this scenario (which is the most common in real systems). Come to this session and you’ll learn in a practical way how you can achieve zero downtime deployments applying advanced deployment techniques for maintaining the ephemeral and persistent state with Istio
Alex Soto
Java Champion, Engineer @ Red Hat. Speaker, CoAuthor of Testing Java Microservices book, Member of JSR374 and Java advocate

Alex is a Software Engineer at Red Hat in Developers group. He is a passionate about Java world, software automation and he believes in the opensource software model. Alex is the creator of NoSQLUnit project, member of JSR374 (Java API for JSON Processing) Expert Group, the co-author of Testing Java Microservices book for Manning and contributor of several open source projects. A Java Champion since 2017 and international speaker, he has talked about new testing techniques for microservices and continuous delivery in the 21st century.
_____________________
https://www.infoq.com/news/2019/05/litmus-chaos-engineering-kube/
Chaos Engineering Kubernetes with the Litmus Framework

Litmus:
- open source chaos engineering framework for Kubernetes
  environments runninG*stateful applications*
- Litmus can be added to CI/CD pipelines
- designed to catch hard-to-detect bugs in Kubernetes
  that might be missed by unit or integration tests.
- focus on application resilience:
  - pre-existing tests for undesirable behavior such:
    - container crash
    - disk failure
    - or network delay
    - packet loss.
- can also be used to determine if a Kubernetes deployment
  is suitable for stateful workloads.
____________________________
<pre zoom>
<span TODO xsmall>cli reference</span>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/">Feature Gates</a>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/cloud-controller-manager/">cloud-controller-manager</a>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-apiserver/">kube-apiserver</a>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">kube-controller-manager</a>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-scheduler/">kube-scheduler</a>
<a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/">kubelet</a>
</pre>
__________________________________
<a href="https://kubernetes.io/docs/concepts/architecture/master-node-communication/">Master-Node communication</a>
____________________________
You can call kubectl get pod with the -o go-template=... option to fetch the status of previously terminated Containers:
kubectl get pod -o go-template='{{range.status.containerStatuses}}{{"Container Name: "}}{{.name}}{{"\r\nLastState: "}}{{.lastState}}{{end}}'  simmemleak-hra99
Container Name: simmemleak
LastState: map[terminated:map[exitCode:137 reason:OOM Killed startedAt:2015-07-07T20:58:43Z finishedAt:2015-07-07T20:58:43Z containerID:docker://0e4095bba1feccdfe7ef9fb6ebffe972b4b14285d5acdec6f0d3ae8a22fad8b2]]
___________________________
https://www.youtube.com/channel/UCvqbFHwN-nwalWPjPUKpvTA
_____________________________
List all pods and its nodes:
https://stackoverflow.com/questions/48983354/kubernetes-list-all-pods-and-its-nodes/48983984#48983984

_______________________
Kubermatic: https://www.loodse.com/
_________________
python + kubernetes api
https://www.redhat.com/sysadmin/create-kubernetes-cron-job-okd
_____________________
https://12factor.net/

__________________________
k8s cli utilities:
https://developers.redhat.com/blog/2019/05/27/command-line-tools-for-kubernetes-kubectl-stern-kubectx-kubens/
-*stern*
 - display the tail end of logs for containers and multiple pods.
 - the stern project comes from Wercker (acquired by Oracle in 2017).
 - rather than viewing an entire log to see what happened most
   recently, you can use stern to watch a log unfurl.

-*kubectx*
 - helpful for multi-cluster installations, where you need to
   switch context between one cluster and another. 
 - Rather than type a series of lengthy kubectl command, kubectx 
   works it magic in one short command.
 - It also allows you to alias a lengthy cluster name into an alias.
 - For example (taken directly from the kubectx website), 
   kubectx eu=gke_ahmetb-samples-playground_europe-west1-b_dublin allows
   you to switch to that cluster by running kubectx eu. 
 - Another slick trick is that kubectx remembers your previous context—
   much like the “Previous” button on a television remote—and allows
   you to switch back by running kubectx -.

-*kubens*
 - easily switch between Kubernetes namespaces.
   $ kubens foo # activate namespace. 
   $ kubens -   # back to previous value.
 - Author: Ahmet Alp Balkan
_____________________________
https://portworx.com/
calable Persistent Storage for Kubernetes

Built from the ground up for containers, PX-Store provides cloud native storage for applications running in the cloud, on-prem and in hybrid/multi-cloud environments.
____________________________
https://www.tigera.io/
(authors of the Calico Zero-Trust Network with Policy-based micro-segmentation)
https://www.projectcalico.org/, 
Why add another layer of overhead when you don't need it?

Sometimes, an overlay network (encapsulating packets inside an extra IP header) is necessary. Often, though, it just adds unnecessary overhead, resulting in multiple layers of nested packets, impacting performance and complicating trouble-shooting. Wouldn't it be nice if your virtual networking solution adapted to the underlying infrastructure, using an overlay only when required? That's what Calico does. In most environments, Calico simply routes packets from the workload onto the underlying IP network without any extra headers. Where an overlay is needed – for example when crossing availability zone boundaries in public cloud – it can use lightweight encapsulation including IP-in-IP and VxLAN. Project Calico even supports both IPv4 and IPv6 networks!
______________________
https://www.kubeflow.org/
The Machine Learning Toolkit for Kubernetes
______________________________
https://k3s.io/
Rancher Labs Lightweight Kubernetes:
- Easy to install. A binary of less than 40 MB, Only 512MB of RAM required

_____________________________
https://github.com/cloudnativelabs/kube-router
______________________
https://github.com/iovisor/kubectl-trace
Schedule bpftrace programs on your kubernetes cluster using the kubectl
_________________________
https://github.com/kubeless/kubeless
Kubernetes Native Serverless Framework https://kubeless.io

___________________________
Automatically sync groups into Kubernetes RBAC
@[https://github.com/cruise-automation/rbacsync]

@[https://www.eweek.com/cloud/how-gm-s-cruise-autonomous-vehicle-effort-is-improving-kubernetes]
_______________
https://rook.io/
https://www.infoq.com/news/2019/08/rook-v1-release/

Rook, a storage orchestrator for Kubernetes, has released version 1.0 for production-ready workloads that use file, block, and object storage in containers. Highlights of Rook 1.0 include support for storage providers through operators like Ceph Nautilus, EdgeFS, and NFS. For instance, when a pod requests an NFS file system, Rook can provision it without any manual intervention.

Rook was the first storage project accepted into the Cloud Native Computing Foundation (CNCF), and it helps storage administrators to automate everyday tasks like provisioning, configuration, disaster recovery, deployment, and upgrading storage providers. Rook turns a distributed file system into storage services that scale and heal automatically by leveraging the Kubernetes features with the operator pattern. When administrators use Rook with a storage provider like Ceph, they only have to worry about declaring the desired state of the cluster and the operator will be responsible for setting up and configuring the storage layer in the cluster.


-->
