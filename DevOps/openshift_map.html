<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
<title>OpenShift map (beta) <!-- ignore --></title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>

<div groupv>
<pre zoom>
<span xsmall>OC Client releases</span>
@[https://github.com/openshift/origin/releases"]
- <a bgorange href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#core-concepts">Core Concepts</a>
- <a bgorange href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/"           >Dev.Guide    </a>
</pre>


<pre zoom>
<span xsmall>External Links</span>
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-new-app" >Create new applications</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-projects" >Monitor and configure projects</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates" >Generate configurations using templates</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-how-builds-work" >Manage builds, including build strategy options and webhooks</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-how-deployments-work" >Define deployments, including deployment strategies</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-routes" >Create and manage routes</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-secrets" >Create and configure secrets</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-integrating-external-services" >Integrate external services, such as databases and SaaS endpoints</a> 
-  <a  href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-application-health" >Check application health using probes</a> 
</pre>

<pre zoom>
<span xsmall>"ACRONYMS"</span>
OPC:  OpenShift Container Platform
App:  (Application). In the context of OPC is the full set
      of processes/services used to have a functional application.
      For example an app can be:
        - 1 pod  running Postgresql
        - 3 pods running FrontEnd
        - 1 pod  running Cache
      In practice the App is described by the DeploymentConfig (dc/) 
      object created during the 'oc new-app' phase.
S2I: Source to Image: One of the possible Build Strategies
SDN: Software defined network

  bc/  == Build Configuration
  dc/  == Deployment configuration
  pod/ == (Running?) pod
</pre>

<pre zoom>
<span xsmall>API Objects "INPUTS&amp;OUTPUTS"</span>
API objects are designed such that there are portions of
the object which specify the desired state of the system,
and other portions which reflect the status or current
state of the system.
ºThis can be thought of as inputs and outputs.º
 The input portions, when expressed in JSON or YAML,
are items that fit naturally as source control managed
(SCM) artifacts. (App. configuration as code)

WARN: API object specifications should be captured with 
'oc export':
 This operation removes environment specific data from the
object definitions (e.g., current namespace or assigned
IP addresses), allowing them to be recreated in different
environments (unlike oc get operations, which output an
unfiltered state of the object). 
</pre>

<pre zoom>
<span xsmall>Common commands</span>
Add (Readiness) Probe
$ oc set probe dc/instantApp --readiness --get-url=http://:8080/*health*

Get Info (describe) Deployments
$ oc describe dc/instantApp # get list of all deployments made

Get Info of Templates
# list global(loaded by cluster admin) templates
$ oc get templates -n openshift

Get Info (describe) Pods
$ oc get pods # find out instances ºrunningº
              # add filter with: --selector app=hiworld
  NAME                    READY     STATUS      RESTARTS   AGE
  hiworld-1-build      0/1       Completed   0          37m
  hiworld-2-build      0/1       Completed   0          33m
  ...

Get info (Describe) Pod Instance
$ oc describe pod/hiworld-4-cd3y3 # Info about given instance.
  Name:       hiworld-4-cd3y3
  Namespace:  wfproject
  Image(s):   172.30.210.155:5000/wfproject/hiworld@sha256:9c941cb...
  Node:       origin/10.0.2.15Start
  Time: Thu, 17 Mar 2016 11:30:08 +1100
  Labels:     ºapp=ºhiworld,ºdeployment=ºhiworld-4,ºdeploymentconfig=ºhiworld
  Status:     Running

Shell Access to Running Pod
$ oc rsh hiworld-8

Edit Deployment Configuration Manually
$ oc edit dc/instantApp -o json # -o json # ← Edit in JSON format
"spec": {
  "ºstrategyº": {
    "ºtypeº": "Rolling",
    "ºrollingParamsº": {
      "ºpreº": { "failurePolicy": "Abort",
        "execNewPod": { 
          "command": [
            "/usr/bin/echo",
            "RUNNING PRE HOOK"], "containerName": "instantApp" }
      },
      "ºpostº": { "failurePolicy": "Abort",
        "execNewPod": {
          "command": [
             "/usr/bin/echo",
             "RUNNING POST HOOK"
          ],
          "containerName": "instantApp" }
      }
    }
},...
</pre>


<pre bgorange zoom>
<span xsmall>Troubleshooting</span>
View Cluster Events</span>
$ oc get events 

Review Pod Logs
$ oc logs hiworld-5-cqlo4 (-f)

Log into running pod:
$ oc rsh "pod_id"

Check global project status:
$ oc status
</pre>


<pre zoom>
<span xsmall>Environement Variables</span>
# set/edit env.var for DC
$ oc set env dc/hiworld MYSQL_DATABASE=mysql

# set/edit env.var for BC
$ oc set env bc/hiworld MAVEN_ARGS_APPEND='-Dmaven.test.skip=true'

# list set of env.vars (that will be) exported to a container
$ oc set env dc/hiworld --list 
  # deploymentconfigs hiworld, container hiworld
  PATH=/opt/app-root/src/bin:...
  STI_SCRIPTS_URL=image:///usr/libexec/s2i
  ...
</pre>

<pre zoom bgorange>
<span xsmall>oc man</span>
$ oc ˂command˃ --help
$ oc options" for a list of global command-line options (applies to all commands).

ºBasic Commands:º
  types           An introduction to concepts and types
  login           Log in to a server
  new-project     Request a new project (new k8s namespace "sub-cluster")
  new-app         Create a new application
  status          Show an ºoverview of the current projectº
  project         Switch to another project
  projects        Display existing projects
  explain         Documentation of resources
  cluster         Start and stop OpenShift cluster

ºBuild and Deploy Commands:º
  rollout         Manage a Kubernetes deployment or OpenShift deployment config
  deploy          View, start, cancel, or retry a deployment
  rollback        Revert part of an application back to a previous deployment
  new-build       Create a new build configuration
  start-build     Start a new build
  cancel-build    Cancel running, pending, or new builds
  import-image    Imports images from a Docker registry
  tag             Tag existing images into image streams

ºApplication Management Commands:º
  get             Display one or many resources
                  -o : shows also Runtime associated info
  describe        Show details of a specific resource or group of resources
  edit            Edit a resource on the server
  set             Commands that help set specific features on objects
  label           Update the labels on a resource
  annotate        Update the annotations on a resource
  expose          ºExposeº a replicated application as a ºservice or routeº
  delete          Delete one or more resources
  scale           Change the number of pods in a deployment
  autoscale       Autoscale a deployment config, deployment, replication controller, or replica set
  secrets         Manage secrets
  serviceaccounts Manage service accounts in your project

ºTroubleshooting and Debugging Commands:º
  logs            Print the logs for a resource
  rsh             Start a shell session in a pod
  rsync           Copy files between local filesystem and a pod
  port-forward    Forward one or more local ports to a pod
  debug           Launch a new instance of a pod for debugging
  exec            Execute a command in a container
  proxy           Run a proxy to the Kubernetes API server
  attach          Attach to a running container
  run             Run a particular image on the cluster
  cp              Copy files and directories to and from containers.

ºAdvanced Commands:º
  adm             Tools for managing a cluster
  create          Create a resource by filename or stdin
  replace         Replace a resource by filename or stdin
  apply           Apply a configuration to a resource by filename or stdin
                  See usage for (dev|pre|pro|...)stage change <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-promoting-applications-methods-tools">here</a>
  patch           Update field(s) of a resource using strategic merge patch
  process         Process a template into list of resources
  export          Export resources so they can be used elsewhere
  extract         Extract secrets or config maps to disk
  idle            Idle scalable resources
  observe         Observe changes to resources and react to them (experimental)
  policy          Manage authorization policy
  convert         Convert config files between different API versions
  import          Commands that import applications

ºSettings Commands:º
  logout          End the current server session
  config          Change configuration files for the client
  whoami          Return information about the current session
  completion      Output shell completion code for the given shell (bash or zsh)
</pre>

<pre zoom bgorange>
<span xsmall>oc types</span>
(output of 'oc types')
Concepts and Types 
Kubernetes and OpenShift help developers and operators build, test, and deploy
applications in a containerized cloud environment. 
Applications may be composed of all of the components below, although most
developers will be concerned with Services, Deployments, and Builds for 
delivering changes. 

Concepts: 

* Containers:
    A definition of how to run one or more processes inside of a portable Linux
    environment. Containers are started from an Image and are usually isolated
    from other containers on the same machine.
    
* Image:
    A layered Linux filesystem that contains application code, dependencies,
    and any supporting operating system libraries. An image is identified by
    a name that can be local to the current cluster or point to a remote Docker
    registry (a storage server for images).
    
* Pods [pod]:
    A set of one or more containers that are deployed onto a Node together and
    share a unique IP and Volumes (persistent storage). Pods also define the
    security and runtime policy for each container.
    
* Labels:
    Labels are key value pairs that can be assigned to any resource in the
    system for grouping and selection. Many resources use labels to identify
    sets of other resources.
    
* Volumes:
    Containers are not persistent by default - on restart their contents are
    cleared. Volumes are mounted filesystems available to Pods and their
    containers which may be backed by a number of host-local or network
    attached storage endpoints. The simplest volume type is EmptyDir, which
    is a temporary directory on a single machine. Administrators may also
    allow you to request a Persistent Volume that is automatically attached
    to your pods.
    
* Nodes [node]:
    Machines set up in the cluster to run containers. Usually managed
    by administrators and not by end users.
    
* Services [svc]:
    A name representing a set of pods (or external servers) that are
    accessed by other pods. The service gets an IP and a DNS name, and can be
    exposed externally to the cluster via a port or a Route. It's also easy
    to consume services from pods because an environment variable with the
    name <SERVICE>_HOST is automatically injected into other pods.
    
* Routes [route]:
    A route is an external DNS entry (either a top level domain or a
    dynamically allocated name) that is created to point to a service so that
    it can be accessed outside the cluster. The administrator may configure
    one or more Routers to handle those routes, typically through an Apache
    or HAProxy load balancer / proxy.
    
* Replication Controllers [rc]:
    A replication controller maintains a specific number of pods based on a
    template that match a set of labels. If pods are deleted (because the
    node they run on is taken out of service) the controller creates a new
    copy of that pod. A replication controller is most commonly used to
    represent a single deployment of part of an application based on a
    built image.
    
* Deployment Configuration [dc]:
    Defines the template for a pod and manages deploying new images or
    configuration changes whenever those change. A single deployment
    configuration is usually analogous to a single micro-service. Can support
    many different deployment patterns, including full restart, customizable
    rolling updates, and fully custom behaviors, as well as pre- and post-
    hooks. Each deployment is represented as a replication controller.
    
* Build Configuration [bc]:
    Contains a description of how to build source code and a base image into a
    new image - the primary method for delivering changes to your application.
    Builds can be source based and use builder images for common languages like
    Java, PHP, Ruby, or Python, or be Docker based and create builds from a
    Dockerfile. Each build configuration has web-hooks and can be triggered
    automatically by changes to their base images.
    
* Builds [build]:
    Builds create a new image from source code, other images, Dockerfiles, or
    binary input. A build is run inside of a container and has the same
    restrictions normal pods have. A build usually results in an image pushed
    to a Docker registry, but you can also choose to run a post-build test that
    does not push an image.
    
* Image Streams and Image Stream Tags [is,istag]:
    An image stream groups sets of related images under tags - analogous to a
    branch in a source code repository. Each image stream may have one or
    more tags (the default tag is called "latest") and those tags may point
    at external Docker registries, at other tags in the same stream, or be
    controlled to directly point at known images. In addition, images can be
    pushed to an image stream tag directly via the integrated Docker
    registry.
    
* Secrets [secret]:
    The secret resource can hold text or binary secrets for delivery into
    your pods. By default, every container is given a single secret which
    contains a token for accessing the API (with limited privileges) at
    /var/run/secrets/kubernetes.io/serviceaccount. You can create new
    secrets and mount them in your own pods, as well as reference secrets
    from builds (for connecting to remote servers) or use them to import
    remote images into an image stream.
    
* Projects [project]:
    All of the above resources (except Nodes) exist inside of a project.
    Projects have a list of members and their roles, like viewer, editor,
    or admin, as well as a set of security controls on the running pods, and
    limits on how many resources the project can use. The names of each
    resource are unique within a project. Developers may request projects
    be created, but administrators control the resources allocated to
    projects.
    
For more, see https://docs.openshift.com

Usage:
  oc types [flags]

Examples:
  # View all projects you have access to
  oc get projects
  
  # See a list of all services in the current project
  oc get svc
  
  # Describe a deployment configuration in detail
  oc describe dc mydeploymentconfig
  
  # Show the images tagged into an image stream
  oc describe is ruby-centos7

Use "oc options" for a list of global command-line options (applies to all commands).
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Builds and image streams</span>
https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-core-concepts-builds-and-image-streams
BUILD PROCESS  (REF: <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-how-builds-work">Build</a>@Dev.Guide)

ºA build configuration describes:  º
º-  build definition               º
º-  set of triggers for when       º
º   a new build should be created. º

------------------------+-----------------------------------+-------------------------
<a href=https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#how-build-inputs-work">INPUT(@BuildConfig.spec)</a>→         BUILD                     →      OUTPUT
------------------------+-----------------------------------+-------------------------
(1) Git                 | - BuildConfig                     | - Container Image (pushed into Im.Reg specified in BuildConfig.output)
(2) Dockerfile          |    - BuildStrategy                |                  
(3) Binary              |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#source-build"  >Source-to-Image (S2I) </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#source-to-image-strategy-options">options</a>|                              
(4) Image               |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#pipeline-build">Pipeline              </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#pipeline-strategy-options"       >options</a>|                              
(5) Input secrets       |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#docker-build"  >Docker                </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#docker-strategy-options"         >options</a>|                              
(6) External artifacts  |      <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#custom-build"  >Custom                </a><a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#custom-strategy-options"         >options</a>|                               
------------------------+-----------------------------------+-------------------------
                                                                ↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑↑
                      --------------------------------------------------------------------------------------------------------------------------------------------------------------------
                      BuildConfig.spec.output KIND               | PUSH TO                                         | ENV.VARS. set in Output IMG               | Image Labels set in Output
                     --------------------------------------------+-------------------------------------------------+-------------------------------------------+-------------------------------------
                      to:                                        | OCP registry and tag in the specified IS        | OPENSHIFT_BUILD_NAME                      | io.openshift.build.commit.author    
                        kind: "ImageStreamTag"                                                                     | OPENSHIFT_BUILD_NAMESPACE                 | io.openshift.build.commit.date      
                        name: "sample-image:latest"                                                                | OPENSHIFT_BUILD_SOURCE     (source URL)   | io.openshift.build.commit.id        
                     --------------------------------------------+-----------------------------------------------  | OPENSHIFT_BUILD_REFERENCE  (Git ref used) | io.openshift.build.commit.message   
                      to:                                        | the name of the output reference                | OPENSHIFT_BUILD_COMMIT                    | io.openshift.build.commit.ref       
                        kind: "DockerImage"                      | will be used as a Docker push specification     | +"any user-defined ENV.VAR"               | io.openshift.build.source-location  
                        name: "myReg.comp.com/myimages/myimg:tag"| The spec. may contain a registry or will default|                                           | + custom ...output.imageLabels
                                                                 | to DockerHub                                    |
                     --------------------------------------------+-----------------------------------------------  |
                      "empty"                                    | then built image will not be pushed             |
                     --------------------------------------------+-----------------------------------------------  |



(1)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#source-code">Git</a>
    source code will be fetched from the location supplied.
    (inline supplied Dockerfile overwrites existing one in Git src)
    |source:
    |  git: 1
    |    uri: "https://github.com/openshift/ruby-hello-world"
    |    ref: "master"
    |  contextDir: "app/dir" 2
    |  dockerfile: "FROM openshift/ruby-22-centos7\nUSER example

(2)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dockerfile-source"         >Dockerfile</a>
(3)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#binary-source"             >Binary</a>
(4)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#image-source"              >Image</a>
(5)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#using-secrets-during-build">Input secrets</a>
(6)<a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#using-external-artifacts"  >External artifacts</a>
</pre>

<pre zoom>
<span xsmall>Build manual control</span>
$ oc start-build 'buildconfig_name' \   ← Manually start new build from existing bc
   --follow  \                          ← stream build's logs to stdout
   --env ENVAR01=value01                ← set environment variable for the build

$ oc start-build --from-build=$build_name    ← Re-run build

$ oc cancel-build 'build1_name' 'build2_name' ... ← opt: use --state=$state to cancel only in given state

$ oc delete bc 'BuildConfigName'
</pre>

<pre zoom>
<span xsmall>View Build Details</span>
$ oc describe build 'build_name'
→ build strategy
→ output destination
→ Digest of the image in the destination registry
→ How the build was created 
→ source revision (ID, author, committer, and message) (Source and Docker strategy) 

$ oc logs -f build/'build_name'      ← stream logs 
$ oc logs -f bc/'buildconfig_name'   ← stream logs of latest build
$ oc logs --version=$V bc/$buildconfig_name ← dump logs of a given version build

Log Verbosity:

set  BUILD_LOGLEVEL ENV.VAR as part of the BuildConfig sourceStrategy(|dockerStrategy):
sourceStrategy:
...
  env:
    - name: "BUILD_LOGLEVEL"
      value: "2"

    * Level 0: output from containers running the assemble script and all encountered errors.
      Level 1: basic info about executed process
      Level 2: very detailed information about the executed process.
      Level 3: very detailed information about the executed process plust listing of the archive contents
      Level 4: Currently produces the same information as level 3.
      Level 5: level 4 + docker push messages
</pre>
<pre zoom>
<span xsmall>Example BuildConfig</span>
(Ussually a BuildConfig is automatically created during $ oc new-app ...)
Once the BuildConfig has been "tunned" refer to <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#starting-a-build">Manually control Builds</a> to see how to launch/cancel/monitor build
kind: "BuildConfig"
apiVersion: "v1"
metadata:
  name: "ruby-sample-build"
spec:
  runPolicy: "Serial"                  ← whether builds will run simultaneously 

  triggers:                            ← builds new image each time
    - type: "GitHub"                     source code changes
      github:                            or
        secret: "secret101"              container image tag
    - type: "Generic"
      generic:
        secret: "secret101"
    - type: "ImageChange"

  source: 4                           ← primary source of input (Git URL, Dockerfile, Binary payload). 
    git:                                More than one source is allowed.
      uri: "https://github.com/openshift/ruby-hello-world"

  strategy:                           ← strategy will be: Source | Docker | Custom
    sourceStrategy:                     Here we use ruby-20-centos7 container image that Source-To-Image
      from:                             will use for the application build
        kind: "ImageStreamTag"
        name: "ruby-20-centos7:latest"
  output: 6
    to:
      kind: "ImageStreamTag"
      name: "origin-ruby-sample:latest"
    imageLabels:                      ← Custom labels added to output image/IS
    - name: "vendor"
      value: "MyCompany"
    - name: "authoritative-source-url"
      value: "registry.mycompany.com"

  postCommit: 7
      script: "bundle exec rake test"


Other examples of source types:
  source:
    git:
      uri: https://github.com/openshift/ruby-hello-world.git 1
    images:
    - from:
        kind: ImageStreamTag
        name: myinputimage:latest
        namespace: mynamespace
      paths:
      - destinationDir: app/dir/injected/dir 2            ← .../somefile.jar from myinputimage will
        sourcePath: /usr/lib/somefile.jar                   be stored in <workingdir>/app/dir/injected/dir
    contextDir: "app/dir" 3                               ← work.dir. for the build 
    dockerfile: "FROM centos:7\nRUN yum install -y httpd" ← Overwrite /app/dir Dockerfile with this
</pre>


<pre zoom>
<span xsmall>available Options for different Build Strategies</span>

 |S2I Strategy            | Docker Strategy        | Custom Strategy              | Pipeline Strategy
 |------------------------+------------------------+------------------------------+-----------------------------------
 |- Incremental Builds    | - FROM Image           | - FROM Image                 | - Provide Jenkinsfile
 |- Extended Builds       | - Dockerfile Path      | - Exposing the Docker Socket |   - Embed Jenkinsfile          in build conf.
 |                        | - No Cache             |                              |   - Include a ref. to git repo in build conf. 
 |                        |                        |                              |
 |- Force Pull            | - Force Pull           |                              |
 |- Environment Variables | - Environment Variables| - Environment Variables      |
 |- secrets               | - secrets              | - Secrets                    |
</pre>

<pre zoom>
<span xsmall>ImageStream</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#image-streams]
NOTE: Image Stream tags must not necesarelly match docker registry images
-  Rather than reference (binary container) images directly,
  application definitions typically abstract the reference
  into an image stream offering a layer of indirection.

- Protect against changes when the current image is upgraded.

- Image streams present a single virtual view of related images,
  similar to an image repository.

- Builds&amp;Deployments can be configured to trigger a new
  Build and/or Deployment when new images are added to the IS
</pre>
</div>


<div groupv>

<pre zoom>
<span xsmall>Deployment</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#deployments]

OCP deployments are described through 3 API/objects:
- DeploymentConfig: describing desired state
- Replication controllers: point-in-time record of the state of a deployment configuration
- 1+ pod/s: 

Ex:
  | kind: "DeploymentConfig"
  | apiVersion: "v1"
  | metadata:
  |   name: "frontend"
  | spec:
  |   template: 1              ← pod template
  |     metadata:
  |       labels:
  |         name: "frontend"
  |     spec:
  |       containers:
  |         - name: "helloworld"
  |           image: "openshift/origin-ruby-sample"
  |           ports:
  |             - containerPort: 8080
  |               protocol: "TCP"
  |   replicas: 5             ← 2
  | 
  |   triggers:
  |     - type: "ConfigChange" 3 ← create new replication contro. any time pod-template changes
  |     - type: "ImageChange"  4 ← create new replication contro. any time image        change
  |       imageChangeParams:
  |         automatic: true
  |         containerNames:
  |           - "helloworld"
  |         from:
  |           kind: "ImageStreamTag"
  |           name: "origin-ruby-sample:latest"
  |   strategy:                
  |     type: "Rolling"        5 ← Recreate | Rolling* | Custom 
  |   paused: false            6 ← disables triggers to allows multiple simultaneous changes 
  |   revisionHistoryLimit: 2  7 ← limits the limit of old replication controllers in case of roll-back
  |   minReadySeconds: 0       8 ← Minimum seconds to wait (after readiness succeed) for a pod
  |                                to be considered available. The default value is 0. 
</pre>


<pre zoom>
<span xsmall>Basic Deployment Operations</span>
$ oc deploy --latest dc/'name'  # Starts Deployment

$ oc rollout history dc/'name'  # get basic info about all app's available revisions
                                # It will also show details any currently running deployment
                                # --revision=1: Filter by revision 1

$ oc describe dc 'name'         # detailed info and its latest revision

$ oc rollout undo dc/'name'     # rollback to the last successful deployed revision
                                # --revision=N: Use rev.N instead of last succes.
                                # triggers are automatically disabled to prevent
                                # accidentally starting a new deployment process 
                                # To re-enable the image change triggers:
                                # $ oc set triggers dc/'name' --auto

$ oc logs -f dc/'name'          # stream (-f) logs of latest revision
                                # ERROR Case:  outputs log of process responsible for deploy.
                                # "OK"  Case:  outputs log from app pod/s
                                # --revision=1: See older revision

$ oc scale dc 'name' --replicas=3 # manual scale
$ oc autoscale ...?               # autoscale


Modify (ENTRYPOINT) startup behavior:
  | spec:
  |   containers:
  |     - name: ˂container_name˃
  |       image: 'image'
  |       command:
  |         - 'command...'
  |       args:
  |         - 'arg1'
  |         - ...


ºAssigning Pod/Node affinityº
PRE-SETUP
- OCP Admin assigns labels to nodes

NOTE: OCP cluster admins can set the default node selector for a project 
      restricting pod deployments to specific nodes.
      (For example region=east to limit geographical deployment)

STEP 01: Use "po".spec.nodeSelector to filter by node labels
apiVersion: v1
kind: Pod
spec:
  nodeSelector:   
    disktype: ssd
...

ºRunning a Pod with a Different Service Accountº
$ oc edit dc/$DEPLOYMENT_CONFIG
    spec:
      securityContext: {}
      serviceAccount: 'service_account'      ← edit
      serviceAccountName: 'service_account'  ← edit

</pre>

<pre zoom>
<span xsmall>k8s Deployments Support</span>

In the upstream Kubernetes project, a new first-class object 
type called deployments was added in version 1.2. 
This object type (referred to here as Kubernetes deployments
for distinction) serves as a descendant of the deployment
configuration object type.

Support for Kubernetes deployments is available as a
Technology Preview feature.
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Replication Controllers</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#replication-controllers]
</pre>
  
<pre zoom>
<span xsmall>Routes</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-core-concepts-routes]
Unique object defined 1 to 1 by service name (vs label selectors)
Options:
 - Edge Termination:
 - Pass-through Termination:
 - Re-encryption Termination:

App promotion:
Routes are the most typical resources that differ stage to stage
in the promotion pipeline, as tests against different stages of
an application access that application via its Route. 
Also, remember that you have options with regard to manual 
specification or auto-generation of host names, as well as
the HTTP-level security of the Route. 
</pre>

<pre zoom>
<span xsmall>Services</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#services]
 Provides fixed IP to access (indirectly) to pods (pods IPs change at redeploy)
 Uses selectos to choose target pods.
 If more than one pod is found "round-roubing" is used.

 3 Services types:
  - (IPTables) HostPort/HosNetwork:  host/port in the "Host" (cluster visible)
  - (IPTables) NodePort: all nodes (included master) will listen in port "X" 
    redirecting to pod
  - (HAProxy?) Routes: "completelly virtual", uses unique URL.
     Can also use session affinity (sticky session based on cookies, ...)
</pre>

<pre zoom>
<span xsmall>Scaling</span>
$ oc scale dc hello --replicas 5

$ oc rollout latest hello

HorizontalPodAutoscaler:
(HPA) Performance metrics must be active
$ oc autoscale dc/myapp \
   --min 1 --max 10 \
   --cpu-percent=80
</pre>

<pre zoom>
<span xsmall>DeploymentConfigs</span>
App (dev|pre|pro|...) Promotion:
- primary resource for defining and scoping the environment
  for a given promotion pipeline (dev|pre|canary|pro|...) stage 
- It controls how the app tarts up.
- there will ussually be many modifications to this object
   as it changes stage.
</pre>

<pre zoom>
<span xsmall>(OCP External) Endpoints</span>
  Certain application-level services (e.g., database instances 
in many enterprises) may not be managed by OpenShift Container Platform.
.
.
.
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Templates</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-templates]
"Normal" Flow:
Create app → tune app → export to template → parameterize template

Template = [BuildConfig set, ImageStream set, DeploymentConfig set, 
            Service set, Route set, secret set, ...?]

Template : set of objects that can be parameterized and processed 
           to produce any list of objects you have permission to create
           (services, bc's, dc's).
           - It  may also define a set of labels to apply to every
             object defined in the template.

- A project can contain  a library of templates

$ oc create -f template.json (or yml)   # Upload template

$ oc process \         ← process tpl to generate config.
  -f tpl_file \
  -l name=otherLabel \ ← labels are applied to every generated 

  -p DB_USER=user01 \  ← set/override template param
  -p DB_NAME=DDBB01 \    Alt: Use --param-file=postgres.env

| oc create -f -       ← Pipe STDOUT to create the objects

.parameters section:
 - list of params that can be overriden

$ oc process --parameters -f 'filename' ← list available params for local tpl
$ oc process --parameters 'tpl_name'    ← list available params for uploaded tpl

Example:
$ oc process --parameters -n openshift rails-postgresql-example
NAME                       DESCRIPTION                                                                                            GENERATOR    VALUE
SOURCE_REPOSITORY_URL      The URL of the repository with your application source code                                                         https://github.com/openshift/rails-ex.git
SOURCE_REPOSITORY_REF      Set this to a branch name/tag/... of repository if not using def.branch
CONTEXT_DIR                Set this to relative path to your project if not root 
APPLICATION_DOMAIN         The exposed hostname that will route to the Rails service                                                           rails-postgresql-example.openshiftapps.com
GITHUB_WEBHOOK_SECRET      A secret string used to configure the GitHub webhook                                                   expression   [a-zA-Z0-9]{40}
... 

 pair you want to override. A parameter reference may appear in any text field inside the template items.

$ oc edit template 'template' ← Edit already uploaded template

$ oc get templates -n openshift

Default OPC templates use a public github source repository containing the necessary app code.
  To build your own version of the application, you must:
  - Fork default repository (SOURCE_REPOSITORY_URL parameter)
    and override SOURCE_REPOSITORY_URL
</pre>

<pre bgorange zoom>
<span xsmall>Example templates</span>
(OCP template + app src)
CakePHP: PHP + MySQL 
  <a href="https://github.com/openshift/origin/tree/master/examples/quickstarts/cakephp-mysql.json">Template definition</a>
  <a href="https://github.com/openshift/cakephp-ex">Source repository</a>

Django + PostgreSQL
  <a href="https://github.com/openshift/origin/tree/master/examples/quickstarts/django-postgresql.json">Template definition</a>
  <a href="https://github.com/openshift/django-ex">Source repository</a>

NodeJS + MongoDB database
  <a href="https://github.com/openshift/origin/tree/master/examples/quickstarts/nodejs-mongodb.json">Template definition</a>
  <a href="https://github.com/openshift/nodejs-ex">Source repository</a>

Rails: Ruby + PostgreSQL
  <a href="https://github.com/openshift/origin/tree/master/examples/quickstarts/rails-postgresql.json">Template definition</a>
  <a href="https://github.com/openshift/rails-ex">Source repository</a>

ºExample templateº
kind: Template
apiVersion: v1
objects:                           ← main portion of the template: list of objects (BuildConfig, DeploymentConfig, ...)
  - kind: BuildConfig
    apiVersion: v1
    metadata:
      name: cakephp-mysql-example
      annotations:
        description: Defines how to build the application
    spec:
      source:
        type: Git
        git:
          uri: "${SOURCE_REPOSITORY_URL}"
          ref: "${SOURCE_REPOSITORY_REF}"
        contextDir: "${CONTEXT_DIR}"
  - kind: DeploymentConfig
    apiVersion: v1
    metadata:
      name: frontend
    spec:
      replicas: "${{REPLICA_COUNT}}"
parameters:
  - name: SOURCE_REPOSITORY_URL
    displayName: Source Repository URL
    description: The URL of the repository with your application source code
    value: https://github.com/openshift/cakephp-ex.git
    required: true
  - name: GITHUB_WEBHOOK_SECRET
    description: A secret string used to configure the GitHub webhook
    generate: expression
    from: "[a-zA-Z0-9]{40}"
  - name: REPLICA_COUNT
    description: Number of replicas to run
    value: "2"
    required: true
message: "... The GitHub webhook secret is ${GITHUB_WEBHOOK_SECRET} ..."
</pre>

<pre zoom>
<span xsmall>Template recommandations</span>
Group related services together in the management console by adding the
service.alpha.openshift.io/dependencies annotation to the Service object 
in your template. 

Example: Group Frontend and Database Services Together on the Management Console Overview

 kind: "Template"
 apiVersion: "v1"
 objects:
   - kind: "Service"
     apiVersion: "v1"
     metadata:
       name: "frontend"
       annotations:
         "service.alpha.openshift.io/dependencies": "[{\"name\": \"database\", \"kind\": \"Service\"}]"
 ...
   - kind: "Service"
     apiVersion: "v1"
     metadata:
       name: "database"
</pre>

<pre zoom>
<span xsmall>Create Template from Existing Objects</span>
Rather than writing an entire template from scratch, you can export existing objects 
from your project in template form, and then modify the template from there by adding
parameters and other customizations. To export objects in a project in template form, run:

$ oc export all --as-template='template_name' ˃ 'template_filename'
            ^^^
            Alt choose amongst:
            BuildConfig Build DeploymentConfig
            ImageStream Pod  ReplicationController
            Route       Service 
</pre>
</div>

<div groupv>
<pre title>
Porting Apps
to OpenShift
</pre>

<pre bgorange zoom>
<span xsmall>Openshift Full Journey</span>
$ oc cluster up # once to install

# Remember: A OCP project is a k8s namespace with extra meta-data (policies, users,...)
            A project can be seen as an isolated virtual cluster of running apps/pods

# -- login into cluster --
$ oc login [-u=<username>] \
  [-p=<password>] \
  [-s=<server>] \
  [-n=<project>] \
  [--certificate-authority=$PATH_TO_FILE.CRT |--insecure-skip-tls-verify]

$ oc new-project myNewProject      # alt 1
$ oc     project myExistingProject # alt 2

$ oc new-app -L  | grep cakephp  # alt 1
$ $ oc new-app --search cakephp  # alt 2
cakephp-...

$ oc new-app cakephp-mysql-example -o yaml    ← Dry run (show what will be created, do NOT run)

# next command will fetch source code,
# create a new BuildConfig,  sets up the builder
# image, builds your application image,
# create a DeploymentConfig and deploys the
# newly created image together
# with the specified environment variables. 
# The application is named "myAppAltName". 
$ oc new-app  \
    --build-env HTTP_PROXY=http://proxy.nt/ \ ← When generating apps from tmpl/src/img
    --build-env BUILD_ENV2=build_value2     \   you can use the --build-env to set
\                                               env.vars to the ºbuild containerº(@runtime)
\                                               Alt: --build-env-file=myBuildEnvVarsFile.env
\
    -env            ENVAR1=app_value1       \ ← When generating apps from tmpl/src/img
    -env            ENVAR1=app_value2       \   you can use the -e|--env to set
    -env            ENVAR1=app_value3       \   env.vars to the ºapp containerº(@runtime)
\                                               Alt: --env-file=myEnvVarsFile.env

    -label         label1=value1           \ ← When generating apps from tmpl/src/img
                                                you can use the -l|--label to add 
                                                labels to the created objects making it easy to
                                                collectively select, configure, and delete objects
                                                associated with the application.
    -name          "myAppAltName"            ← Do not use default (github project) name 
    https://github.com/openshift/rubyHelloW  ← create first  pod from git repo (autodetect language)
    php+mysql                                ← create second pod from 2 images alternatively 

# verify the environment variables have been added
# by viewing the JSON document of the just created
# DeploymentConfig:

$ oc get dc myAppAltName -o json

# WARN: Build can fail in first try due to quota restrictions.
#       This is normal in production environments.
# Just edit the (autogenerated by new-app) BuildConfig to add quotas:
oc edit bc/myAppAltName
kind: BuildConfig
spec:
  postCommit: {}
- resources: {}         ← By default is "empty"
+ resources:            ← Example quotas:
+   limits:             ← 
+     cpu: 500m         ←
+     memory: 1G        ←     
+   requests:           ←  
+     cpu: 100m         ←   
+     memory: 256M      ←     
  runPolicy: Serial



# Check the bc/dc/build process:
$ oc logs -f    bc/myAppAltName-1 ← --version(opt) filter available
$ oc logs -f build myAppAltName-1 
$ oc logs -f    dc/myAppAltName-1 ← --version(opt) filter available
$ oc logs -f  pods/myAppAltName-1 ← -c "container" for multicontainer pods

$ oc get pods ← Examine running pods
$ oc status   ← Examine porject status (components+relationships)

# Expose HTTP Service externally:
$ oc expose service myAppAltName --hostname=myExternalDomainName

$ wget http://... ← Ex:check running pod REST service (if any)
</pre>

<pre zoom>
<span xsmall>Adapting config files to OCP</span>
Ex: change app config/database.yml to make it "OCP Friendly"

 ORIGINAL APP           |        OPC Friendly
 database.yml           |        database.yml
 -----------------------+----------------------------------------------------------------
                        | ˂% user     = ENV["DB_USER"] %˃
                        | ˂% pass     = ENV["DB_PASS"] %˃
                        | ˂% db_service = ENV.fetch("DATABASE_SERVICE_NAME","").upcase %˃
                        | 
default: ⅋default       | default: ⅋default
  adapter : postgresql  |   adapter: postgresql
  encoding: unicode     |   encoding: unicode
  pool    : 5           |   pool    : ˂%= ENV["DB_MAX_CONNECTIONS"] || 5 %˃
  host    : localhost   |   host    : ˂%= ENV["#{db_service}_SERVICE_HOST"] %˃
  username: rails       |   username: ˂%= user %˃
  password:             |   password: ˂%= pass %˃
  port    : 54323       |   port    : ˂%= ENV["#{db_service}_SERVICE_PORT"] %˃
  database: myApp01DB   |   database: ˂%= ENV["DB_DATABASE"] %˃
                        | 
                        | "Elvis" config: 
                        | ˂% user = ENV.key?("DB_ADMIN_PASS") ? "root" : ENV["DB_USER"] %˃
                        | ˂% pass = ENV.key?("DB_ADMIN_PASS") ? ENV["DB_ADMIN_PASS"] : ENV["DB_PASS"] %˃
</pre>

<pre zoom>
<span xsmall>OCP&amp;Git</span>
- OCP requires Git Installed:
  - Building an App usually requires a git remote source repo accesible by the OCP Cluster
</pre>

<pre zoom>
<span xsmall>Set Up Maven Mirror</span>
<span xsmall>(Java Specific)</span>
- Use it for non-public artifacts and as cache.
- Many maven-aware images have a MAVEN_MIRROR_URL ENV.VAR that can be used to simplify the process
  If not, read the Nexus documentation to configure your build properly.

STEPS:

PRE-SETUP:
Cluster Admin: Setup Read/Write Persistent Volume (PV)

$ oc new-app sonatype/nexus ← Download+deploy the official Nexus container image
               ↑↑↑↑↑↑↑↑    
             default login: admin
                     pass : admin123

$ oc expose svc/nexus       ← Create route by exposing the newly created Nexus service

$ oc get routes             ← find out the pod's new external address
(output similar to)
→ NAME      HOST/PORT                              PATH      SERVICES   PORT       TERMINATION
→ nexus     nexus-myproject.192.168.1.173.xip.io             nexus      8081-tcp
    
# (Confirm that Nexus is running with wget|curl|...) 

NOTE: Nexus comes pre-configured for the Central Repository, but you may need others
      for your application. For many Red Hat images, it is recommended to add the
      jboss-ga repository at Maven repository.

This is a good time to set up readiness and liveness probes. These will periodically check to see that Nexus is running properly.

$ oc set probe dc/nexus \         ← Set up liveness probes
    --liveness \
    --failure-threshold 3 \
    --initial-delay-seconds 30 \
    -- echo ok
$ oc set probe dc/nexus \         ← Set up readiness probe
    --readiness \
    --failure-threshold 3 \
    --initial-delay-seconds 30 \
    --get-url=http://:8081/nexus/content/groups/public


PERSISTENCE:

<span bgorange>
$ oc volumes dc/nexus --add \             ← Add PVC to the dc/nexus:
    --name 'nexus-volume-1' \               This command replaces the previous emptyDir vol. 
    --type 'pvc' \                          with a 1GB  PVC  mounted at /sonatype-work, 
    --mount-path '/sonatype-work/' \        
    --claim-name 'nexus-pv' \             ← Due to the change in configuration, pod will 
    --claim-size '1G' \                     be redeployed automatically.
    --overwrite
</span>


CREATE A BUILD TO TEST IT:

Define an (example) build using the created Nexus repository. Th

The example builder image supports ENV.VAR MAVEN_MIRROR_URL.
(othewise modify builder image to fix MVN settings)

$ export BASE_IMAGE=openshift/wildfly-100-centos7:latest
$ export    GIT_SRC=https://github.com/openshift/jee-ex.git
$ oc new-build ${BASE_IMAGE}~${GIT_SRC} \
    -e MAVEN_MIRROR_URL='http://nexus.myNexusProject:8081/nexus/content/groups/public'

$ oc logs build/jee-ex-1 --follow

CONFIRMING SUCCESS:

confirm that application’s dependencies have been creating navigating to:
http://<NexusIP>:8081/nexus/content/groups/public 

</pre>

</div>

<div groupv>
<pre title>
Project
Administration 
</pre>

<pre zoom>
<span xsmall>OpenShift Projects</span>
- An OCP Project is a Kubernetes namespace (k8s virtual "subcluster") plus
  extra Openshift metadata for handling users and policies

- Itallows a community of users to organize and manage their content
  in isolation from other communities.
</pre>

<pre zoom>
<span xsmall>Project creation</span>
ºPRE-SETUP:º
  - the authenticated user must first be allowed project creation by cluster administrator.
  - max. number of projects may be limited by the system administrator. 
    Once the limit is reached, existing projects must be deleted to create new ones.

  $ oc new-project $project_name \     Ex: $ oc new-project hello-openshift \
      --description="$description" \           --description="example project demonstrating OpenShift v3" \
      --display-name="$display_name"           --display-name="Hello OpenShift"


$ oc get projects            # List project

$ oc project <project_name>  # Swith working project

$ oc status                  ← Show Project Status
                               provides high-level overview of current project,
                               with its components and their relationships.

$ oc delete project prj_name ← Delete project
</pre>


<pre zoom TODO>
<span xsmall>Project&amp;Users</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-core-concepts-projects-and-users]
...
</pre>

<pre zoom>
<span xsmall>Policies</span>
2 types:
  - cluster:
    $ oc describe cluster policy

  - project:
    $ oc ??? 
</pre>

<pre zoom>
<span xsmall>User Roles</span>
$ oc policy add-role-to-user edit demo-user
</pre>

<pre zoom>
<span xsmall>Service Accounts</span>
12.1. Overview

When a person uses the OpenShift Container Platform CLI or web console, their 
API token authenticates them to the OpenShift API. 

For example Replication controllers make API calls to create or delete pods.

Service accounts provide a flexible way to control API access without sharing 
a regular user's credentials.

Service accounts belongs to a given project.

Every service account has an associated user name that can be granted roles, 
just like a regular user.

The user name is derived from its project and name:

system:serviceaccount:<project>:<name>

Ex: add the view role to the robot service account in the top-secret project:
$ oc policy add-role-to-user view \
  -z system:serviceaccount:top-secret:robot ← z flag (optional) can help avoiding typos/errors


Every service account is also a member of two groups:

system:serviceaccount           Includes all service accounts in the system. 
system:serviceaccount:$project  Includes all service accounts in the specified project. 

Ex: 

# allow all service accounts in all projects to view resources in the top-secret project:
$ oc policy add-role-to-group view system:serviceaccount -n top-secret
# allow all service accounts in the managers project to edit resources in the top-secret project:
$ oc policy add-role-to-group edit system:serviceaccount:managers -n top-secret

Default Service Accounts and Roles created in every project:

Service       Account Usage

builder       Used by build pods. It is given the system:image-builder role,
              which allows pushing images to any image stream in the project 
              using the internal Docker registry.

deployer      Used by deployment pods and is given the system:deployer role,
              which allows viewing and modifying replication controllers and
              pods in the project.

default       Used to run all other pods unless they specify a different 
              service account.

All service accounts in a project are given the system:image-puller role,
which allows pulling images from any image stream in the project using the
internal Docker registry.
</pre>

<pre zoom>
<span xsmall>Managing Service Accounts</span>

$ oc get sa               ← list existing service accounts in the current project:
NAME       SECRETS   AGE
builder    2         2d
default    2         2d
deployer   2         2d


$ oc create sa robot            ← Create new sa
serviceaccount "robot" created    two secrets are automatically added to it:
                                     - an API token
                                     - credentials for the OC Registry
                                  (Those two secrets do NOT expire but can be revoked)
                                  Can be seen by describing the sa:
                                  $ oc describe sa robot
                                  → Name       : robot
                                  → Namespace  : project1
                                  → Labels     : <none>
                                  → Annotations: <none>
                                  → Image pull secrets: robot-dockercfg-qzbhb
                                  → 
                                  → Mountable secrets:  robot-token-f4khf
                                  →                     robot-dockercfg-qzbhb
                                  → 
                                  → Tokens:             robot-token-f4khf
                                  →                     robot-token-z8h44
                                  

(See manual to
  - 12.5. Enabling Service Account Authentication
  - 12.6. Managed Service Accounts
  - 12.7. Infrastructure Service Accounts
  - 12.8. Service Accounts and Secrets
  - 12.9. Managing Allowed Secrets
  - 12.10. Using a Service Account’s Credentials Inside a Container
  - 12.11. Using a Service Account’s Credentials Externally )
</pre>

<pre zoom>
<span xsmall>Security Ctx Constr. (oc scc)</span>
- control the actions a ºpodº (vs a user) can perform
 and what resources it can access.('restricted SCC' by default)

$ oc get scc
→ NAME             PRIV  CAPS SELINUX   RUNASUSER        FSGROUP   SUPGROUP PRIORI READONLY 
→                                                                                  ROOTFS  
→ anyuid           false []   MustRunAs RunAsAny         RunAsAny  RunAsAny 10     false    
→ hostaccess       false []   MustRunAs MustRunAsRange   MustRunAs RunAsAny <none> false    
→ hostmount-anyuid false []   MustRunAs RunAsAny         RunAsAny  RunAsAny <none> false    
→ hostnetwork      false []   MustRunAs MustRunAsRange   MustRunAs MustRunAs<none> false    
→ nonroot          false []   MustRunAs MustRunAsNonRoot RunAsAny  RunAsAny <none> false    
→ privileged       true  []   RunAsAny  RunAsAny         RunAsAny  RunAsAny <none> false    
→ ºrestrictedº       false []   MustRunAs MustRunAsRange   MustRunAs RunAsAny <none> false    
→ 
→ NAME             VOLUMES
→ 
→ anyuid           [configMap downwardAPI emptyDir              persistentVolumeClaim secret]
→ hostaccess       [configMap downwardAPI emptyDir hostPath     persistentVolumeClaim secret]
→ hostmount-anyuid [configMap downwardAPI emptyDir hostPath nfs persistentVolumeClaim secret]
→ hostnetwork      [configMap downwardAPI emptyDir              persistentVolumeClaim secret]
→ nonroot          [configMap downwardAPI emptyDir              persistentVolumeClaim secret]
→ privileged       [*]
→ restricted       [configMap downwardAPI emptyDir persistentVolumeClaim secret]

$ oc describe scc anyuid
→ ...
</pre>

<pre zoom>
<span xsmall>Check if User Can Create Pods (scc-*)</span>
- Using the scc-review and scc-subject-review options,
  you can see if an individual user, or a user under a
  specific service account, can create or update a pod.

- Using the scc-review option, you can check if a service
  account can create or update a pod:
  The command outputs the security context constraints that admit the resource.

Ex: check if a user with the system:serviceaccount:projectname:default service account
    can a create a pod:

  $ oc policy scc-review -z system:serviceaccount:projectname:default -f my_resource.yaml

Ex: check whether a specific user can create or update a pod:

  $ oc policy scc-subject-review -u <username> -f my_resource.yaml

Ex: check if a user belonging to a specific group can create a pod in a specific file:

$ oc policy scc-subject-review -u <username> -g <groupname> -f my_resource.yaml
</pre>
  
<pre zoom>
<span xsmall>Show permissions for authenticated user</span>
Within your project, you can determine what ºverbs(/actions)º you can
perform against all namespace-scoped resources (including third-party resources).

Test scopes in terms of the user and role:

  $ oc policy can-i --list --loglevel=8
  → (output helps you to determine what API request to make to gather the information)

To receive information back in a user-readable format:

  $ oc policy can-i --list
  → (output provides a full list)

To determine if I you can perform specific verbs:

  $ oc policy can-i <verb> <resource>

User scopes can provide more information about a given scope.
Example:
  $ oc policy can-i <verb> <resource> --scopes=user:info
</pre>
</div>

<div groupv>
<pre title>
Cluster
Administration
</pre>

<pre zoom>
<span xsmall>Image Registry</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#architecture-infrastructure-components-image-registry]

- Multiple project in the same cluster can access the same image streams.
- Multiple clusters can access the same external registries.
- Clusters can only share a registry if the OpenShift Container Platform internal image registry is exposed via a route. 
.
.
</pre>


<pre zoom>
<span xsmall>Storage (PV/PVC)</span>
    Seq. diagram
STEP 1: PersistentVolume: specific resource
STEP 2: PersistentVolumeClaim : 
        request PV + min size, r/rw, ...  Used by pods.
STEP 3: Use PVC in Pod
</pre>

<pre zoom>
<span xsmall>Pod Definition with a Claim</span>
Cluster administrators create PersistenVolume(s) to isolate from the underlying OCP technology (NFS, Gluster, iSCSI,...)

Users create PersistemVolumeClaim(s) to reuse part of the PV created by cluster admins.

mounts can be:
 -  Read Only
 - Read Write Once: Only a pod can write
 -  Read Write Many: Many pods can write

(Admins can reserve a PV to a PVC)
STEP 1(admin)                            STEP 2(dev)                    STEP 3(dev)
apiVersion: v1                          |apiVersion: "v1"              |apiVersion: "v1"
kind: PersistentVolume                  |kind: "PersistentVolumeClaim" |kind: "Pod"
metadata:                               |metadata:                     |...
  name: pv0001                          |  name: º"claim1"º              |spec:
spec:                                   |spec:                         |  containers:
  capacity:                             |  accessModes:                |    -
    storage: 1Gi                        |    - "ReadWriteOnce"         |      ...
  accessModes:                          |  resources:                  |      volumeMounts:
  - ReadWriteOnce                       |    requests:                 |        -
  nfs:                                  |      storage: "1Gi"          |          mountPath: "/var/www/html"
    path: /tmp                          |  volumeName: "pv0001"        |          name: º"pvol"º
    server: 172.17.0.2                  |                              |  volumes:
  persistentVolumeReclaimPolicy: Recycle|                                   -
  claimRef:                             |                                     name: º"pvol"º
    name: ºclaim1º                      |                                     persistentVolumeClaim:         
    namespace: default                  |                                     claimName: º"claim1"º

</pre>

<pre zoom>
<span xsmall>Storage Notes</span>
- Specifying a volumeName in your PVC does not prevent a different PVC from 
binding to the specified PV before yours does.  Your claim will remain 
Pending until the PV is Available.
- Therefore, to avoid these scenarios and ensure your claim gets bound to the 
volume you want, you must ensure that both volumeName and claimRef are 
specified.
- You can tell that your setting of volumeName and/or claimRef influenced the 
matching and binding process by inspecting a Bound PV and PVC pair for the pv.
kubernetes.io/bound-by-controller annotation. The PVs and PVCs where you set 
the volumeName and/or claimRef yourself will have no such annotation, but 
ordinary PVs and PVCs will have it set to "yes".
 - When a PV has its claimRef set to some PVC name and namespace, and is 
reclaimed according to a Retain or Recycle reclaim policy, its claimRef will 
remain set to the same PVC name and namespace even if the PVC or the whole 
namespace no longer exists.
</pre>



<pre zoom TODO>
<span xsmall>Managing Volumes (Dev.Guide)</span>
@[https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-volumes]
.
.
.
</pre>


<pre zoom TODO>
<span xsmall>OpenVSwitch</span>
  (OCP SDN)
Types:
  ovs-subnet
  ovs-mutitenant     (network by project)
  ovs-networkpolicy  (fine grained policy access)
</pre>
</div>


</body>
</html>

<!-- {

$ oc get events
____________________
$ rol self-provisioning es  lo que permite crear projectos
______________
$ Groups:
  cluster:
  - cluster-admin "todo"
  - cluster-view  "ver todo"

 admin     "todo dentro de un proyecto", volume?
 edit      "no puede manipular secrets, conectar a un contenedor, ..", volume-claim
 view      "ver proyecto salvo secrets"

______________

$ oc login guarda el token en ~/.kube/config
______________
Service Accounts


Overview

When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:

    Replication controllers make API calls to create or delete pods

    Applications inside containers can make API calls for discovery purposes

    External applications can make API calls for monitoring or integration purposes

Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.
Usernames and groups

Every service account has an associated username that can be granted roles, just like a regular user. The username is derived from its project and name: system:serviceaccount:<project>:<name>

For example, to add the view role to the robot service account in the top-secret project:

$ oc policy add-role-to-user view system:serviceaccount:top-secret:robot

Every service account is also a member of two groups:

    system:serviceaccounts, which includes all service accounts in the system

    system:serviceaccounts:<project>, which includes all service accounts in the specified project

For example, to allow all service accounts in all projects to view resources in the top-secret project:

$ oc policy add-role-to-group view system:serviceaccounts -n top-secret

To allow all service accounts in the managers project to edit resources in the top-secret project:

$ oc policy add-role-to-group edit system:serviceaccounts:managers -n top-secret

Enable service account authentication

Service accounts authenticate to the API using tokens signed by a private RSA key. The authentication layer verifies the signature using a matching public RSA key.

To enable service account token generation, update the master configuration serviceAccountConfig stanza to specify a privateKeyFile (for signing), and a matching public key file in the publicKeyFiles list:

serviceAccountConfig:
  ...
  masterCA: ca.crt 
  privateKeyFile: serviceaccounts.private.key 
  publicKeyFiles:
  - serviceaccounts.public.key 
  - ...

    CA file used to validate the API server’s serving certificate
    Private RSA key file (for token signing)
    Public RSA key files (for token verification). If private key files are provided, then the public key component is used. Multiple public key files can be specified, and a token will be accepted if it can be validated by one of the public keys. This allows rotation of the signing key, while still accepting tokens generated by the previous signer.
Managed service accounts

Service accounts are required in each project to run builds, deployments, and other pods. The managedNames setting in the master configuration file controls which service accounts are automatically created in every project:

serviceAccountConfig:
  ...
  managedNames: 
  - builder 
  - deployer 
  - default 
  - ...

    List of service accounts to automatically create in every project
    A builder service account in each project is required by build pods, and is given the system:image-builder role, which allows pushing images to any image stream in the project using the internal Docker registry.
    A deployer service account in each project is required by deployment pods, and is given the system:deployer role, which allows viewing and modifying replication controllers and pods in the project.
    A default service account is used by all other pods unless they specify a different service account.

All service accounts in a project are given the system:image-puller role, which allows pulling images from any image stream in the project using the internal Docker registry.
Infrastructure service accounts

Several infrastructure controllers run using service account credentials. The following service accounts are created in the OpenShift infrastructure namespace at server start, and given the following roles cluster-wide:

    The replication-controller service account is assigned the system:replication-controller role

    The deployment-controller service account is assigned the system:deployment-controller role

    The build-controller service account is assigned the system:build-controller role. Additionally, the build-controller service account is included in the privileged security context constraint in order to create privileged build pods.

To configure the namespace where those service accounts are created, set the openshiftInfrastructureNamespace field in the master configuration file:

policyConfig:
  ...
  openshiftInfrastructureNamespace: openshift-infra

Set limitSecretReferences field in master configuration file to true to require pod secret references to be whitelisted by their service accounts. Set its value to false to allow pods to reference any secret in the namespace.

serviceAccountConfig:
  ...
  limitSecretReferences: false

i
Service Accounts

    Overview
    Usernames and groups
    Enable service account authentication
    Managed service accounts
    Infrastructure service accounts

Overview

When a person uses the command line or web console, their API token authenticates them to the OpenShift API. However, when a regular user’s credentials are not available, it is common for components to make API calls independently. For example:

    Replication controllers make API calls to create or delete pods

    Applications inside containers can make API calls for discovery purposes

    External applications can make API calls for monitoring or integration purposes

Service accounts provide a flexible way to control API access without sharing a regular user’s credentials.
Usernames and groups

Every service account has an associated username that can be granted roles, just like a regular user. The username is derived from its project and name: system:serviceaccount:<project>:<name>

For example, to add the view role to the robot service account in the top-secret project:

$ oc policy add-role-to-user view system:serviceaccount:top-secret:robot

Every service account is also a member of two groups:

    system:serviceaccounts, which includes all service accounts in the system

    system:serviceaccounts:<project>, which includes all service accounts in the specified project

For example, to allow all service accounts in all projects to view resources in the top-secret project:

$ oc policy add-role-to-group view system:serviceaccounts -n top-secret

To allow all service accounts in the managers project to edit resources in the top-secret project:

$ oc policy add-role-to-group edit system:serviceaccounts:managers -n top-secret

Enable service account authentication

Service accounts authenticate to the API using tokens signed by a private RSA key. The authentication layer verifies the signature using a matching public RSA key.

To enable service account token generation, update the master configuration serviceAccountConfig stanza to specify a privateKeyFile (for signing), and a matching public key file in the publicKeyFiles list:

serviceAccountConfig:
  ...
  masterCA: ca.crt 
  privateKeyFile: serviceaccounts.private.key 
  publicKeyFiles:
  - serviceaccounts.public.key 
  - ...

    CA file used to validate the API server’s serving certificate
    Private RSA key file (for token signing)
    Public RSA key files (for token verification). If private key files are provided, then the public key component is used. Multiple public key files can be specified, and a token will be accepted if it can be validated by one of the public keys. This allows rotation of the signing key, while still accepting tokens generated by the previous signer.
Managed service accounts

Service accounts are required in each project to run builds, deployments, and other pods. The managedNames setting in the master configuration file controls which service accounts are automatically created in every project:

serviceAccountConfig:
  ...
  managedNames: 
  - builder 
  - deployer 
  - default 
  - ...

    List of service accounts to automatically create in every project
    A builder service account in each project is required by build pods, and is given the system:image-builder role, which allows pushing images to any image stream in the project using the internal Docker registry.
    A deployer service account in each project is required by deployment pods, and is given the system:deployer role, which allows viewing and modifying replication controllers and pods in the project.
    A default service account is used by all other pods unless they specify a different service account.

All service accounts in a project are given the system:image-puller role, which allows pulling images from any image stream in the project using the internal Docker registry.
Infrastructure service accounts

Several infrastructure controllers run using service account credentials. The following service accounts are created in the OpenShift infrastructure namespace at server start, and given the following roles cluster-wide:

    The replication-controller service account is assigned the system:replication-controller role

    The deployment-controller service account is assigned the system:deployment-controller role

    The build-controller service account is assigned the system:build-controller role. Additionally, the build-controller service account is included in the privileged security context constraint in order to create privileged build pods.

To configure the namespace where those service accounts are created, set the openshiftInfrastructureNamespace field in the master configuration file:

policyConfig:
  ...
  openshiftInfrastructureNamespace: openshift-infra

Set limitSecretReferences field in master configuration file to true to require pod secret references to be whitelisted by their service accounts. Set its value to false to allow pods to reference any secret in the namespace.

serviceAccountConfig:
  ...
  limitSecretReferences: false
_______________
Minishift:
The server is accessible via web console at:
https://192.168.64.2:8443
You are logged in as:
  User: developer
  Password: <any value>
To login as administrator:
$  oc login -u system:admin
  Logged into "https://192.168.64.2:8443" as "system:admin"

________________
    ºUsers + HTPPassword:º
    Enable HTTP Auth. in master config.:
<pre zoom>
vim /etc/origin/master/master-config.yaml
  provider: 
-   #kind: denyAllPasswordIdentityProvider
+   kind: HTPPasswordIdentityProvider

$ systemctl restart ???-openshift-???
</pre>
    Adding a new user (id):
<pre zoom>
$ htpasswd -cb  \
    /etc/origin/master/openshift-passwd ricardo redhat
</pre>
________________________
<pre zoom>
REF: <a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/">RedHat OpenS. Dev.Guide</a>
@startuml
title Wilfly + DDBB

participant github
participant developer as dev
participant OpenshiftConsole as cc
participant OpenshiftWebConsole as wc
participant Project/Pod as project

ºSTEP 1: Create app from templateº
dev →   github: fork
dev →       cc: oc ºnew-projectº insultapp \
                   --display-name="Elizabe..."
cc  → +project:
dev →       git: new project repo (https://github.com/myUser/myProject)
dev →        cc: oc ºnew-appº \
                    wildfly:lastest~@[https://github.com/myUser/myProject] \
                    -name='insults'
                 (OpenShift will generate build&amp;deployment configs)
cc  →     +pod: new pod(project) from template
cc  →       cc: display log
note over cc
    →Found image
    ...
    →ºSuccessº
    ...
    Run 'oc status' to view your app
end note
dev →       cc: oc ºexpose serviceº insults
                (using web con. the route is automatically created)
dev →       cc: oc ºget routesº
                (verify app by visiting the URL)
  Output is similar to:
  NAME    HOST/PORT        PATH    SERVICE  LABELS
  insults insul....xip.io  insults          app=insults

note over github, project STEP 2: Create database

dev →       wc: go to Project 
                 → click Add to project
                  → filter by "postgres"
                   → Selecte Database postgresql-persistent
dev →       wc: setup conf. settings (Mem. limit, service name, ..)
dev →       wc: click create
wc  →     +pod: creates pod running database
                env vars are created:
                POSTGRESQL_PORT_5432_TCP_ADDR=172.30.76.249
                ...
dev →       cc: oc ºenv dcº insults \
                  -e POSTGRESQL_USER=insult
                  -e PGPASSWORD=insult
                  -e POSTGRESQL_DATABASE=insults
                # adds env.vars to DeploymentConfig (ºcommon to all podsº)

dev →      dev: edit createSchema.sql
dev →       wc: open terminal for running postgres pod and execute:
                $ cat createSchema.sql | psql \
                  -h $POSTGRESQL_SERVICE_HOST \
                  -p $POSTGRESQL_SERVICE_PORT \
                  -U $POSTGRESQL_USER $POSTGRESQL_DATABASE 
note over dev,wc:
   secrets can be used to automate password creation
end note
dev →       dev: Edit pom.xml to add deps on driver
                 ˂dependency˃
                   ˂groupId˃org.postgresql˂/groupId˃
                     ˂artifactId˃postgresql˂/artifactId˃
                     ˂version˃9.4-1200-jdbc41˂/version˃
                 ˂/dependency˃

ºSTEP 3: Create REST Endpointº
dev →       dev: Edit pom.xml
                 ˂dependency˃
                   ˂groupId˃javax˂/groupId˃
                     ˂artifactId˃javaee-api˂/artifactId˃
                     ˂version˃7.0˂/version˃
                     ˂scope˃provided˂/scope˃
                 ˂/dependency˃
</pre>
______________________________
 If a Jenkinsfile exists in the root or specified context directory of the source repository when creating a new application, OpenShift Container Platform generates a Pipeline build strategy. Otherwise, if a Dockerfile is found, OpenShift Container Platform generates a Docker build strategy. Otherwise, it generates a Source build strategy.

You can override the build strategy by setting the --strategy flag to either docker, pipeline or source.

$ oa new-app /home/user/code/myapp --strategy=docker
_______________________________
https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/developer_guide/#dev-guide-environment-variables
___________________________
App Up&amp;Running Flow?
PRE-CONDITIONS:
  - OCP Cluster up&amp;Running
  - Project and users created

PRE-SETUP:
  - Kubernetes/OCP Cluster "admin":
      Create Persistent-Volumes for App (Pods sets to be scheduled)
    
  - OCP Project "admin":
      secret set → (Used by) → (BuildConfig set, DeploymentConfig set)

  - App Developer: 
      Creates Repository with Source Code → git_repo → (Used by) → BuildConfig

  - App Developer?: 
      Creates Dockerbuild set →(generates) → Container_Image set → ImageStream set

RECURRENT:
  - App Developer: 
      Commit to git_repo → (triggers) → New BuildConfig instance
  
- "Deploy/Run app":
    git_repo
       ↓
    BuildConfig set → (generates)→ ImageStream set → (Used by) → DeploymentConfig set

    Service     set                                → (Used by) → DeploymentConfig set
       ↓
    Route       set                                → (Used by) → DeploymentConfig set
  
  
  DeploymentConfig set → Kubernetes → Schedules pods
_____________________________________
<!-- 
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#docker-build">Docker strategy</a>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#source-build">Source Strategy</a>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#custom-build">Custom Strategy</a>
<a TODO href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.5/html-single/architecture/#pipeline-build">Pipeline Build</a>

__________________________________
Cluster
    + -- Project1 (default cpu/memory limits for pods)
    |        +-- App01
    |               |
    |               +-- BuildConfig  (strategy, ...)
    |               |
    |               +-- DeploymentConfig (cpu/memory limits, strategy,)
    |                
    |                
    |                
    |                
    |                
    | -- Project2
___________________________________
https://blog.openshift.com/local-development-self-signed-certificates/
____________________
https://access.redhat.com/articles/2745171

Deploying OpenShift Container Platform 3 on VMware vSphere
Updated July 12 2018 at 6:46 PM - English

This reference architecture describes how to deploy and manage Red Hat OpenShift Container Platform 3 on VMware vSphere. Red Hat OpenShift Container Platform allows for specific configuration parameters to be set to take full advantage of cloud specific features. These documents explain how OpenShift and vSphere can be used together for a successful installation and deployment. Also, steps and links to a code base will be provided to allow for the installation of the exact reference architecture environment using a VMware vSphere account.
________________________________________
https://access.redhat.com/documentation/en-us/reference_architectures/2018/pdf/deploying_and_managing_openshift_3.9_on_vmware_vsphere/Reference_Architectures-2018-Deploying_and_Managing_OpenShift_3.9_on_VMware_vSphere-en-US.pdf
Reference Architectures 2018 Deploying and Managing OpenShift 3.9 on VMware vSphere
________________________________________
OpenShift Training:
[
Online Free accounts:
-  https://www.openshift.com/learn/get-started/

https://developers.redhat.com/products/cdk/overview/
Red Hat Container Development Kit provides a pre-built Container Development Environment based on Red Hat Enterprise Linux to help you develop container-based applications quickly. The containers you build can be easily deployed on any Red Hat container host or platform, including: Red Hat Enterprise Linux, Red Hat Enterprise Linux Atomic Host, and our platform-as-a-service solution, OpenShift Container Platform 3.
]
_______________________________
golang-s2i:
https://github.com/rhtps/s2i-golang
________________________________
https://github.com/openshift/source-to-image
Source-to-Image (S2I) is a toolkit and workflow for building reproducible Docker images from source code. S2I produces ready-to-run images by injecting source code into a Docker container and letting the container prepare that source code for execution. By creating self-assembling builder images, you can version and control your build environments exactly like you use Docker images to version your runtime environments
___________________________
https://docs.openshift.com/container-platform/3.7/dev_guide/dev_tutorials/openshift_pipeline.html
____________________________
installing Openshift on AWS (4 minutes video):
https://developers.redhat.com/openshift/?sc_cid=701f20000012pVAAAY

https://developers.redhat.com/blog/2019/03/07/openshift-4-0-developer-preview-on-aws-is-up-and-running/
_____________________
https://developers.redhat.com/blog/2019/09/05/red-hat-openshift-4-on-your-laptop-introducing-red-hat-codeready-containers/?sc_cid=701f2000000RtqCAASq

We are pleased to announce that Red Hat CodeReady Containers is now available as a Developer Preview. CodeReady Containers brings a minimal, preconfigured OpenShift 4.1 or newer cluster to your local laptop or desktop computer for development and testing purposes. CodeReady Containers supports native hypervisors for Linux, macOS, and Windows 10. You can download CodeReady Containers from the CodeReady Containers product page on cloud.redhat.com.
_____________________

} -->
