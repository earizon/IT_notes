<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <meta name="keywords" content="cloud,aws,azure,scaleway,101,AD,api management,auditing,backup,batch,billing,storage,cache,cold storage,comparative,compute,devops,diagram,esb,event stream,troubleshooting,web,security,serverless,microservice">
   <title>CLOUD map (WiP)</title>
<head>
<script type=module>import('/IT_notes/map_v1.js')</script>
<link rel='stylesheet' type='text/css' href='/IT_notes/map_v1.css' />
</head>

<body>
<br/>
<span xbig>Azure (v 1.0)</span>&nbsp;&nbsp;&nbsp;
<a href="https://portal.Azure.com">[Azure Portal]</a>,
<a href="https://docs.microsoft.com/en-us/rest/api/azure/">[FULL REST API!!!]</a>,
<a href="https://Azure.microsoft.com/resources/templates/">[Public Template Collection]</a>,
<a href="https://https://shell.Azure.com/powershell">[Online PowerShell console]</a>,
<a href="https://docs.microsoft.com/en-us/cli/azure/get-started-with-azure-cli">[Cli get Started]</a>,

<br/>
<div groupv>
<pre zoom labels="azure,resource,">
<span xsmall>Who-is-Who</span>
(Forcibly incomplete but still quite pertinent list of core people and companies)
REF: @[${wikipedia}/Microsoft_Azure#Key_people]
- Mark Russinovich: CTO, Microsoft Azure[84] @[${wikipedia}/Mark_Russinovich]
- Scott Guthrie   : Microsoft Executive Vice President of
                    Cloudâ…‹AI group
- Jason Zander    : Executive Vice President, Microsoft Azure
- Julia White     : Corporate Vice President, Microsoft Azure



- Michael Collier, Robin Shahan. Authors oF Fundamentals of Azure
</pre>

<pre zoom labels="azure,101,AD,webapps,vm,storage,network,ddbb,sdk">
<span xsmall>Regions WorldWide</span>
  (Output from $Âº$ az account list-locations -o tableÂº, @2020-04-01)
  eastasia        japanwest           uksouth            uaecentral
  southeastasia   japaneast           ukwest             uaenorth
  centralus       brazilsouth         westcentralus      southafricanorth
  eastus          australiaeast       westus2            southafricawest
  eastus2         australiasoutheast  koreacentral       switzerlandnorth
  westus          southindia          koreasouth         switzerlandwest
  northcentralus  centralindia        francecentral      germanynorth
  southcentralus  westindia           francesouth        germanywestcentral
  northeurope     canadacentral       australiacentral   norwaywest
  westeurope      canadaeast          australiacentral2  norwayeast

RÂºWARNÂº: resource price differs between regions

<hr/>
<span xsmall>Mind-map Services</span>
BÂºAzure SDK:Âº
 - Python  : @[https://docs.microsoft.com/en-us/azure/developer/python/azure-sdk-install]
 - JAVA SDK: @[https://docs.microsoft.com/en-us/java/api/overview/azure/?view=azure-java-stable]
 - .NET    :  ...

ÂºGENERAL             COMPUTE                         NETWORKING                  STORAGEÂº
 â”œâ”€ Dashboard        â”œâ”€ Virt. machines               â”œâ”€ Virtual networks         â”œâ”€ blobs,tables,
 â”œâ”€ resource group   â”œâ”€ Virt. machines (classic)     â”œâ”€ Virtual networks(Clas)   â”‚  queues,file-shares
 â”œâ”€ all resources    â”œâ”€ Virt. machines scale sets    â”œâ”€ load balancers           â”œâ”€ storage accounts
 â”œâ”€ subscriptions    â”œâ”€ container services           â”œâ”€ application gateways     â”œâ”€ storage accounts (clas)
 â”œâ”€ cost mgn + bill. â”œâ”€ batch accounts               â”œâ”€ Virtual network gateways â”œâ”€ data lake store
 â”œâ”€ reservations     â”œâ”€ cloud services (classic)     â”œâ”€ local   network gateways â”œâ”€ simple device audits
 â””â”€ help + support   â”œâ”€ remoteapp collections        â”œâ”€ dns zones                â”œâ”€ recovery service audits
                     â”œâ”€ container registries         â”œâ”€ route tables             â”œâ”€ backup vaults (clas)
                     â”œâ”€ availability sets            â”œâ”€ cdn profiles             â”œâ”€ site recovery vaults(clas)
                     â”œâ”€ disks                        â”œâ”€ traffic manager profiles â””â”€ import/export jobs
                     â”œâ”€ snapshots                    â”œâ”€ expressroute circuits
                     â”œâ”€ images                       â”œâ”€ Net.Security groups      ÂºSOLUTIONSÂº
                     â”œâ”€ disks (classic)              â”œâ”€ Net.Security groups(clas) â”œâ”€ Power BI
                     â”œâ”€ vm images (classic)          â”œâ”€ network interfaces        â”‚  â””â”€ Simplified display
                     â”œâ”€ citrix xendesktop essentials â”œâ”€ public ip addr .          â”‚     of data and charts
                     â”œâ”€ citrix xenapp essentials     â”œâ”€ reserved ip addr. (clas)  â”œâ”€ Office 365
                     â”œâ”€ function apps                â””â”€ connections               â””â”€ Microsoft Dynamcis
                     â””â”€ Kubernetes AKS                                                   â””â”€ Planning Software

ÂºWEB-MOBILE                        DATABASES                     INTELIGENCE + ANALYTICSÂº
 â”œâ”€ app services                   â”œâ”€ sql ddbbs                  â”œâ”€ hdinsight clusters
 â”œâ”€ logic apps                     â”œâ”€ sql data-warehouses ddbbs  â”œâ”€ M.L. studio workspaces
 â”œâ”€ cdn profiles                   â”œâ”€ sql server stretch  ddbbs  â”œâ”€ stream analytics jobs
 â”œâ”€ media services                 â”œâ”€ AZ cosmos db               â”œâ”€ cognitive services
 â”œâ”€ search services                â”œâ”€ redis caches               â”œâ”€ data lake analytics
 â”œâ”€ mobile engagement              â”œâ”€ data factories             â”œâ”€ data factories
 â”œâ”€ api management services        â”œâ”€ AZ ddbb for mysql servers  â”œâ”€ power bi Wspace collection
 â”œâ”€ notification hubs              â”œâ”€ AZ ddbb for postgresql     â”œâ”€ analysis services
 â”œâ”€ notification hubs namespaces   â”œâ”€ sql elastic pools          â”œâ”€ data catalog
 â”œâ”€ integration accounts           â””â”€ sql servers                â”œâ”€ customer insight
 â”œâ”€ app services plans                                           â”œâ”€ log analysis
 â”œâ”€ app services enviroments                                     â”œâ”€ M.L. studio web Ser.plans
 â”œâ”€ api connections                                              â”œâ”€ M.L. studio web Services
 â”œâ”€ app service certificates                                     â”œâ”€ M.L. experimentation
 â”œâ”€ function apps                                                â””â”€ M.L. model management
 â””â”€ app service domains

ÂºData+Analysis                           Monitoring+Management               App Development ToolsÂº
 â”œâ”€ SQL DDBB                             â”œâ”€ monitor                          â”œâ”€ Team service accounts
 â”œâ”€ DocumentDB                           â”œâ”€ application insights             â”œâ”€ team projects
 â”œâ”€ Data Factoring                       â”œâ”€ log analytics                    â”œâ”€ devtest labs
 â””â”€ Machine Learning                     â”œâ”€ automatino accounts              â”œâ”€ application insights
 IoT                                     â”œâ”€ recovery service vaults          â”œâ”€ api management services
 â”œâ”€ IoT Hub                              â”œâ”€ backup vaults (classic)          â”œâ”€ Visual Studio
 â”œâ”€ Event hubs                           â”œâ”€ site recovery vaults (classic)   â””â”€ Notification Hubs
 â”œâ”€ Stream Analytics jobs                â”œâ”€ scheduler job collections
 â”œâ”€ Machine learning studio workspaces   â”œâ”€ traffic manager profiles
 â”œâ”€ notification hubs                    â”œâ”€ advisor                         ÂºOtherÂº
 â”œâ”€ notification hubs namespaces         â”œâ”€ intune                           â”œâ”€ AZ ad domain ser.
 â”œâ”€ mach. learn. studio web srv plans    â”œâ”€ intune app protection            â”œâ”€ AZ ddbb migration services
 â”œâ”€ mach. learn. studio web srv          â”œâ”€ activity log                     â”œâ”€ AZ databricks
 â”œâ”€ function apps                        â”œâ”€ metrics                          â”œâ”€ batch ai
 â”œâ”€ machine learning experimentation     â”œâ”€ diagnostic settings              â”œâ”€ bot services
 â””â”€ machine learning model management    â”œâ”€ alerts                           â”œâ”€ classic dev srvs.
                                         â”œâ”€ solutions                        â”œâ”€ cloudamqp
ÂºEnterprise IntegrationsÂº                â””â”€ free services                    â”œâ”€ crypteron
 â”œâ”€ Logic Apps                                                               â”œâ”€ device provisioning srvs.
 â”œâ”€ integration accounts                                                     â”œâ”€ devops projects
 â”œâ”€ biztalk services                    ÂºAdd-onsÂº                            â”œâ”€ event grid subscriptions
 â”œâ”€ services bus                         â”œâ”€ new relic accounts               â”œâ”€ event grid topics
 â”œâ”€ api management services              â”œâ”€ mysql databases                  â”œâ”€ genomics accounts
 â”œâ”€ storessimple device managers         â”œâ”€ mysql databases clusters         â”œâ”€ location based services accts
 â”œâ”€ sql server strech databases          â”œâ”€ sendergrid accounts              â”œâ”€ logic apps custom connector
 â”œâ”€ data factories                       â”œâ”€ appdynamics                      â”œâ”€ mailjet email service
 â””â”€ relays                               â”œâ”€ aspera server on demand          â”œâ”€ managed applications
                                         â”œâ”€ bing maps api for enterprise     â”œâ”€ marketplace
ÂºSecurity + IdentityÂº                    â”œâ”€ cloudmonix                       â”œâ”€ migration projects
 â”œâ”€ security center                      â”œâ”€ content moderator                â”œâ”€ nosql (document db) accounts
 â”œâ”€ key vaults                           â”œâ”€ hive streaming                   â”œâ”€ nuubit cdn
 â”œâ”€ AZ active directory                  â”œâ”€ livearena broadcast              â”œâ”€ on-premises data gateways
 â”œâ”€ AZ ad b2c                            â”œâ”€ mycloudit - AZ desktop hosting   â”œâ”€ operation log (classic)
 â”œâ”€ multi-factor authentication (mfa)    â”œâ”€ myget-hosted nuget, npm, bower.. â”œâ”€ policy
 â”œâ”€ user and groups                      â”œâ”€ pokitdok platform                â”œâ”€ power bi embedded
 â”œâ”€ enterprise applications              â”œâ”€ ravenhq                          â”œâ”€ recent
 â”œâ”€ app registrations                    â”œâ”€ raygun                           â”œâ”€ resource explorer
 â”œâ”€ AZ ad connect health                 â”œâ”€ revapm cdn                       â”œâ”€ route filters
 â”œâ”€ AZ ad cloud app discorevy            â”œâ”€ signiant flight                  â”œâ”€ service catalog mnged appl. defs
 â”œâ”€ AZ add privileged identity mgn       â”œâ”€ sparkpost                        â”œâ”€ service health
 â”œâ”€ AZ ad identity protection            â”œâ”€ stackify                         â”œâ”€ storage sycn services
 â”œâ”€ AZ information protection            â”œâ”€ deep security saas               â”œâ”€ tags
 â”œâ”€ rights management (rms)              â”œâ”€ the identity hub                 â”œâ”€ templates
 â”œâ”€ access reviews                       â””â”€ marketplace add-ons              â”œâ”€ Time S. Insights env.
 â””â”€ app service certificates                                                 â”œâ”€ Time S. Insights event sources
                                                                             â”œâ”€ Time S. Insights Ref data sets
                                                                             â”œâ”€ ...
                                                                             â””â”€ whats new
</pre>

<pre zoom labels="azure,pricing,resource,billing,calculator,git">
<span xsmall>PRICING</span>
[Pricing  Calculator] @[https://azure.microsoft.com/es-es/pricing/calculator/]
[Azure Free Accounts] @[https://azure.microsoft.com/es-es/free/?ref=microsoft.com]
                      - 12 months free service
                      - â‚¬170 credits to test "anything" for 30 days
                      - 25 services free "for ever"

<hr/>
<span xsmall bgblue>Always-Free Products!!</span>
@[https://azure.microsoft.com/en-us/free/#always-free]
-ÂºCosmos DB (DDBB)Âº              -ÂºAzure DevOpsÂº                        -ÂºBatch (Compute)Âº
  5 GB                             5 users
  400 request unnits               (Unlimited private Git repos)        -ÂºAutomation: (Mgm.â…‹governance)Âº
                                                                          500 minutes job runtime
-ÂºApp Service (Compute)Âº         -ÂºSecurity Center (Security)Âº
  10 web, mobile or API apps       Policy Assessmentâ…‹Recommendations    -ÂºData Catalog (Analytics)Âº
                                                                          Unlimited
-ÂºFunctions (Compute)Âº           -ÂºAdvisor (Management and Governance)Âº
  1.000.000 requests per month     Unlimited                            -ÂºVirtual NetworkÂº
                                                                          50 virutal networks
-ÂºEvent Grid (Integration)Âº      -ÂºLoad Balancer (Networking)Âº
  100.000 opertations/month       Public load balanced IP(VIP)          -ÂºInter-VNET data transferÂº
                                                                          Inbound only
-ÂºAKS (Compute)Âº                 -ÂºData Factory (DDBB)Âº
                                   4 activities low frequency           -ÂºBandwidth (Data Transfer)Âº
-ÂºDeveTests LabsÂº                                                         5 GB outbound
                                 -ÂºSearch (Containers)Âº
-ÂºAD B2C (Identity)Âº               10.000 documents                     -ÂºVS CodeÂº
  50.000 Auths/month
                                 -ÂºNotification Hubs (Containers)Âº      -ÂºML ServerÂº
-ÂºService Fabric (Containers)Âº     1.000.000 Push Notifications           Free
                                                                          Devâ…‹Run Râ…‹Python models
-ÂºSQL Server 2017 Dev.Âº                                                   on preferred platform
</pre>

<pre zoom labels="azure,101,cli,devops,scripting,">
<span xsmall>az summary</span>
@[https://docs.microsoft.com/cli/azure]


$Âº$ az login        Âº â† Log in to Azure

$Âº$ az account      Âº â† Manage subscription info

$Âº$ az disk list    Âº â† List all Managed Disks.

$Âº$ az vm list      Âº â† List all VMs

$Âº$ az network      Âº â† Manage Network resources

</pre>



<pre zoom bgorange labels="azure,101,aaa,rbac,governance,devops,secrets,diagram,subscription,policy" id="azure_governance">
<span bgorange xsmall>Governance Hierachy</span>
REFS:
- @[https://www.credera.com/blog/credera-site/azure-governance-part-1-understanding-the-hierarchies/]
- @[https://www.credera.com/blog/credera-site/azure-governance-part-2-using-subscriptions-resource-groups-building-blocks/]
- @[https://www.credera.com/blog/credera-site/azure-governance-part-3-applying-controls-and-security-policies/]


â”Œâ†’ ENTERPRISE PREâ”€SETUP:
â”‚  â”€ Setup Azure AD ("OK" if already subscribed to Office 365).
â”‚    This allows existing AD IDs to be used for Enterprise enrollment,
â”‚    subscriptions, RBAC permissions, ...
â”‚
â”‚  â”€ Optional. Enable (onâ”€premises) Win.Server AD â†â†’ Azure AD  identity_synchronization
â”‚    (probably using Azure AD Connect) to centralize:
â”‚    â”€ identity management.
â”‚    â”€ federation.
â”‚    â”€Âºsingle signâ”€onÂº
â”‚
â”‚  ÂºAzure Enrollment Hierarchy: (Defined in Enterprise Portal)Âº
â”‚
â”‚         â˜ Access control starts from a billing perspective.â˜œ
â”‚
â”‚                     FUNCTIONAL         BUSINESS     GEOGRAPHIC    RESPONSIBLE
â”‚                                        UNIT                       PARTIES
â”‚   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â””â”€â”€ ENTERPRISE         Enterprise        Enterprise   Enterprise  â”€ 1+Enter.Admin
                       Enrollment        Enrollment   Enrollment  - Create Dptâ…‹Acc.Adms
@[https://ea.azure.com]     â”‚                â”‚         â”‚          - Read-only Admins
                            â”‚                â”‚         â”‚            to acc.billing
                            â”‚                â”‚         â”‚            â…‹purposes
                            â”‚                â”‚         â”‚          - 1 Tenant == AZ AD encompassing whole Org.
                            â”‚                â”‚         â”‚            (N subscriptions and users)
    â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”¼  â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”¼ â”€ â”€ â”€ â”€â”€â”‚â”€ â”€       â”€ â”€ â”€ â”€ â”€ â”€ â”€
    DEPARTMENTS        â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€...   â”Œâ”€â”´â”€...
                    Accounting  IT    Consumer        US           â† Department Admins
                                      Electronics    East
                       â”‚         â”‚       â”‚             â”‚
    â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”¼â”€ â”€ â”€  â”€ â”¼ â”€ â”€ â”€ â”¼ â”€ â”€ â”€ â”€ â”€ â”€ â”¼ â”€ â”€       â”€ â”€ â”€ â”€ â”€ â”€â”€ â”€
    ACCOUNTS           â”‚         â”‚       â”‚             â”‚
@[https://account.windowsazure.com]      â”‚             â”‚
              GÂº*2Âº Account   Account  Account      Account        â† 1+ admins
    manages          Owner     Owner    Owner        Owner           linked to 1 email
    subcriptions       â”‚         â”‚       â”‚             â”‚            - Belong to Azure AD
                       â”‚         â”‚       â”‚             â”‚              MS Win.Live acct
                       â”‚         â”‚       â”‚             â”‚              (personal use)
                       â”‚         â”‚       â”‚             â”‚            - Owns 1+ subscriptions
                       â”‚         â”‚       â”‚             â”‚            - ÂºDefault service adminÂº
                       â”‚         â”‚       â”‚             â”‚              Âºfor its subscriptionsÂº
    â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€ â”€â”‚â”€ â”€ â”€  â”€ â”¼ â”€ â”€ â”€ â”¼ â”€ â”€ â”€ â”€  â”€ â”€â”‚â”€ â”€
                      â”Œâ”´â”€â”€â”€â”     â”‚      â”Œâ”´...         ...
  OÂºSUBSCRIPTIONSÂº  App1 App1 Product. App1                        â† Subcription:
OÂºcontainer forÂº    Dev  Test Apps                                 - A singleÂºservice administratorÂºis
OÂºbilling â…‹    Âº    â”œâ”€ÂºResource Group01Âºâ†                            defined through the Azure Account
OÂºsecurity boundaryÂºâ”‚  â”‚                                             Portal.
                    â”‚  â”‚                                           - owners can be defined in Azure Portal
                    â”‚  â”‚                                           - co-admins can be added in Classic Portal
                    â”‚  â”œâ”€ storage acct                               (automatically added as subs. owners
                    â”‚  â”œâ”€ resource02                                 in Azure Portal)
                    â”‚  â”œâ”€ ...
                    â”‚
                    â”‚                                              @[https://portal.azure.com]
                    â”‚                                              ###########################
                    â”œâ”€ÂºResource Group02Âº
                    ...
                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                BÂºtip: use Âºresourceâ”€groupsÂº â†’ to group by common lifecycleÂº
                BÂº     and Âº(key/value)tagsÂº â†’ to bgroup by any other categoryÂº
                                               (prj=proj01,layer=frontend,...)

                $Âº$ az account show  --output table Âº â† Show current subcription
                  EnvironmentName    HomeTenantId    IsDefault  Name    State    TenantId
                  -----------------  --------------  ---------  ------  -------  ------------
                  AzureCloud         3048dc87-43...  True       name01  Enabled  3048dc87-...



                $Âº$ az account list --output table  Âº â† List of subscriptions for logged-in user
                  Name     CloudName   SubscriptionId  State    IsDefault
                  -------  ----------- -----------     -------  ---------
                  name01   AzureCloud  a3eb687a-...    Enabled  True
                  name02   AzureCloud  b89530ad-...    Enabled  False
                  ...

    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


      subscription N â†’ ONLY TRUST â†’ 1 Azure AD instance
                                      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                               - Used by subscription to authen.
                                 users, services, and devices.
                               - Used by ASM, ARM and Office 365.
                                         â””â”¬â”˜   â””â”¬â”˜
                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”
                       v                          v
                       ASM (Classic) Azure      â”‚ ARM  Azure
                       Service  Management      â”‚ Resource Manager
                     RÂºWARN: DeprecatedÂº        â”‚
                       â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      Microsoft      â”‚ can only add people to   â”‚ can only add people to
       Account       â”‚ admin. roles using their â”‚ admin. roles using their
      (Win.Live ID)  â”‚ Microsoft Accounts.Âº*1Âº  â”‚ Microsoft Accounts.Âº*1Âº
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      Nonâ”€Microsoft  â”‚ Can use Microsoft        â”‚ Can use Microsoft
      (Work,school)  â”‚ and/or Org Accounts      â”‚ and/or Org Accounts
      ,Account       â”‚                          â”‚
      â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      admin roles    â”‚ Account Admin(AA)Âº*2Âº    â”‚ Owner      : CRUD "everything"
                     â”‚ Service Admin(SA)Âº*2Âº    â”‚ Contributor: CRUD "everything" except resources
                     â”‚ Coâ”€Admin                 â”‚ Reader     : READ "everything"
                       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                      Any account in AD +
                      Windows Live IDs

      Âº*1Âº:  a new Azure AD directory can be created under the subscription,
             and then the default directory for subscription changed.
             ($ az login --tenant yourdir.onmicrosoft.com)

      Âº*2Âº: can add, remove, and modify Azure resources in subscription.
            default SA == AA.

        -Âºsubscription N â†’ trusts ONLY ONE â†’ Az.ADÂº
        -ÂºEach new subscription is provided with an existing/new AD to managerÂº
         Âºadmins/co-adminis of such subscription.Âº
        -Âº"SILO" for billing, key limits and security boundaryÂº
        - Azure limits (number of cores/resources, etc.).
        - Contains and organizes all resources and establishes
          governance principles over them.
        -ÂºVNet-PeeringÂº allows forÂºnetwork links between subscriptionsÂº

OÂºSUBSCRIPTION POLICIESÂº
  â”‚ ÂºBILLINGÂº
  â””â”€ÂºPOLICIESÂº(define key limits: number of VMs, storage accounts,...)
      â”œâ”€ Resource Tags
      â”œâ”€ÂºRESOURCE GROUPSÂº
      â”‚  â”‚- @[http://azure.microsoft.com/en-us/documentation/articles/azure-preview-portal-using-resource-groups/]
      â”‚  â”‚- logical "container" for resources sharing
      â”‚  â”‚BÂºcommon lifecycleÂº (web + ddbb + balancer +...)
      â”‚  â”‚  ^^^^^^^^^^^^^^^^
      â”‚  â”‚  Also used to group common policies, and access control
      â”‚  â”‚ (provision/update/decommission)
      â”‚  â”‚- Applications are commonly segregated into
      â”‚  â”‚  resource groups because they share a common
      â”‚  â”‚  Ex.:Blockchain Workbench resource-group
      â”‚  â”‚    @[https://docs.microsoft.com/en-us/azure/blockchain/workbench/deploy]
      â”‚  â”‚- Maps roles/users to resources.
      â”‚  â”‚- can contain resourcesÂºlocated in different regionsÂº.
      â”‚  â”‚  (but the group itself belongs to a well-defined region)
      â”‚  â”‚- can be used to scope access control for admin. actions.
      â”‚  â”‚
      â”‚  â”œâ”€ resource_1 â† â”‚resourceâ”‚ÂºNâ†â”€â†’1Âºâ”‚resource groupâ”‚
      â”‚  â”œâ”€ resource_2   â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  ğŸ–’    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”‚  â”œâ”€ ...          Resources can be added/removed/moved to/from
      â”‚  â””â”€ resource_n   a resource_group at any time
      â”‚     ^^^^^^^^^^
      â”‚     they can interact with other resources in diferent resource groups.
      â”‚
      â”œâ”€ÂºRBACÂº: fine-grained access
      â”‚  - can be assigned to a subscription and
      â”‚    inherited by resource groups.
      â”‚  - Admins can assign roles to users and groups at a specified
      â”‚    scope.(subscription, resource group, resource)
      â”‚    Ex:
      â”‚    - user1 allowed to manage Virt.Mach. in subcription
      â”‚      user2 allowed to manage DDBB       in subcription
      â”‚
      â”‚  -ÂºRBAC flavorsÂº:
      â”‚    â”œ Owner      : Full access + grant/delegate to other users
      â”‚    â”œ Contributor: Full access
      â”‚    â”” Reader     : view resources
      â”‚   ÂºËƒ30 pre-built rolesÂº(Network|DB|Virt.Mach  Contributor, ...)
      â”‚
      â”‚
      â”œâ”€ÂºResource LocksÂº
      â”‚  - Extra protection designed to
      â”‚   Âºprevent(accidental, non-desired) removalÂº of
      â”‚    existing resources ("storage account", ...)
      â”‚  - Apply to any scope
      â”œâ”€ÂºAzure AutomationÂº
      â””â”€ÂºAcure Security CenterÂº

</pre>

<pre zoom labels="azure,101,monitoring,diagram,TODO,">
<span xsmall>Monitor</span>
@[https://docs.microsoft.com/en-us/azure/azure-monitor/overview]

                  â”ŒAzure Monitorâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚               â§ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                  â”‚               â”‚ â”‚Insights                                      â”‚â”‚
                  â”‚               â”‚ â”‚Application   Container   VM   Monit.Solutionsâ”‚â”‚
                  â”‚               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                  â”‚               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                  â”‚               â”‚ â”‚Visualize                                     â”‚â”‚
Application    â”€â” â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚Dashboards    Views      PowerBI  Workbooks   â”‚â”‚
OS              â”‚ â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
Azure Resource  â”‚ â”‚ â”‚â”‚Metricsâ”‚â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
Subscription    â”œâ”€â”€â†’â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”œâ”€â”€â†’â”¤ â”‚Analyze                                       â”‚â”‚
Azure Tenant    â”‚ â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚   â”‚ â”‚Metric Analytics     Log Analysis             â”‚â”‚
Custom Source  â”€â”˜ â”‚ â”‚â”‚ Logs  â”‚â”‚   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                  â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚   â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚Respond                                       â”‚â”‚
                  â”‚               â”‚ â”‚Alerts         Autoscale                      â”‚â”‚
                  â”‚               â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                  â”‚               â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                  â”‚               â”‚ â”‚Integrate                                     â”‚â”‚
                  â”‚               â”‚ â”‚Logic Apps     Export_APIs                    â”‚â”‚
                  â”‚               â© â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>

<pre zoom labels="azure,auditing,devops,security,advisor,troubleshoot,billing,TODO">
<span xsmall>Troubleshoot</span>
ÂºAzure AdvisorÂº
- Automated consulting resource that examines current configurations
  and make practical recommendations in the following areas:
  - High availability (HA)
  - Security
  - Performance
  - Cost: potential cost savings such as underutilized VMs.
</pre>


<pre zoom labels="azure,arm,security,TODO">
<span xsmall>ARM Policies:</span>
BÂºARM custom policiesÂº
- explicit BÂºDENY mechanismÂº preventing users from
BÂºbreaking oranization standardsÂº to access resources.
- Commonly used to:
  - enforceBÂºnaming conventionsÂº,
  - control resource-types that can be provisioned.
  - require resource tags
  - restrict provisioning locations.

- To create a custom ARM policy:
  - Create a policy definition (JSON file) describing the
    actions and/or resources that are specifically denied.
In PowerShell:
New-AzureRmPolicyDefinition \
   -Name ServerNamingPolicyDefinition \
   -Description â€œPolicy to enforce server naming conventionâ€ \
   -Policy â€œC:\json\policyServerNaming.jsonâ€
</pre>

<pre zoom labels="azure,security,aaa,TODO">
<span xsmall>Security Center</span>
- best practice analysis and security policy management
  for all subscription (or resource group) resources.
- target: security teams, risk officers

- Application of advanced analytics (IA, behavioral analysis,...)
  leveraging global threat intelligence from Microsoft products
  and services, Microsoft Digital Crimes Unit (DCU), and Microsoft
  Security Response Center (MSRC), and external feeds.
</pre>




</div>
<div group><!-- IaaS -->
<span title>IaaS</span>
<hr/>
<div groupv>
<span title>VMs</span><br/>

<pre zoom labels="Azure,VM,IaaS,code_snippet,diagram">
<span xsmall>provision VMs</span>
BÂºCreation checklistÂº
  â”” start with the virtual networks (vnets)
  Â·
  â”” Choose a good name convention.
  RÂºWARNÂº: up to 15 chars (win) | 64 chars (linux)
    Ex: dev-usc-webvm01
    ENV LOCATION INTANCE PRODUCT  ROLE
    dev  uw      01      product1 SQL
    pro  ue      02      product2 web
    qa   ...                  ...

-BÂºworkload(VM size) classificationÂº
   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
   -BÂºgeneral purpose   Âº: balanced cpuâ”€toâ”€memory ratio.
   -BÂºcompute optimized Âº: high cpuâ”€toâ”€memory ratio.
   -BÂºmemory optimized  Âº: high memoryâ”€toâ”€cpu ratio.
   -BÂºstorage optimized Âº: high disk throughput and io.
   -BÂºgpu               Âº: targeted for heavy graphics/video editing.
                           model training and inferencing (deep learn)
   -BÂºhigh performance  Âº: fastest cpu VMs with optional highâ”€throughput
    BÂºcomputes          Âº: network interfaces.

    â˜ note: VM size can be up/down-graded while the VM is running
            but resizing will RÂºautomatically rebootÂº the machine and
          RÂºIP settings can changeÂº after reboot.
            - portal will filter out non-compatible choices with curent
              underlying hardware.
            - With stop/deallocate portal will allow any size for
              site/region, since VMs are removed and restarted in
              same/different server.


- OÂºPRICING MODELÂº
    â”” separate costs for:
      â””ÂºcomputeÂº:
      Â· (windows OS licence is included in price)
      Â· â””Âºpay-as-you-goÂº:
      Â·   priced per-hour,  billed per-minute
      Â· â””Âºreserved VM instancesÂº:
      Â·   -  upfront commitment for 1/3 years in location
      Â·   -BÂºup to 72% price savingsÂº(vs pay-as-you-go)
      â”” storage:
        â””OÂºSTORAGE ACCOUNTÂº
        Â·  â””OÂºprovides access to subscription objects in A.Storage Âº
        Â·  Â·
        Â·  â””OÂºVMs always have 1+ Storage accounts to hold each VMÂº
        Â·  Â·
        Â·  â”” Ussually linked to a resource-group. When the res.group is destroyed,
        Â·  Â· so is the storage account.
        Â·  Â· $ az storage account create \
        Â·  Â·   --resource-group rg01     \  â† link to this res.group
        Â·  Â·   --name st01               \
        Â·  Â·   --location eastus2        \
        Â·  Â·   --sku standard_lrs
        Â·  â””BÂºstorage account types:Âº
        Â·  Â· -BÂºstandardÂº: 500 i/o secs
        Â·  Â· -BÂºpremiumÂº : solid-state drives (ssds)
        Â·  â”” fixed-rate limit ofÂº20,000 I/O operations/secÂº per STORAGE ACCOUNT
        Â·                       Âº^^^^^^^^^^^^^^^^^^^^^^^^^Âº
        Â·                       Âºor up to 40 standard VHDÂº
        Â·                        ^^^^^^^^^^^^^^^^^^^^^^^^
        Â·                        use more STORAGE ACCOUNTS for
        Â·                        extra I/O | disks
        Â·
        â”” RÂºWARNÂº: storage resouces still billed when VMs are stopped/deallocated.
        Â· â˜At least 2 virtual hard disks(VHDs) per VM:
        Â·  - disk 1: OS (tends to be quite small)
        Â·  - disk 2: temporary storage.
        Â·  - Additional disks:
        Â·    - max number determined by VM Size selection
        Â·    - data for VHD BÂºheld in A.storage as page BLOBsÂº
        Â·
        â”” BÂºAzure allocates/bills space ONLY FOR THE STORAGE USEDÂº
        Â·
        â””Â·at disk creation GÂºtwo optionsÂº available for VHD:
          -Âºunmanaged disksÂº:
            -RÂºowner responsible for STORAGE ACCOUNTSÂº.
            - pay for the amount of space used.
          -Âºmanaged disksÂº:
            -GÂºrecommended, managing shifted to AzureÂº
            - "you" specify the disk size (up to 4 TB)
              and Azure creates and manages disk and storage.

-QÂºOS OPTIONSÂº
   ------------
   -QÂºWindowsÂº
   -QÂºLinuxÂº
   -QÂºMarketplace imagesÂº â† ex: Wordpress = Linux+LAMP
   -QÂºcustom 64-bit OS disk imageÂº:

<hr/>
<span xsmall>VM quotas</span>
@[https://docs.microsoft.com/en-us/azure/virtual-machines/windows/quotas]


<span xsmall>VM creation</span>
â†’ @[https://portal.Azure.com]  â†’ create resource
  â†’ marketplace â†’ search "windows server 2016 datacenter"
    â†’ create â†’ basics tab â†’ check subscription
      â†’ "create new resource group"
        â†’ Fill params:
          - VM name
          - region
          - availability options
          - image
          - size
          - administrator user/password.
          - "inbound port rules":
             - rdp (3389)
             - http
             - ...
            â†’ Click Âº"review + create"Âº
              â†’ Testing connection to new instance:
              Â· â†’ select "connect" on VM properties.
              Â·   â†’ "download rdp file"
              Â·     â†’ launch (local desktop) rdp client
              Â·       and select:
              Â·       "windows security" â†’ "more choices"
              Â·       â†’ "use different account"
              Â·
              â†’ Optional: save Resource Group as ARM template:
                â†’ ... VM instance â†’ settings â†’ automation script:
                  â†’ "save resource template for later use"

<hr/>
<span xsmall>A. VM extensions</span>
- small applications for simple provisioning automatition.
- use case:
  install/configure additional software on new VM
 ÂºAFTER INITIAL DEPLOYMENTÂº
- use specific configuration.
- monitor and execute automatically.
- trigger VM extension against (new|existing) VM deployment
  using cli, powershell, ARM templates or portal.
<hr/>
<span xsmall>Automation services</span>
- Requires a new ÂºAzure Automation AccountÂº.
- use case: "lot of infrastructure services"
- higher-level automation service.
- automate frequent, error-prone management tasks:
  -Âºprocess management automationÂº:
    provides VM with BÂºwatcher-tasksÂº reacting to
    specific BÂºerror event/sÂº in the VM/datacenter.
  -ÂºAzure configuration managementÂº:
    - track new software updates for the OS, filtering
      in/out (including/excluding) specific updates.
      and react accordingly.
    - manages all VMs, pcs, devices, ..
  -Âºupdate managementÂº:
    - enabled directly fromÂºAZURE AUTOMATION ACCOUNTÂºfor
      VMs or just a single VM from the VM blade in the portal.
    - manage updates and patches for your VMs.
      - assess status of available updates.
      - schedule installation.
      - review deployment results
    - provide (services) for process and config.management.
</pre>


<pre zoom labels="IaaS,azure,VM,code_snippet,sla">
<span xsmall>VMs availability Management</span>


 â”‚ FAULT â”‚     â”‚ FAULT â”‚
 â”‚DOMAIN1â”‚     â”‚DOMAIN2â”‚

 â”Š       â”Š     â”Š       â”Š
 â”‚ VM1   â”‚     â”‚       â”‚BÂºVM4Âº and BÂºVM5Âºin Fault Domain
 â”‚       â”‚     â”‚ VM2   â”‚  1/2 should be "mirrors" of
 â”‚ VM3   â”‚     â”‚       â”‚  each other
 â”‚â”ŒÂ·Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·Â·+Â·Â·Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â”
 â”‚Â·BÂºVM4Âºâ”‚     â”‚BÂºVM4Âº â”‚ Availability Â·
 â”‚Â·BÂºVM5Âºâ”‚     â”‚BÂºVM5Âº â”‚ Set          Â·
 â”‚â””Â·Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·Â·Â·Â·â”‚Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â”˜
    â†‘            â†‘
- at least two instances of each VM
  per Avai.Set.

- VMs and their managed-disks in an Avai.Set are
BÂºguaranteed to spread across DIFFERENTÂº:
  -BÂºfault domainsÂº : sharing common power/network.
  -BÂºupdate domainsÂº: sharing smae maintenance/reboot schedule.
  - avail. sets RÂºdo not protectÂºagainst software/data errors.
<hr/>
<span xsmall>A.Site RECOVERY</span>
-  replicates physical/VMs/resources/hyper-V/VMware
   from site/location 1 to  site/location 2.

-  enables Azure as recovery-destination.

-OÂºvery simple to test failoversÂº
 OÂºAFTER ALL, YOU DON'T HAVE A GOOD DISASTER RECOVERY PLANÂº
 OÂºIF YOU'VE NEVER TRIED TO FAILOVERÂº.

- recovery plans can include custom powershell scripts,
  automation runbooks, manual steps, ...
</pre>

<span title>Azure resource Manager (ARM)</span>
<pre zoom bgorange labels="azure,101,IaaS,resource_template">
<span xsmall>ARM</span>
 - ARM stands for Azure Resource Manager

   â”‚Resourceâ”‚ â† instantiates and provides Examples:
 â”Œâ†’â”‚Providerâ”‚   operations to work with  ÂºRESOURCE PROVIDER           RESOURCEÂº
 â”‚              resources                 -----------------           --------
 â”‚              â””â”€â”€â”€â”¬â”€â”€â”€â”˜                 microsoft.compute           VM
 â”‚       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     microsoft.storage           storage account
 â”‚       â†“                                microsoft.web               web apps related res.
 â”‚  â”‚Resourceâ”‚ : Azure VM,Disk,Queue,...  microsoft.keyvault          vault related res.
 â”‚       â†‘                                microsoft.keyvault/vaults â† Full resource name
 â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   "provider"/"resource"
 â”‚                       â”Œâ”€â”€â”´â”€â”€â”€â”€â”        tip: list existing providers
 â”‚   â”‚Resourceâ”‚ â† set of resources      $Âº$ az provider list | jq '.[]["namespace"]'Âº
 â”‚   â”‚Group   â”‚   sharing slifeâ”€cicle
 â”‚     â†‘
 â”‚     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚               â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
 â”‚ BÂºARMÂºmanages resource groups through Resource Providers
 â”‚                                       â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â”‚
 â”‚ â”‚JSON templateâ”‚ â†’ input â†’ BÂº|ARM|Âºâ†’ Evaluates input, dependencies and
 â”‚                                    "forwards" actions in correct order to
 â”‚                                     resource providers
 â”‚                                     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

<hr/>
<span xsmall>JSON templates</span>
 - Parameterized "macros" to allow replicated re-use of resource deployments.
 - Templates areBÂºMaintained by Dev/DevOps in gitÂº.
 - Each new solution created in Portal automatically generates
   an associated JSON template.  Click "download a template for automation"
   to fetch it.

    Ex.JSON template
     "resources": [                                  â” â”Œâ†’BÂºâ”‚ARMâ”‚
      {                                              â”‚ â”‚
       "apiversion": "2016-01-01",                   â”‚ â”‚   - parse input, map    to REST request
    â”Œâ”€â†’ "type":Âº"microsoft.storage/storageaccounts"Âº,â”‚ â”‚     to "microsoft.storage" â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
    Â·   "name": "mystorageaccount",                  â”‚ â”‚     Resource manager           Â·
    Â·   "location": "westus",                        â”œâ”€â”˜                                Â·
    Â·   "sku": { "name": "standard_lrs" },           â”‚                                  Â·
    Â·   "kind": "storage",                           â”‚                                  Â·
    Â·   "properties": { }                            â”‚                                  Â·
    Â· }                                              â”‚                                  Â·
    Â· ]                                              â”˜                                  Â·
    â””Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â”                                        Â·
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        v                                      â”‚
        PUT                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”Œ ${BASE_URL}/providers/Âºmicrosoft.storage/storageaccountsÂº/mystorageaccount?API-version=2016-01-01
      |
      | {
      |   "location": "westus",
      |   "properties": { }
      |   "sku": { "name": "standard_lrs" },
      |   "kind": "storage"
      | }
      |
      â”” ${BASE_URL}: https://management.Azure.com/subscriptions/${subsid}/resourcegroups/${resgrpnm} )

  - tip: you can create purupose-specific templates plus a BÂºMASTER TEMPLATEÂº linking all children.
    more info at:
  @[https://docs.microsoft.com/en-us/Azure/Azure-resource-manager/templates/linked-templates]

<div cli>
   $ az VM create \                           â† create new VM:
        --resource-group testresourcegroup \  â† link to existing Resource Group
        --name           test-wp1-eus-VM \
        --image          win2016datacenter \
        --admin-username jonc \
        --admin-password areallygoodpasswordhere
</div>
GÂºApplying a ARM JSON templateÂº:

 -GÂº STEP 1)Âº
  Edit/customize template (vim, VS Code+ARM extension, ...)

 -GÂºSTEP 2)
  create from template<div cli>
  $ az group deployment create \       â† deploy to reso. group
    --name $deploymentName \
    --resource-group $resourcegroupname \
    --template-file "AzureDeploy.JSON" â† alt 1: local input template
   (--template-uri https://...         â† alt 2:  http template)

  $ az storage account show \          â† check correct deploy
    --resource-group $resourcegroupname \
    --name $storageaccountname
</div>
</pre>

<pre zoom labels="azure,IaaS,encryption,secrets,storage,">
<span xsmall>A.Disk encryption</span>
- data at REST encrypted before writing

- main disk encryption technologies/options for VMs include:
  â””Âºstorage service encryption (SSE)Âº
    -Âºmanaged by "storage account" adminÂº
    - Default in BÂºmanaged disksÂº
    - 256-bit symmetric AES

  â””ÂºAzure disk encryption (ADE)Âº
    - managed by VM owner.
    - bitlocker(windows), dm-crypt (linux)
    - VMs boot underÂºcustomer-controlled keys and policiesÂº
    - integrated with Azure key vault
    -RÂºnot supported in basic tier VMsÂº
    -RÂºnot compatible with  on-premises key management service (KMS)Âº

-Âºpre-setup: create/update vaultÂº<div cli>
  $Âºaz keyvault createÂº           \ â† create (or update) keyvault
      --name OÂº$KV_NAMEÂº          \   in existing key-vault
      --resource-group $resrc_grp \
      --location $region          \ â† Âºmust be in same region of VMsÂº
                                              cross regional limits).
      --enabled-for-disk-encryption true  â† required for disk encryption</div>
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       three policies can be enabled.
       - disk encryption:
       - deployment     : allow microsoft.compute resource provider to
                          fetch secrets during resource creation.
       - template deploy: allow Azure resource manager to fetch
                          secrets when referenced in a template deploy.
<div cli>
$ azÂºVM encryption enableÂº               \ â† encrypt existing VM disk
    --resource-group $resource-group     \
    --name OÂº$VM-NAMEÂº                   \
    --disk-encryption-keyvaultOÂº$KV_NAMEÂº\
    --volume-type [all | OS | data]      \ â† encrypt all disks or just OS disk.
    --skipvmbackup

  $ az VM encryption show       \           â† check result
    --resource-group $resrc_grp \
    --name OÂº$VM-NAMEÂº
</div>

-To decrypt:<div cli>
$ azÂºVM encryption disableÂº\
  --resource-group $resrc_grp \
    --volume-type [all | OS | data]
  --name OÂº$VM-NAMEÂº
</div>
</pre>
</div>

<div groupv>
<span title>batch (jobs) Service</span>
<pre zoom labels="azure,IaaS,batch,diagram">
<span xsmall>Overview</span>
- Use-case: High-level-workloads for specific tasks like rendering, BigData, ...

ÂºPRE-SETUPÂº                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   - createÂºbatch account *1Âº         â”‚  Azure storage                  â”‚
    (az, portal, ...)                 â”‚                                 â”‚
                                      â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   - set pool params (number+size of   â”‚ 2)                ^ 4)
     nodes, OS image, apps to install  â”‚ download input    â”‚ upload task
     on nodes,...)Âº*5Âº                 v files and apps    â”‚ output
   - setÂºpool allocation mode: *2Âº   â”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   - (opt) associate storage acc.    â”‚  Azure batch                      â”‚
     to batch acc.*3                 â”‚                                   â”‚
                                     â”‚OÂºjobÂº sets the pool for its task  â”‚
                    1)               â”‚  computations                     â”‚
                    create pool,     â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   jobs and tasks   â”‚  â”‚job             task    task â”‚  â”‚
  â”‚             â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”˜  â”‚
  â”‚application  â”‚                    â”‚job n â†â†’ m pools*6 â”‚       â”‚       â”‚
  â”‚or service   â”‚  3)monitor tasks   â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”  â”‚
  â”‚             â”œâ†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’   â”‚                VM      VM*4 â”‚  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚  â”‚Âºpool of computer nodesÂº     â”‚  â”‚
                                     â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  note:
  when adding tasks to a job, the batch service automatically
  schedules the tasks associated application for execution.

  Âº*1Âº: 1+ batch account  per subscription.                  Âº*4Âº: every node is assigned a unique name
         (ussually 1 batch.acc per region)                      and ip address.  when removed from pool
         note: batch account n â†â†’ m batch workloads             node is resetted.
               batch account 1 â†â†’ n node pools (ussually 1)
    - batch account forms the base URL for auth:             Âº*5Âº: compute node type:
      https://{account-name}.{region-id}.batch.Azure.com        - dedicated   :
                                                                - low-priority: cheaper
  Âº*2Âº: pool allocation mode:                                   target number of nodes:
      - batch service (def): pools allocated transparently      ^^^^^^
      - user subscription: batch VMs,... are created            pool might not reach the desired
        directly in your subscription when pool is              number of nodes.  (max quota
        created.                                                reached,...)
        required to use Azure reserved VM instances.
        - you must also register your subscription           Âº*6Âº:
          with Azure batch, and associate the account        - a pool can be created for each job   or
          with an Azure key vault.                           - a pool can be reused  for dif  jobs.
                                                             - jobs can be assigned priorities.
  Âº*3Âº: following storage account supported:                   tasks in jobs with higher priority are
    - general-purpose v2 (gpv2) accounts                       inserted into the queue ahead of
    - general-purpose v1 (gpv1) accounts                       others.  lower-priority tasks already
    - BLOB storage accounts (currently                         running are not preempted.
      supported for pools in VM config)

  - max. number of failed-task retries   - client app. can add tasks to a job or
    constraint can be set                  a job manager task can be specified
                                           with the needed info to create the
                                           required tasks for a job
                                           (job mgr taks required for jobs
                                            created by job schedule)

  - by default, jobs remain in             Âºapplication packagesÂº
    active-state when all tasks complete.  - it allows to upload/manage multiple
    use 'onalltaskscomplete' property        app verssion run by tasks.
    change the default behaviour.          - application packages can be set for:
                                             - pool: app deploy.to every node
                                             - task: app deploy.to just  node/s
                                                     scheduled to run task

BÂºQUOTAS AND LIMITSÂº
  resource quotas
  resource                             default limit   maximum limit
  batch accounts
  per region/per subscription          1 - 3           50
  dedicated cores per batch account    10 - 100        n/a
  low-priority cores per batch account 10 - 100        n/a  *1
  active jobs and job schedules        100 - 300       1000
  per batch account
  pools per batch account              20 - 100        500

  *1 if pool allocation mode == "user subscription"
     batch VMs and other resources are created directly
     in the user subscription at pool creating.
     subscription quotas for regional compute cores
     will apply

Note: tightly coupled applications can also be run using of the new
     Âºtwo message passing interface(MPI) APIÂº: (Microsoft MPI or Intel MPI)
      as alternative to Batch jobs

</pre>

<pre zoom labels="azure,IaaS,batch">
<span xsmall>run batch job (cli)</span>
<div cli>
$ az group create \            â† create resource group
  --name rg01 \
  --location eastus2

$ az storage account create \  â† create storage account
  --resource-group rg01     \
  --name st01               \
  --location eastus2        \
  --sku standard_lrs

$ az batch account create   \  â† create batch account
  --name mybatchaccount     \
  --storage-account st01    \  â† link with storage account
  --resource-group rg01     \
  --location eastus2

$ az batch account login    \  â† authenticate to batch service.
    --name mybatchaccount   \
    --resource-group rg01   \
    --shared-key-auth

$ image="canonical:ubuntuserver:16.04-lts"
$ a_sk_id="batch.node.ubuntu 16.04"
$ az batch pool create       \ â† create pool
  --id mypool                \
  --VM-size standard_a1_v2   \
  --target-dedicated-nodes 2 \ â† pool node size
  --image ${image}           \ â† (linux image)
  --node-agent-sku-id $a_sk_id

$ az batch pool show   \       â† check status
  --pool-id mypool     \
  --query "allocationstate"    â† (pool created in 'resizing'
                                 state while nodes are
                                 bootstraped)


$ az batch job create \        â† create job.
  --id myjob          \
  --pool-id mypool             â† link to pool


$ script="/bin/bash -c '...'"
$ az batch task create \       â† create 1st task
  --task-id mytask01   \
  --job-id myjob       \
  --command-line "${script}"

$ az batch task show \         â† check task status
  --job-id myjob \               amongs output many details,
  --task-id mytask01             take note of 'exitcode'
                                 and 'nodeid'.

$ az batch task file list \    â† list output files
  --job-id myjob          \
  --task-id mytask1       \
  --output table

$ az batch task file download \ â† download file
  --job-id myjob \
  --task-id mytask1 \
  --file-path stdout.txt \      â† remote file
  --destination ./stdout.txt    â† local destination
</div>
</pre>

<span title>create containerized solutions</span>
<pre zoom labels="azure,IaaS,aks,k8s,container">
<span xsmall>AKS core concepts</span>
- managed RÂºsingle-tenantÂº kubernetes service
OÂºpriceÂº: pay by number of AKS nodes running apps.
- built on top of OOSS A.container service engine
  (acs-engine).
- creation params:
   - number of nodes
   - size   of nodes.
- aks upgrades orchestrated through cli or portal.
- cluster master logs available in Azure log analytics.

- the following RÂºcompute resources are reservedÂº on each worker
  node for kubelet, kube-proxy, and kube-dns:
  Â· CPU    : 60ms
  Â· RAM    : RÂº20% up to 4 gibÂº
             ex: RAM: 7GIB -RÂº1.4GIBÂº(20%) = 5.6 GIB free for OS+Apps

- RÂºresource reservations cannot be changedÂº

ÂºAKS node pools:Âºnodes with same configuration

 AKS cluster 1 â†â†’ n node pools.
                  ^
                - just 1 default-node-pool defined at AKS resource creation
                - scaling is performed against default-node-pool.

BÂºHA appÂº "==" deployment controller + deployment
                                       ----------
                                       pod replicas set
                                     + container image/s
                                     + attached storage
                                     + ...
                                       ----------
                                       ^deployment can be updated.
                                       dep.control. will take care
                                       or draining/updating

- BÂºdeployment pod-disruption-budgetsÂºcan be set to define
  the â˜ BÂºminimum quorum needed for correct app behaviourÂº.
  This is taking into account when draining existing nodes
  during update. Ex: A cluster of 5 nodes can have a maximum
  of 1 node failure, so controllers will get sure that no
  more than 2 nodes are drained.

-Âºdefault namespacesÂº at cluster creation:
  Â·ÂºdefaultÂº    : default ns for pods and deployments
  Â·Âºkube-systemÂº: reserved for core resources (dns,proxy,dashboard,...)
  Â·Âºkube-publicÂº: typically unsed, visible in whole cluster by any user.


BÂºAKS ACCESS AND IDENTITYÂº
  -Âºk8s service accountsÂº:
    - one of the primary user types in k8s
    - targeting services (vs human admins/developers).
    - it  exists and is managed by k8s API.
    - credentials stored as k8s secrets.
      - can be used by authorized pods to communicate with API server.
        (authentication token in API request)

  -Âºnormal k8s user accountÂº:
    - "traditional" access to human admins/developers
    - no account/password is stored  but external identity
      solutions can be "plugged in".
      (a.active directory in Azure aks)

  - AD-integrated aks allows to grant users or groups access to
    kubernetes resources per-namespace or per-cluster:
    -ÂºOpenID connect is used for authenticationÂº, identity layer
      built on top of the OAuth 2.0 protocol.
    - to verify OpenID auth-tokens the aks cluster used
      webhook token authentication. (webhook == "http callback")

    $Âºaz aks get-credentialsÂº â† obtain kubectl configuration context.

  user are prompted to sign in with their Azure AD credentials
  before interacting with aks cluster through kubectl.

BÂºk8s RBACÂº
  - granted per-namespace / per-cluster
  - granted per-user      / per-user-group
  - fine control to:
  -  user/groups n â†-â†’ m namsp.role  1 â†â†’ n permission grant
                    â”‚   ( or             - create resource.^
                    â”‚    clust.role)     - edit   resource.â”‚
                    â”‚                    - view appâ”€logs   â”‚
                    â”‚                    - ...             â”‚
                    â”‚               â˜ no concept of deny â”€â”€â”˜
                    â”‚                 exists, only grant

             (ns)   rolebindings
             clusterrolebindings
  - Azure RBAC:
    - extra mechanism for access control
    - designed to work on resources within subscription.
    - Azure RBAC roles definition outlines permissions to be applied.
    -  a user or group is then assigned this role definition for
       a particular scope (individual resource/resource-group/subscription)

BÂºaks security concepts for apps and clustersÂº
  - master security:
    - by default kubernetes API server uses a public ip address+fqdn.
    - API control access includes RBAC and AD.
  - worker node security:
    - private virtual network subnet,no public ip
    - ssh is enabled by default to internal ip address.
    - Azure network security group rules can be used to further
      restrict ip range access to the aks nodes.
    - managed disks automatically encrypted at REST used for storage.
  - network security
    - Azure virtual network subnets: they require site-to-site vpn or
      express route connection back to on-premises network.
    - k8s ingress controllers can be defined with private ip
      addresses so services are only accessible over this
      internal network connection.

  -ÂºAzure network security groupsÂº
   - rules defining allowed/denied traffic based on sourceâ…‹destination ip ranges/ports/protocols
   - created to allow k8s-API-tls traffic and nodes-ssh access.
     modified automatically when adding load balancers, port mappings,
     ingress routes.

  -Âºk8s secretsÂº
    - used to inject sensitive data into pods.
    - k8s API used to create new secret. "consumed" by pods/"deployments".
    - stored in tmpfs, never in disk. tmpfs deleted "as soon as possible"
      when no more pods in node require it.
    - only accesible within secrete namespace.

BÂºNETWORK CONCEPTS FOR APPS IN AKSÂº
- HA apps network options:
  - load balance
  - set tsl ingress traffic termination or routing of multiple components.
    - for security reasons, network traffic flow must be restrictec
      into|between pods and nodes.

  - k8s provides an abstraction layer to virtual networking:
    WORKER                     K8S
   ÂºNODESÂº â”€â”€ connected to â”€â”€ Virt.Network
     â”‚
     â””â†’ provides (in/out)bound rules to ÂºpodsÂº
        through kube-proxy

  -ÂºservicesÂº
    - logically group pods to allow for direct access via an
      ip address or dns name and on a specific port.

  - aks allowsGÂºtwo network models for k8s clustersÂº:
    -GÂºbasic networking   Âº: network resources createdâ…‹configured at cluster creation.
                           (default) subnet names, ip address range can not be customized.
                           following features are provided:
                           - expose a kubernetes service externally|internally through
                             Azure load balancer.
                           - pods can access resources on the public internet.
    -GÂºadvanced networkingÂº: aks cluster is connected to existing virtual
                            network resources and configurations.
                            this vir.NET provides automatic connectivity to other
                            resources and integration with a rich set of capabilities.
                            - nodes will use the Azure container networking
                              interface (cni) kubernetes plugin.
                            - every pod is assigned an ip address in the vir.NET.
                            - pods can directly communicate with other pods and
                              nodes in the virtual network.
                            - a pod can connect to other services in a peered
                              virtual network, including Âºon-premises networksÂº
                              over expressroute and site-to-site (s2s) vpns.
                             Âºpods are also reachable from on-premisesÂº.
                            - pods in a subnet that have service endpoints
                              enabled can connect to services like a.storage, SQL DB,....
                            - allows for user-defined routes (udr) to route traffic
                              from pods to a network virtual appliance.

  -ÂºIngress controllers:Âº
    - use case: complex application traffic control.
    - an ingress resource can be created with nginx, ..., or se
      with the aks http application routing feature.
      (an external-dns controller is also created and
       required dns a records updated in a cluster-specific
       dns zone)

BÂºSTORAGE OPTIONS FOR Deployments (APPS)Âº
  -k8s BÂºvolumesÂº
  -k8s BÂºpersistent volumesÂº
  -BÂºstorageclassesÂº <!-- storage class -->
   - alternative to persistent volume
   - to define different tiers of storage (premium vs standard,
     disk vs file, ...)
     used normally for dynamic provisioning,

  - two initial storageclasses are created:
    â˜Âºtake care when requesting persistent volumes so that theyÂº
     Âºuse the appropriate storage needed.Âº
   ÂºdefaultÂº: use  Azure standard storage to create a managed disk.
              reclaim policy: Azure disk deleted@pod termination.
   Âºmanaged-premiumÂº: use Azure premium storage to create managed disk.
              reclaim policy: Azure disk deleted@pod termination.
    new ones can be create through using 'kubectl'.


kind:ÂºstorageclassÂº
apiversion: storage.k8s.io/v1
metadata:
  name: managed-premium-retain
provisioner: kubernetes.io/Azure-disk
reclaimpolicy: retain    â† delete*|retain: what to do
                           with underlying storage
parameters:                (a.disk) when the pod is deleted
  storageaccounttype: premium_lrs
  kind: managed

apiversion: v1                       kind:ÂºpodÂº
kind:ÂºpersistentvolumeclaimÂº â†Â·Â·Â·Â·Â·â” apiversion: v1
metadata:                          Â· metadata:
  name: Azure-managed-disk         Â·   name: nginx
spec:                              Â· spec:
  accessmodes:                     Â·   containers:
  - readwriteonce                  Â·     - name: myfrontend
  storageclassname: managed-premiumÂ·       image: nginx
  resources:                       Â·       volumemounts:
    requests:                      Â·       - mountpath: "/mnt/Azure"
      storage: 5gi                 Â·         name: volume
                                   Â·   volumes:
                                   Â·     - name: volume
                                   Â·       persistentvolumeclaim:
                                   â””Â·Â·Â·Â·Â·    claimname: Azure-managed-disk

BÂºSCALING OPTIONSÂº
  - manually scale podsâ…‹nodes
    - you define the replica or node count, and k8s schedules
      creation/draiing of pods
  - horizontal pod autoscaler (ÂºhpaÂº)
    - hpa monitors demand and automatically scale the number of replicas.
      by querying the Âºmetrics APIÂº (k8s 1.8+) each 30secs
    - when using the hpa in a deployment, minâ…‹max number of replicas are set.
      optionally metric-to-monitor is also defined (cpu usage,...).
    - to minimize these race events (new metrics arrive before creation/draing
      has taken place) cooldown/delay can be set.
     (how long hpa will wait after first event  before another one is triggered)
      default: 3 min to scale up, 5min to scale down
  -Âºcluster autoscalerÂº
    - it adjusts number ofÂºnodesÂº (vs pods) based on the requested compute resources
      in the Âºnode poolÂº. it checks API server every 10 secs.
    - typically used alongsideÂºhpaÂº.
  -ÂºAzure container instance (aci) integration with aksÂº

  - burst to Azure container instances
    - rapidly scale aks cluster.
    - secured extension to aks cluster. virtual kubelet component is
      installed in aks cluster that presents aci as a virtual
      kubernetes node.

BÂºdeveloper best practicesÂº
  - define pod resource requests and limits on all pods.
    (note:  deployment will be rejects otherwise if the cluster uses resource quotas)
    primary way to manage the compute resources. it provides good hints to
    the k8s scheduler

    kind: pod
    apiversion: v1
    metadata:
      name: mypod
    spec:
      containers:
      - name: mypod
        image: nginx:1.15.5
        resources:
          requests:         â†         amount of cpu/memory needed  by pod.
            cpu: 100m
            memory: 128mi
          limits:           â† maximum amount of cpu/memory allowed to pod.
            cpu: 250m
            memory: 256mi

  - develop and debug applications against an aks cluster using dev spaces.
    - this ensures that RBAC, network or storage needs are ok before deployment
    - with Azure dev spaces, you develop, debug, and test applications directly
      against an aks cluster.
    - visual studio code extension is installed for dev spaces that gives an
      option to run and debug the application in an aks cluster:

  - visual studio code extension provides intellisense for k8s resources,
    helm charts and templates. you can also browse, deploy, and edit
    kubernetes resources from within vs code. the extension also provides an
    intellisense check for resource requests or limits being set in the pod
    specifications:

  - regularly check for application issues with 'kube-advisor' tool
    to detect issues in your cluster.
    - kube-advisor scans a cluster and reports on issues that it
      finds.
</pre>

<pre zoom labels="azure,IaaS,aks">
<span xsmall>deploy cluster(cli)</span>
(related application code, dockerfile, and k8s manifest available at:
@[https://github.com/Azure-samples/Azure-voting-app-redis])

Âºpre-setupÂº
 $ az group create     \         â† create a resource group
   --name myakscluster \
   --location eastus

 $ az aks create  \               â† create aks cluster
   --resource-group myakscluster \
   --name myakscluster \
   --node-count 1
   --enable-addons monitoring \   â† availables in a.portal
   --generate-ssh-keys              after a few minutes
  (wait a few minutes to complete)  portal â†’ resource group
                                     â†’ cluster â†’ monitoring â†’ insights(preview)
                                       â†’ choose "+add filter" â†’ select namespace as property â†’
                                         â†’ choose "all but kube-system"
 $ az aks install-cli command     â†       â†’ choose "view the containers"install kubectl locally

 $ az aks get-credentials \       â† download credentials and
  --resource-group myakscluster \   configure kubectl touse them.
  --name myakscluster

 $ kubectl get nodes              â† verify setup
 (output will be similar to)
 â†’ name                          status    roles     age       version
 â†’ k8s-myakscluster-36346190-0   ready     agent     2m        v1.7.7
 â†’

Âºrun the applicationÂº

$ kubectl apply -f Azure-vote.yaml   â†â”€â”€â”
â†’ deployment "Azure-vote-back" created  â”‚
â†’ service "Azure-vote-back" created     â”‚
â†’ deployment "Azure-vote-front" created â”‚
â†’ service "Azure-vote-front" created    â”‚
                                        â”‚
$ cat Azure-vote.yaml â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
01	  apiversion: apps/v1
02	  kind:ÂºdeploymentÂº
03	  metadata:
04	    name: Azure-vote-back
05	 Âºspec:Âº
06	    replicas: 1
07	    selector:
08	      matchlabels:
09	        app: Azure-vote-back
10	   Âºtemplate:Âº
11	      metadata:
12	        labels:
13	          app: Azure-vote-back
14	     Âºspec:Âº
15	        containers:
16	        - name: Azure-vote-back
17	         Âºimage:Âºredis
18	          resources:
19	            requests:
20	              cpu: 100m
21	              memory: 128mi
22	            limits:
23	              cpu: 250m
24	              memory: 256mi
25	          ports:
26	          - containerport: 6379
27	            name: redis
28	  --- #separator
29	  apiversion: v1
30	  kind:ÂºserviceÂº
31	  metadata:
32	    name: Azure-vote-back
33	  spec:
34	    ports:
35	    - port: 6379
36	    selector:
37	      app: Azure-vote-back
38	  --- #separator
39	  apiversion: apps/v1
40	  kind:ÂºdeploymentÂº
41	  metadata:
42	    name: Azure-vote-front
43	 Âºspec:Âº
44	    replicas: 1
45	    selector:
46	      matchlabels:
47	        app: Azure-vote-front
48	   Âºtemplate:Âº
49	      metadata:
50	        labels:
51	          app: Azure-vote-front
52	     Âºspec:Âº
53	        containers:
54	        - name: Azure-vote-front
55	         Âºimage:Âºmicrosoft/Azure-vote-front:v1
56	          resources:
57	            requests:
58	              cpu: 100m
59	              memory: 128mi
60	            limits:
61	              cpu: 250m
62	              memory: 256mi
63	          ports:
64	          - containerport: 80
65	          env:
66	          - name: redis
67	            value: "Azure-vote-back"
68	  --- #separator
69	  apiversion: v1
70	  kind:ÂºserviceÂº
71	  metadata:
72	    name: Azure-vote-front
73	 Âºspec:Âº
74	    type:ÂºloadbalancerÂº
75	    ports:
76	    - port: 80
77	    selector:
78	      app: Azure-vote-front

BÂºtest the applicationÂº

  $ kubectl get service Azure-vote-frontÂº--watchÂº â† monitor deployment progress
  â†’ name               type           cluster-ip   external-ip   port(s)        age
  â†’ Azure-vote-front   loadbalancer   10.0.37.27   52.179.23.131   80:30572/tcp 2m
  â†’                                                ^
  â†’                                                initially "pending"

BÂºdelete clusterÂº
  $ az aks delete \                    â† once finished delete the cluster
    --resource-group myresourcegroup \
    --name myakscluster --no-wait

  note: Azure active directory service principal used by the aks cluster is not removed.
</pre>

<pre zoom labels="azure,IaaS,docker,k8s,container">
<span xsmall>publish image</span>
BÂºAzure container registry overviewÂº
  - managed docker registry serviceÂºbased on ooss docker registry 2.0.Âº

  - Azure container registry build (acr build):
    - used to build container images in Azure.
      on-demand or fully automated builds from source code commit

    registry 1â†â†’n repositories 1 â†â†’ n images
                  (multilevel)
  - registries are available in three skus:
    - basic   : cost-optimized for learning purposes
    - standard: increased storage limits and image throughput.  (production)
    - premium : higher limits in storage (high-volume)/concurrent ops,
                geo-replication (single registry across multiple regions)

    - webhook integration is supported
    - registry authentication with AD, and delete functionality.
    - a fully qualified registry name has the form myregistry.Azurecr.io.

    - access control is done through AD service principal or provided
      admin account.
      $ docker login' â† standard command to auth with a registry.

  -ÂºrepositoryÂº: a registry contains one or more repositories, which store
    groups of container images. a.cont.registry supports multilevel repo
    namespaces allowing to group collections of images related to a specific app,
    or a collection of apps to specific development or operational teams.
    ex:
    myregistry.Azurecr.io/aspnetcore:1.0.1          â† corporate-wide image
    myregistry.Azurecr.io/warrantydept/dotnet-build â† .NET apps build-image/s
                                                      across 'warranty' department
    myregistry.Azurecr.io/warrantydept/customersubmissions/web â† web image, grouped
                                                       in the customer submissions app,
                                                       owned by 'warranty' department

  -ÂºimageÂº: read-only snapshot of a docker-compatible container.

  ÂºAzure container registry tasksÂº
  - suite of features within Azure container registry that provides streamlined
    and efficient docker container image builds in Azure.
    - automate container OS and framework patching pipeline,
      building images automatically when your team commits code
      to source control.
    - Âºmulti-step tasksÂº (preview feature)provides step-based task
      definition and execution for building, testing, and patching container images
      - task steps define individual container image build and push
        operations.
      - they can also define the execution of one or more containers, with
        each step using the container as its execution environment.

BÂºdeploy image with cliÂº
  $ az group create \
    --name myresourcegroup \
    --location eastus

  $ az acr create \               â† create acr instance
    --resource-group myresourcegroup \
    --name mycontainerregistry007

  $ az acr login --name $acrname  â† log in to acr
                                    needed before push/pull

  $ query="[].{acrloginserver:loginserver}"
  $ az acr list \                      â† obtain full login server name
    --resource-group myresourcegroup \   of the acr instance.
    --query "${query}"
    --output table

  $ docker tag \                      â† tag image
  microsoft/aci-helloworld \        â† local existing image
  $acrloginserver/aci-helloworld:v1 â† new tag. fully qualified name of
                                      acr login server ($acrloginserver)
                                      needed

  $ docker push \                     â† push image to acr instance.
    $acrloginserver/aci-helloworld:v1

  $ az acr repository list \          â† check 1: image is uploaded
    --name $acrname --output table
  â†’ result
  â†’ ----------------
  â†’ ...
  â†’ aci-helloworld

  $ az acr repository show-tags \     â† check 2: show tag
    --name $acrname \
    --repository aci-helloworld
    --output table
  â†’ result
  â†’ --------
  â†’ ...
  â†’ v1

BÂºDEPLOY IMAGE TO ACIÂº (A.Container instance: "Docker?")
  $ az acr update \     â† enable the admin user on your registry
    --name $acrname \       â†‘
    --admin-enabled true  â˜ in production scenarios
                            you should use a service
                            principal for container
                            registry access


  $ az acr credential show \    â† retrieve password
    --name $acrname \           â† once admin is enabled username == "registry name"
    --query "passwords[0].value"

  $ az container create \       â† deploy container image
     --resource-group myresourcegroup \
     --name acr-quickstart \
    --image ${acrloginserver}/aci-helloworld:v1
    --cpu 1 --memory 1 \        â† 1 cpu core, 1 gb of memory
    --registry-username $acrname \
    --registry-password $acrpassword
    --dns-name-label aci-demo --ports 80

  (an initial feedback from a.resource manager is provided
   with container details).

  $ az container show \         â† repeate to "monitor" container status
    --resource-group myresourcegroup \
    --name acr-quickstart \
    --query instanceview.state
    of your container and check


  $ az container show \                â† retrieve container's fqdn
    --resource-group myresourcegroup \
    --name acr-quickstart \
    --query ipaddress.fqdn             â†
  ex output:
  "aci-demo.eastus.Azurecontainer.io"

  $ az container logs \                â† show container logs
    --resource-group myresourcegroup \
    --name acr-quickstart
</pre>

<span title>Network</span>
<pre zoom labels="azure,TODO">
<span xsmall>Networking</span>
@[https://docs.microsoft.com/en-us/office365/enterprise/microsoft-cloud-networking-for-enterprise-architects]
</pre>
<pre zoom labels="azure,TODO">
<span xsmall>Hybrid Cloud</span>
@[https://docs.microsoft.com/en-us/office365/enterprise/microsoft-hybrid-cloud-for-enterprise-architects">Microsoft Hybrid Cluod forEnterprise Architecs]
</pre>
</div>
</div> <!-- IaaS -->

<div group><!-- App Service -->
<span title>App Development: App Service Platform</span>
<hr/>
<div groupv>
<pre zoom labels="azure,dev_framework,comparative,webapps">
<span xsmall>Web apps</span>
- (app service) Âºweb appsÂº is the recomended service for webapps development.
  - for microservice architecture, (App Service)Âºservice fabricÂº could be better.
  - for full control, A.VMs could be better.

-BÂºapp service (webapps, service fabric, ...)plansÂº
  - An app runs in an app service plan.
  - a newÂºapp service planÂºin a region automatically creates new
    associated compute resources where all apps in plan will share resources.
    app service plan defines:
    ----------------
    - region (west us, east us, etc.)
    - number of VM-instances â† Âºeach app will run on all the VMsÂº
                               Âºmultiple app deployment slots also run on those VMsÂº
                               Âºenabled logs-diagnosis/backups/webjobs also share the VMsÂº
    - size of VM instances (small, medium, large)
    - pricing : Determine price and available features.
      tier
               -Âºshared compute:Âºfreeâ…‹shared base tiers, runs an app in
                               RÂºmulti-app/multi-customer shared VMsÂº
                 -  cpu quotas for each app is allocated.
                 -RÂºit doesn't scale outÂº
                 (target: development/testing/...)
               -Âºdedicated compute:Âºbasic/standard/premium/premiumv2
                 -Âºapps in same app-service planÂºshare dedicated AzureÂºVMsÂº
                 -BÂºthe higher the tier, the more VM instances to scale-outÂº
               -Âºisolated:Âº
                 - dedicated VMs on dedicated virt.nets for apps
                 - use-case: app is resource-intensive, independent scale-out
                   from other apps in plan.
                 - the app needs resource in a different geographical region.
                 -BÂºmaximum scale-out capabilitiesÂº
               -Âºconsumption:Âºonly available to function apps, that
                             BÂºscale dynamically on demandÂº
<hr/>
<span xsmall>create new</span>
 ÂºBASH                                                      POWERSHELLÂº
 -----------------------------------------------------      -----------

$ gitrepo=.../Azure-samples/php-docs-hello-world            $gitrepo=".../Azure-samples/app-service-web-dotnet-get-started.git"
$ webappname=mywebapp$random                                 $webappname="mywebapp$(get-random)"
$ location=westeurope                                        $location="west europe"

$ az group create \           â† create resource group   â†’    new-Azurermresourcegroup
  --location $location  \                                      -name group01
  --name $webappname                                           -location $location

$ az appservice plan create \ â† create app service plan â†’   new-Azurermappserviceplan
  --name $webappname \                                         -name $webappname
                        \                                      -location $location
  --resource-group group01 \                                   -resourcegroupname group01
  --sku free                  â†  use free tier          â†’      -tier free

$ az webapp create \          â† create web app          â†’    new-Azurermwebapp
  --name $webappname \                                         -name $webappname
                        \                                      -location $location
  --resource-group group01 \                                   -appserviceplan $webappname
  --plan $webappname                                           -resourcegroupname group01

$ az webapp deployment \      â† deploy code from        â†’    $propertiesobject = @{
  source config \               git repo              â”ŒÂ·â†’        repourl = "$gitrepo";
  --name $webappname \                                â”‚          branch = "master";
  --resource-group group01 \                          â”‚          ismanualintegration = "true";
  --repo-URL $gitrepo \       â† git source   Â·Â·Â·Â·Â·Â·Â·Â·â”€â”˜      }
  --branch master \
  --manual-integration                                       set-Azurermresource
                                                               -propertyobject $propertiesobject
                                                               -resourcegroupname group01
                                                               -resourcetype microsoft.web/sites/sourcecontrols
                                                               -resourcename $webappname/web
                                                               -apiversion 2015-08-01
                                                               -force
- Finally test deployed app with browser:
  http://$webappname.Azurewebsites.NET

$ az group delete \       â† clean up after finishing.  â†’     remove-Azurermresourcegroup
  --name myresourcegroup                                       -name group01
                                                               -force
</pre>
<pre zoom labels="azure,webapps,dev_framework,">
<span xsmall>Runtime patching</span>
- OS patching includes:
  - physical servers.
  - guest VMs running App service.
- both OS and runtimes are automatically updated aligned
  with theÂºmonthly patch tuesday scheduleÂº.
- new stable versions of supported language runtimes
  (major, minor, or patch) are periodically added to app
  service instances.
  - some updatesÂºoverwriteÂºinstallation and apps
    automatically run in updated runtimes at restart.
    (.NET, php, java SDK, tomcat/jetty)
  - some others do a Âºside-by-sideÂºinstallation.
    Devs/DevOps must manually migrate the app
    to the new runtime version. (node.JS, python)

    if app-service-setting was used to configure runtime,
    change it manually like:
  $Âº$ common="--resource-group $groupname --name $appname"       Âº
  $Âº$ az webapp config set --net-framework-version v4.7 $common  Âº
  $Âº$ az webapp config set --php-version            7.0 $common  Âº
  $Âº$ az webapp config set --python-version         3.4 $common  Âº
  $Âº$ az webapp config set --java-version 1.8           $common  Âº
  $Âº$ az webapp config set --java-container tomcat      $common  Âº
  $Âº                       --java-container-version 9.0          Âº
  $Âº$ az webapp config appsettings set --settings       $common \Âº
  $Âº  website_node_default_version=8.9.3                         Âº

Âºquery OS/Runtime update status for instancesÂº
  go to kudu console â†’
  ------------------------------------------------------------------------
  windows version     https://${appname}.scm.Azurewebsites.NET/env.cshtml
                      (under system info)
  ------------------------------------------------------------------------
  .NET version        https://${appname}.scm.Azurewebsites.NET/debugconsole
                      $ powershell -command \
                      "gci 'registry::hkey_local_machine\software\microsoft\net framework setup\ndp\cdf'"
  ------------------------------------------------------------------------
  .NET core version   https://${appname}.scm.Azurewebsites.NET/debugconsole
                      $ dotnet --version
  ------------------------------------------------------------------------
  php version         https://${appname}.scm.Azurewebsites.NET/debugconsole,
                      $ php --version
  ------------------------------------------------------------------------
  default node.JS ver cloud shell â†’
                      $ az webapp config appsettings list \
                        --resource-group ${groupname} \
                        --name ${appname} \
                        --query "[?name=='website_node_default_version']"
  ------------------------------------------------------------------------
  python version      https://${appname}.scm.Azurewebsites.NET/debugconsole
                      $ python --version
  ------------------------------------------------------------------------

    note: access to registry location
hkey_local_machine\software\microsoft\windows\currentversion\component based
servicing\packages, where information on â€œkbâ€ patches is stored, is locked
down.
<hr/>
<span xsmall>in/out IPs</span>
APP-SERVICE
ENVIRONMENT
-----------
    isolated: static, dedicated in/out-bound IPs provisioned.
non-isolated: non-app-service-environments apps share network with other apps.
              in/out-bound ip addresses can be different and even change
              in certain situations.
              -ÂºinboundÂºIP address may change when:
                - app deleted/recreated in different resource group.
                - last app deleted/recreated in a resource group+region.
                - existing ssl binding deleted (ex: certificate renewal)
                TIP: Q: How to obtain static inbound IP:
                     A: configure an ip-based ssl binding (self-signed is OK)
                        withBÂºcertificate bound to an IP addressÂº
                        forcing an static IP in App service provisioning.
              -Âºoutbound ips rangeÂº:
                - each app has a set number of -unknown beforehand- Outbound ip
                  addresses at any given time. To show the full set:
                  $ az webapp show \
                    --resource-group $group_name \
                    --name $app_name \
                    --queryÂº"outboundipaddresses"Âº\ â† or "possibleoutboundipaddresses"
                    --output tsv                      to show all possible ips
                                                      regardless pricing tiers
<hr/>
<span xsmall>Hybrid connections</span>
REF: <a href="https://docs.microsoft.com/en-us/Azure/service-bus-relay/relay-hybrid-connections-protocol">relay-hybrid-connections-protocol</a>
 - hybrid Con. provides accessÂºfrom your app to an application tcp:port endpointÂº:

                     â”Œ service â”€â”€â”
                     â”‚ bus relay â”‚              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œweb app â”         â”‚ --\ /--   â”‚   remote     â”‚- remote â”‚
  â”‚        â†Â·connect.â†’ ---x---   â†Â·Â·Â·connectÂ·Â·Â·Â·â†’  serviceâ”‚
  â”‚        â”‚         â”‚ --/ \--   â”‚              â”‚- HCM    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€^â”€â”€â”€â”€â”€â”€â”˜
  app serv. host     create a tls 1.2           H)ybrid C)onnection M)anager
                     tunnel from/to             installed in remote
                     tcp:port to/from           (on-premise) service
                     remote srv
<hr/>
<span xsmall>A.traffic manage</span>
 Âºtraffic-manager.profile:Âº    â†Âºtraffic-managerÂºkeeps track of  app service apps status
   1+ app-service endpoints      (running, stopped, or deleted) to route client requests
       ^                         (status regularly communicated to profile)
  - apps in standard|premium mode
  - only 1 app-service endpoint allowed per region per profile.
    (remaining apps become unavailable for selection in profile).

 routing methods
 -Âºpriority   :Âº- use primary app for traffic,
                - provide backups in case of primary failure.
 -Âºweighted   :Âº- distribute (evenly|weighted) traffic across a set of apps
 -Âºperformance:Âº- for apps in different geographic locations,
                  use the "closest" (lowest net latency)
 -Âºgeographic :Âº- direct users to apps based on geographic location their
                  DNS query originates from.

 step 1) create profile in Azure traffic-manager
 step 2) add (app service|...) endpoints

 â˜To keep in mind:
  - failover and round-robin functionality only within same region
    (for premium mode is multi-region?)
  - Within a region, (App-service) app deployments using an
    app service can combine with anytoher A.cloud service
    endpoints in hybrid scenarios.
  - (App service) endpoints set in traffic-manager-profile
    is visible (not editable) in app-configure page â†’ profile â†’ domain-names
  - after adding an app to a profile, the site URL on the dashboard
    of the app's portal page displays the custom domain URL of the app
    if set up, otherwise, traffic-manager.profile URL (ex,
    contoso.trafficmanager.NET) is shown.
    - both app direct-domain-name and traffic-manager URL are visible
      on the app's configure page under the domain names section.
  - DNS map must also be configured to point to traffic-manager URL.
    (custom domain names continue to work)
</pre>
<pre zoom labels="azure,dev_framework,webapps">
<span xsmall>local_cache</span>
 - web role : view of content.
 - write-but-discard OÂºcacheÂº of storage content
 - created asynchronously on-site startup.
 - when OÂºcacheÂº is ready, site is switched to
   run against it.

 - benefits:
   - immune to storage-latencies.
   - immune to storage (un|)planned downtimes.
   - fewer app restarts due to storage share changes.

- d:\home  â†’ local VM OÂºcacheÂº,      â†Â·Â·Â·Â· limits:
             one-time copy of:             - def 300 MB
             d:\home\site\                 - max   2 GB
             d:\home\siteextensions\

  d:\local â†’ temporary VM-specific storage.

â”Œâ†’ d:\home\logfiles  local VM app log files
â”œâ†’ d:\home\data      local VM app data.
â”‚  ^^^^^^^^^^^^^^^^
â”‚  - copied in BÂºbest-effort to shared content storeÂº periodically.
â”‚    RÂº(sudden crash can loose some data)Âº
â”‚  - up to a one-minute delay in the streamed logs.
â”‚  - app restart needed to update OÂºcache-contentÂº if BÂºshared storageÂº
â”‚    is changed somewhere else.
â”‚
â””â†’ shared content store layout changes:
   - logfiles renamed â†’ logfiles+"uid"+time_stamp.
   - data     renamed â†’ data    +"uid"+time_stamp.
                                ^^^^^^^^^^^^^^^^^^
                                math a VM where app is running.
</pre>
<pre zoom labels="azure,dev_framework,webapps">
<span xsmall>ASE</span>
- Overview:
  - ASE: App Service Enviroment
  - provides BÂºfully isolated/dedicated environment at at high scaleÂº.
  - windows web apps
    linux   web apps  (preview)
    mobile      apps
    docker containers (preview)
    functions
  - ases can be created  multi-region.
   (ideal for scaling stateless apps)
  - secure connections over VPNs to on-premises can be setup.
  RÂºWARN: own pricing tierÂº
      flat monthly rate for an ASE that pays for infrastructure
      (it doesn't change with the size of the ASE)
    + cost per App-service-plan vcpu.
      (all apps hosted are in isolated-pricing SKU).

  ÂºASES v2Âº:
  - provide a surrounding to safeguard your apps in
    a subnet of your network and provides your own private
    deployment of  app service.

- subscription 1 â†â†’ 1 ase 1 â†â†’ 0...100 App service plan-instances.
                               ^^^^^^^
                     100 instances in   1 app service plan up to:
                       1 instances in 100 app service plan(single-instance)

                     â˜Remember:
                     - An app runs in an app service plan.
                     - a newÂºapp service planÂºin a region automatically
                       creates new associated compute resources where all
                         apps in plan will share resources.

-BÂºASE = front-ends         +  workersÂº  â†Â·Â·Â·Â·Â·Â·Â· no management needed
        ^^^^^^^^^^             ^^^^^^^
   http/https termination      host customer apps in sizes:
   and load balancing          - 1 vcpu/ 3.5 GB RAM
   automatically added as      - 2 vcpu/ 7.0 GB RAM
   app service plans           - 4 vcpu/14.0 GB RAM
   scale out.
</pre>
<pre zoom labels="azure,webapps,dev_framework,comparative">
<span xsmall>Webjobs</span>
- Overview:
  - webjob = "background task"
  - program or script BÂºin the same context as a web/API/mobile app.Âº
    ^^^^^^^^^^^^^^^^^
    .cmd/bat/exe (using windows cmd)
    .ps1 (using powershell)    .py  (using python)
    .sh  (using bash)          .JS  (using node.JS)
    .php (using php)           .jar (using java)

 - â˜ alternative: Azure functions. ("Evolution" of webjobs)

 -Âºwebjob typesÂº
        CONTINUOUS                                TRIGGERED
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚starts immediately when the webjob is  â”‚ starts only when triggered  â”‚
â”‚created. to keep the job from ending,  â”‚ manually or on a schedule.  â”‚
â”‚the program or script typically does   â”‚                             â”‚
â”‚its work inside an endless loop. if theâ”‚                             â”‚
â”‚job does end, you can restart it.      â”‚                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚runs on all instances that the web app â”‚ runs on a single instance   â”‚
â”‚runs on. you can optionally restrict   â”‚ that Azure selects for      â”‚
â”‚the webjob to a single instance.       â”‚  load balancing.            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚supports remote debugging.             â”‚ doesn't support remote      â”‚
â”‚                                       â”‚ debugging.                  â”‚
â””â”€^â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  enable "always on" to avoid timeouts (not available in free tiers)

â˜ note:
 webjobs canÂºtime out after 20 minutes of inactivity.Âº
 reset/reactive timeout withÂºgit deploymentÂºor to the web app's pages in
 theÂºportal reset the timerÂº
 requests to the actual site do not reset the timer.

Âºcreating a continuous/manual webjobÂº
Azure portal
 â†’ go to app service page â†’ existing app service (web/API/mobile app)
  â†’ select "webjobs"
    â†’ select "add",
      Fill in requested settings and confirm:

       SETTING       SAMPLE VALUE
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
      â”‚name        â”‚myjob01        â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚file upload â”‚consoleapp.zip â† .zip file with executable/script plus supporting files
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚type        â”‚continuous     â† continuous â”‚ triggered
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚scale       â”‚multi instance â† set multi(==all) or single instance.
      â”‚(continuous â”‚               â”‚ only single in free/share tier
      â”‚ type only) â”‚               â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
      â”‚triggers    â”‚manual         â”‚
      â”‚(triggered  â”‚               â”‚
      â”‚ type only) â”‚               â”‚
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       new webjob will appear on the webjobs page.

<hr/>
<span TODO xsmall>LogicApps vs WebJobs</span>
@[https://docs.microsoft.com/en-us/azure/azure-functions/functions-compare-logic-apps-ms-flow-webjobs]
</pre>

</pre>

<pre zoom labels="azure,webapps,TODO">
<span xsmall>Deploy ASP.net|WAR to App Serv.</span>
@[https://docs.microsoft.com/en-us/azure/app-service/deploy-zip]
</pre>
</div>

<div groupv>
<span title>Mobile Backend Support</span>
<pre zoom labels="azure,dev_framework,mobile,">
<span xsmall>backend howto</span>
Backend will provide support for:
 - A.AD, OAuth 2.0 social providers, custom AA providers (with SDK)
 - Connection to ERPs.
 - offline-ready Apps with periodic Data Sync
 - Push notifications,
 - ...
 ÂºSTEP 1)Âºcreate new Azure mobile app backend
  â†’ portal â†’ "create a resource" â†’ search for "mobile apps" â†’ "mobile apps quickstart"
    â†’ Click on "create" and fill requested data:
      - unique app name: to beÂºpart of the domain nameÂºfor new app service
      - resource group : existing|new
      â†’ press "create" and wait a few minutes for the service
        to be deployed
        â†’ watch notifications (bell) icon for status updates.

 ÂºSTEP 2.1)Âºconfigure the server project and connect to a database
â†’ portal â†’ A.Services â†’ "app services" â†’ select a mobile apps back end â†’ "quickstart"
  â†’ select client platform (ios, android, xamarin, cordova, windows (c#)).
    â†’ if database connection is not configured, create new one like:
      a. create a new SQL database and server.
      b. wait until the data connection is successfully created.
      â†’ under 2. "create a table API", select node.JS for backend language.
        â†’ accept the acknowledgment
          â†’ select "create todoitem table"

 ÂºSTEP 2.2)Âºdownload and run the client project once the backend is configured:
 - opt1: modify existing app to connect to Azure.
 - opt2: create a new client app. ex:
   - go back to "quick start" blade for your mobile app backend.
     â†’ click create a new app â†’ download uwp app template project,
       already customized to connect with mobile app backend.
     â†’ (optional) add uwp app project to the same solution as the
       server project.
       this makes it easier to debug and test both the app and
       the backend in the same visual studio solution.
       (visual studio 2015+ required)
       â†’ visual studio: press "f5" to deploy and run the app.

 ÂºSTEP 3)Âº deploy and run the app
<hr/>
</pre>
<pre zoom labels="azure,mobile,push,dev_framework,">
<span xsmall>push notifications</span>
- Overview:
  - Service delivered through non-standardÂºplatform notification systemsÂº(BÂºPNSÂº).
    offeringÂºbarebone message push functionalitiesÂº to devices:
    - apple push notification service(apns)
    - firebase cloud messaging        (fcm)
    - windows notification service    (wns)

Âºsequence diagram (summary):Âº
                                      â”Œâ”€â”€â”€â”4)store  Âºpre-setup)Âº
â”Œâ”€â”€â”€â”€â”€â”€â” â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  handle  1 client_app â†’ BÂºpnsÂº: request unique-push-handle
â”‚mobileâ”‚  3)store pns handle   â”‚app     â”‚â†â”˜                                        (and temporary)
â”‚app   â”‚                       â”‚backâ”€endâ”‚            2 client_app â† BÂºpnsÂº: unique-push-handle
â””â”€â”€â”€â”€â”€â”€â”˜ â†â”€â”€â”€â”€â”€â”               â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                   ^^^^^^^^^^^^^^^^^^
  â”‚ ^          â”‚ 6)send to          â”‚                                       uris   in wns
  â”‚ â”‚2)handle  â”‚ device             â”‚                                       tokens in apns
  â”‚ â”‚          â”‚                    â”‚ 5) message                            ...
  â”‚ â”‚          v                    â”‚    handle      3 client_app â†’ backend: unique-push-handle
  â”‚ â””â”€â”€â”€â”€â”€â”€â”€ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚                4 backend    â†’ backend: store handle (in DDBB,provider,...)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚(p)lataform   â”‚ â†â”€â”€â”€â”€â”€â”˜
  1)request  â”‚(n)otificationâ”‚                       Âºsending push messages)Âº
  pns handle â”‚(s)ervice     â”‚                        5 backend â†’ OÂºpnsÂº    :  (message, unique-push-handle)
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        6 OÂºpnsÂº  â†’ client_app: message (ussing ip net or
                                                                             phone network)
Âºhow-to summaryÂº
 - PRE-SETUP)
   -Âºpush notification extension packageÂºrequired
     (included in "quick-start server project" template)
     more info at:
     "work with the .NET backend server SDK for a.mob.apps"
     @[https://docs.microsoft.com/en-us/Azure/app-service-mobile/app-service-mobile-dotnet-backend-how-to-use-server-SDK]
    --- common steps -----------------------------------
    developer â†’      Azure: config. notification hub
                            Azure portal â†’ "app services" â†’ select existing app back end.
                              â†’ under settings, select "push"
                                â†’ select "connect" to add a Âºnotification hub resourceÂº to the app
                                  (add new or reuse existing hub)
                                  later this notification hub is used to connect to a
                                  OÂºpnsÂº to push to devices.

    developer â†’ m.appstore: register app for push notifications
                            visual studio (2015+) â†’ solution explorer
                            â†’ right-click on the uwp app project
                             â†’ click store
                              â†’Âºassociate app with the store...Âº
                                in the wizard â†’ click next â†’ sign in (with  microsoft account)
                                reserve a new app name : "enter app name"
                                â†’ click "reserve"
                                app registration will follow. after that
                                â†’ select the new app name â†’ click "next" â†’ click "associate".
                               Âºthis will adds the required microsoft store registrationÂº
                               Âºinformation to the application manifest.Âº
                               ------------------------------------------
                               navigate and sing-in to Âºwindows dev centerÂº
                               â†’ go to "my apps" â†’ click "new app registration"
                                 â†’ expand "services Ëƒ push notifications":
                                   click "live services" site under
                                   microsoft Azure mobile services@"push notifications" page,
                                 â†’ write down the values under application secrets and the package sid âœ
                                   in the registration page, (used next to configure mobile app backend).â”€â”
                                 â¿ (keep them safe)                                                       â”‚
                                   application id + secret is used to configure ms account auth           â”‚
                                                                                                          â”‚
    developer â†’  a.portal: configure the back end to send push notifications                              â”‚
                           Azure portal â†’ browse all â†’ app services.                                      â”‚
                            â†’ select a mobile apps back end                                               â”‚
                             â†’ under settings, select "app service push"                                  â”‚
                              â†’ select your notification hub name.                                        â”‚
                               â†’ go to windows (wns): enter the                                      â†â”€â”€â”€â”€â”˜
                                 security key (client secret) and package sid
                                 obtained from the live services site.
                                â†’ click "save"
                                back end is now configured to use wns to send push notifications.

    developer â†’ server app:  update the server to send push notifications
                          Âºalt 1) .NET backend projectÂº
                          visual studio â†’ right-click the server project
                           â†’ click "manage nuget packages"
                            â†’ search for "microsoft.Azure.notificationhubs" (client lib)
                             â†’ click "install"
                              â†’ expand controllers â†’ open todoitemcontroller.cs:
                               â†’ add the following using statements:
                                 | using system.collections.generic;
                                 | using microsoft.Azure.notificationhubs;
                                 | using microsoft.Azure.mobile.server.config;
                               â†’ in the posttodoitem method, add the following code after
                                 the call to insertasync:
                                 | // get the settings for the server project.
                                 | httpconfiguration config = this.configuration;
                                 | mobileappsettingsdictionary settings =
                                 |    this.configuration.
                                 |      getmobileappsettingsprovider().
                                 |        getmobileappsettings();
                                 |
                                 | // get the notification hubs credentials for the mobile app.
                                 | string notificationhubname = settings.notificationhubname;
                                 | string notificationhubconnection = settings.
                                 |    connections[mobileappsettingskeys].
                                 |     notificationhubconnectionstring].
                                 |      connectionstring;
                                 | // create the notification hub client.
                                 |
                                 | notificationhubclient hub = notificationhubclient
                                 |   .createclientfromconnectionstring(
                                 |      notificationhubconnection,
                                 |      notificationhubname);
                                 |
                                 | var windowstoastpayload = // define a wns payload
                                 |   @"Ë‚toastËƒË‚visualËƒË‚binding template=""toasttext01""ËƒË‚text id=""1""Ëƒ"
                                 |   + item.text + @"Ë‚/textËƒË‚/bindingËƒË‚/visualËƒË‚/toastËƒ";
                                 | try {
                                 |   // send the push notification.
                                 |   var result = await
                                 |      hub.sendwindowsnativenotificationasync(windowstoastpayload);
                                 |
                                 |   // write the success result to the logs.
                                 |   config.services.gettracewriter().info(result.state.tostring());
                                 | } catch (system.exception ex) {
                                 |   // write the failure result to the logs.
                                 |   config.services.gettracewriter()
                                 |     .error(ex.message, null, "push.sendasync error");
                                 | }

                                 this code tells the notification hub to send a push notification after a
                                 new item is insertion.

                               â†’ republish the server project.

                          Âºalt 2. node.JS backend projectÂº
                           ( based on the quickstart project )
                           â†’ replace the existing code in the todoitem.JS file with:
                             (when editing the file on your local computer (vs online editor), republish the server project)

                             | var Azuremobileapps = require('Azure-mobile-apps'),
                             | promises = require('Azure-mobile-apps/src/utilities/promises'),
                             | logger = require('Azure-mobile-apps/src/logger');

                             | var table = Azuremobileapps.table();

                             | table.insert(function (context) {
                             | // for more information about the notification hubs javascript SDK,
                             | // see http://aka.ms/nodejshubs
                             | logger.info('running todoitem.insert');

                             | // define the wns payload that contains the new item text.
                             | var payload = "Ë‚toastËƒË‚visualËƒË‚binding template=\toasttext01\ËƒË‚text id=\"1\"Ëƒ"
                             |               + context.item.text + "Ë‚/textËƒË‚/bindingËƒË‚/visualËƒË‚/toastËƒ";
                             |
                             | // execute the insert.  the insert returns the results as a promise,
                             | // do the push as a post-execute action within the promise flow.
                             | return context.execute()
                             |   .then(function (results) {
                             |     // only do the push if configured
                             |     if (context.push) {
                             |       // send a wns native toast notification.
                             |       context.push.wns.sendtoast(null, payload, function (error) {
                             |         if (error) {
                             |           logger.error('error while sending push notification: ', error);
                             |         } else {
                             |           logger.info('push notification sent successfully!');
                             |         }
                             |       });
                             |     }
                             |     // don't forget to return the results from the context.execute()
                             |     return results;
                             |   })
                             |   .catch(function (error) {
                             |       logger.error('error while running context.execute: ', error);
                             |   });
                             | });
                             |
                             | module.exports = table;

                             this sends a wns toast notification that contains the item.text when a new
                             todo item is inserted.


    developer â†’ client app: add push notifications to your app
    ----------------------------------------------------
</pre>
<pre zoom labels="azure,mobile,offline">
<span xsmall>offline apps</span>
- Overview:
  - push: send all tables, avoiding out-of-order execution
  - pull: performed on per-table customizable queries
          pull against locally modified table will triggers
          a push first to minimize conflicts
â”Œâ”€â”€â”€â”€â”€â”€â”€â”                          â”Œâ”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”
â”‚mobile â”‚    â†’ÂºpushÂºCUD changes   â†’â”‚RESTâ”‚ â†’ â”‚remote â”‚
â”‚local  â”‚      since last push     â”‚API â”‚   â”‚backendâ”‚
â”‚DDBB   â”‚                          â”‚    â”‚   â”‚DDBB   â”‚
â”‚storageâ”‚    â† ÂºpullÂº(query_name,  â””â”€â”€â”€â”€â”˜ â† â””â”€â”€â”€â”€â”€â”€â”€â”˜
â””â”€â”€â”€â”€â”€â”€â”€â”˜             query)
  ^       â”‚                       â”‚  non-null query name will force
  |       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  incremental sync. *1
  |         OÂºsync contextÂº          'updatedate' from latest pull
  |            operation queue       is stored in the SDK local
  |            ordered cud list      system tables. further pulls
  |                                  retrieve from 'updatedate'
  |                                  client SDK uses sort itself
  |                                  ignoring 'orderby' from server
  |                                  query_name must be unique per app.
  |
  |
  |         - associated with aÂºmobile client objectÂº
clear stale data:               â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
IMobileServiceSyncTable.        â†‘
 ÂºpurgeAsyncÂº       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
exception thrown if â”œ imobileserviceclient.  (.NET client SDK)
ops awaiting sync   â”‚ // init sync. context:
                    â”‚ imobileservicessynccontext.
                    â”‚   initializeasync(localstore)
                    â”œ msclient
                    â”œ ...
            - changes made withÂºsync tablesÂº
              tracked in sync context:
              - client controls when to sync
                (call to push local cud)

*1 if query has 1 parameter, one way to create a
   unique query name is to incorporate the parameter value.
   ex: filtering on userid:
   | await todotable.pullasync(
   |     "todoitems" + ÂºuseridÂº,
   |     synctable.where(u =Ëƒ u.userid ==ÂºuseridÂº));

-ÂºUpdate client app for offline supportÂº

- init syncContext to local store.
- reference table/s through the IMobileServiceSyncTable interface.

STEP 1) install sqlite runtime for the universal windows platform:
  â†’ visual studio â†’ nuget package manager for uwp app project
   â†’ (search and) install 'microsoft.Azure.mobile.client.sqlitestore' nuget package.
    â†’ in solution explorer, right-click references â†’ add reference... â†’
      universal windows â†’ extensions:
      â†’ enable 'sqlite for universal windows platform' and
               'visual c++ 2015 runtime for universal windows platform apps'
        â†’ open mainpage.xaml.cs and uncomment line:
          #define offline_sync_enabled
         â†’ press f5 to rebuild-and-run client app.

STEP 2) update the app to disconnect from the backend
  offline mode:
  when adding items in offline mode, exception handler
  serves to handle offline pipeline, adding
  new items added in local store.

  edit app.xaml.cs:
  comment out initialization to add invalid
  mobile app URL (simulate offline):
  |   public static mobileserviceclient
  |      mobileservice = new
  |       mobileserviceclient("https://foo");

  new item "save" push fails triggers "cancelledbynetworkerror" status, but new items exist in local store.
  if suppressing these exceptions client behaves as still connected to the mobile app backend.

STEP 3) update the app to reconnect to the backend
        at startup, 'onnavigatedto' event handler calls
  'initlocalstoreasync'  that in turn calls
  'syncasync'           to sync local store with backend

  restore correct URL of mobileserviceclient and rerun (f5)

  'updatecheckedtodoitem' calls syncasync to sync each
  completed item with the mobile app backend.
  'syncasync' calls both push and pull.

- when offline, normal CRUD operations work as if connected.
  Following methods are used to synch local store with server:
  - imobileservicessynccontext.ÂºpushasyncÂº:
  - imobileservicessynccontext.ÂºpullasyncÂº:
  started from a BÂºIMobileServiceSyncTableÂº.
  - pushothertables parameter controls whether other tables in the
    context are pushed in an implicit push.
  - query parameter takes an imobileservicetablequery or odata query
    string to filter the returned data.  the queryid parameter is used to
    define incremental sync.

-Âºpurgeasync:Âºapp should periodically call it to purge stale data.
  use 'force' param to purge any changes not yet synced.
</pre>
<span title>create A.App-service API apps</span>
<pre zoom labels="azure,dev_framework,api_management,">
<span xsmall>APIs_howto</span>
ÂºAPI management overviewÂº

  once ready product
  is published           created by admins
  â†“                      â†“
  product    â† relN2M â†’  API  1 â†â†’ n operations
  ^            --------              ^
  open or      title                 |
  protected    description           |
  |            terms-of-use          |
  |                                  |
  |                                  |
  |                                  |
  developers                         API contains a reference to the
  1) subscribe to product            back-end service that
     (for protected products)        implements the API, and its
  2) call the API operation/s        operations map to the
                                     operations implemented by the
                                     back-end service.

ÂºAPI gatewayÂº :                          ÂºA. portalÂº (API admins)
 - end-point for API calls                - define or import API schema.
   â†’ route to backend.                    - package APIs into products.
 - verifies API keys/JWT/certs/...        - set up policies like quotas
 - enforces usage quotas and rate limits.   or transformations on the APIs.
 - transforms API on the fly (no code,    - get insights from analytics.
   API or operation level)                - manage users.
 - caches backend responses
 - logs call metadata

Âºdeveloper portalÂº (API consumer developers)
 - read API documentation.
 - try out an API via the interactive console.
 - create an account and subscribe to get API keys.
 - access analytics on their own usage.
 - view/call operations
 - subscribe to products.
 - developer portal URL  indicated in:
   Azure portal â†’ API management service instance â†’ dashboard

product   â†’ grant visibility to â”€â”
                                 â†“
                                 group/s
                                 â†‘ â”‚
developer â†’ belongs to  n â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                                   â”‚
  â”€ immutable system groups: â†â”€â”€â”€â”€â”€â”˜
    ÂºadministratorsÂº: subscription admins.
                      - manage API-management service instances:
                        create APIs/operations/products
    ÂºdevelopersÂº    : authenticated developer portal users.
                      - access to developer portal
    ÂºguestsÂº        : unauthenticated developer portal users
                      - can be granted certain access (read only)

  â”€ custom groups: (or existing AD tenants groups)
    use case example: custom group for developers affiliated with
    a specific 3rd organization allow access to APIs from a
    product containing relevant APIs only.

ÂºdevelopersÂº
- created or invited to join by admins.
- sign up from the developer portal.
- when developers subscribe to a product they are granted the
 Âºprimary and secondary keyÂºfor the product used to make calls
  into its APIs.

ÂºpoliciesÂº
- allow A.portal to change the behavior of the API
  through configuration.
- collection of statements executed sequentially on
  the request or response of an API.
- popular statements include:
  - format conversion (xml to JSON)
  - call rate limiting , number of incoming calls from a developer.
  - :..
- policy expressions can be used as attribute values or
  text values

ÂºAPI management terminologyÂº
-Âºbackend APIÂº- an http service that implements your API and its
  operations.
-Âºfrontend API/APIM APIÂº-
BÂºan APIM API does not host APIs, it creates facades for your APIsÂº
  in order to customize the facade without touching back end API.
-ÂºAPIM API operation : each APIM API contains a reference to the
  back end service implementing the API, and its operations map
  to the operations implemented by the back end service.

Âºcreate new A. APIM service instanceÂº
  portal â†’ create a resource â†’ integration â†’ API management:
     fill form:
     - name   â† used also as default domain name
                {name}.Azure-API.NET.
     - subscription 	
     - resource group
     - location
     - organization name
     - administrator email
     - pricing tier        (developer tier to evaluate the service)
     â†’ click on  create.
       (wait up to 15 minutes)

ÂºCreate new APIÂº
Azure portal â†’ select existing APIM instance
 â†’ select APIs (under API management)
  â†’ select "+ add API" (left menu)
    â†’ select "blank API" from list.
     â†’ enter settings for the API.
       name         value
       display name "blank API"

       web service  "http://a.org" â† leave empty for mock up
       URL(option)
       (optional)
       URL scheme   "https"

       URL suffix   "hbin"         it identifies this specific
                                   API in this apim instance.

       products     "unlimited"    if you want for the API to be
                                   published and be available to
                                   developers, add it to a
                                   product.  you can do it
                                   during API creation or set it
                                   later.
       note: by default, each API management instance comes with
       two sample products: starter and unlimited.

      â†’ select "create"
        at this point,RÂºyou have no operations in apim that map toÂº
      RÂºthe operations in your back-end API.Âºif you call an
        operation that is exposed through the back end but not
        through the apim, you get a 404.

  â˜ note: by default, when you add an API, even if it is connected to
    some back-end service, apim will not expose any operations
    Âºuntil you whitelist them.Âº
     whitelist the operation of back-end service by creating a
     apim operation that maps to the back-end operation.

     â†’Âºadd and test a parameterized operationÂº
      â†’ select the API just created
       â†’ click "+ add operation".
        â†’ in the URL, select get and enter /status/{code} in the resource.
          optionally, provide info associated to {code} param.
          (number for type, def. value ,...)
         â†’ enter fetchdata for display name.
          â†’ select "save"

     â†’Âºtest an operationÂº
       Azure portal (alternatively use developer portal)
       â†’ select the "test tab"
        â†’ select "fetchdata"
         â†’ press "send"
         response to http://a.org/status/200 operation is displayed

Âºcreate and publish a productÂº
â†’ select "products" (menu left)
 â†’ select "+ add" and fill:
   display name
   name
   description
   state                  â† press "published" to publish it
   requires subscription  â† check it if user is required to
                            subscribe before using it
   requires approval      â† check it to make admin review it
                            and accept/reject subscription
                            (auto-approved otherwise).
   subscript.count limit  â† limit simultaneous subscriptions
   legal terms
   APIs                   â† API list to include in produc
  â†’ select "create"

 â˜ tip: you can create or update user's subscription to
        a product with custom subscription keys through
        REST API or powershell command.
</pre>
<pre zoom labels="azure,api_management,dev_framework,web,code_snippet,swagger,">
<span xsmall>Swagger API docs</span>
ASP.NET C# how-to with Swashbuckle
-ÂºOverviewÂº
 (asp.NET core) main components:
 -Âºswashbuckle.aspnetcore.swaggerÂº:
   swagger object model and middleware to expose swaggerdocument
   objects as JSON endpoints.

 -Âºswashbuckle.aspnetcore.swaggergenÂº:
   builds swaggerdocument objects directly from
   routes, controllers, and models.
   typically combined with the swagger endpoint middleware to
   automatically expose swagger JSON.

 -Âºswashbuckle.aspnetcore.swaggeruiÂº:
   swagger ui tool embedded version.
   - it includes built-in test harnesses for
     the public methods.

-Âºpackage installationÂº
  visual studio â†’ â†’ view â†’ other windows
  â†’ alt.1: package manager console
    â†’ nav to the dir containing todoapi.csproj:
      â†’ execute:
        $ install-package BÂºswashbuckle.aspnetcoreÂº

  â†’ alt.2: from "manage nuget packages" dialog
    right-click the project in solution explorer
    â†’ manage nuget packages
     â†’ set the package source to â€œnuget.orgâ€
         enter BÂºswashbuckle.aspnetcoreÂº in search box
         selectbÂºswashbuckle.aspnetcoreÂº package from
         browse tab and click install

- Âºadd and configure swagger middlewareÂº

  (startup configureservices method)
  public void configureservices(iservicecollection services) {
    services.adddbcontextË‚todocontextËƒ(opt =Ëƒ
      opt.useinmemorydatabase("todolist"));
    services.addmvc().setcompatibilityversion(
      compatibilityversion.version_2_1);

    servicesÂº.addswaggergenÂº(c =Ëƒ        // â† STEP 1) register swagger generator
    {                                    //           by defining 1+ swagger docs.
      c.swaggerdoc("v1",
      new info {
        title = "my API",
        version = "v1"
      });
    });
  }

  using swashbuckle.aspnetcore.swagger;   // â† STEP 2) import in info class:

  (startup.configure method)
  public void configure
  (iapplicationbuilder app) {
      appÂº.useswagger()Âº;                 //â† STEP 3) enable middleware to serve
                                          //          generated swagger as JSON endpoint.

      appÂº.useswaggeruiÂº(c =Ëƒ {           // â† STEP 3) enable swagger-ui
        c.swaggerendpoint(
           "/swagger/v1/swagger.JSON",
           "my API v1");
        // c.routeprefix = string.empty; â† Un-comment to serve ui at /
      });
      app.usemvc();
  }

  STEP 4) test setup: launch the app, and navigate to
     http://localhost:"port"/swagger/v1/swagger.JSON
     http://localhost:"port"/swagger  â† ui

- Âºdocumenting the object model - API info and descriptionÂº
 the configuration action passed to the addswaggergen method adds
 information such as the author, license, and description:

 // register the swagger generator, defining 1+ swagger docs.
 services.addswaggergen(c =Ëƒ
 {
     c.swaggerdoc("v1", new info
     {
         version = "v1",
         title = "todo API",
         description = "a simple example asp.NET core web API",
         termsofservice = "none",
         contact = new contact
         {
             name = "shayne boyer",
             email = string.empty,
             URL = "https://twitter.com/spboyer"
         },
         license = new license
         {
             name = "use under licx",
             URL = "https://example.com/license"
         }
     });
 });

- Âºenabling XML commentsÂº
STEP 1) configure project to enable xml output doc.
   visual studio â†’ solution explorer â†’ right-click project
   â†’ select edit â†’ "project_name".csproj and add next lines :

   Ë‚propertygroupËƒ
       Ë‚generatedocumentationfileËƒtrueË‚/generatedocumentationfileËƒ
       Ë‚nowarnËƒ$(nowarn);1591Ë‚/nowarnËƒ â† semicolon-delimited list
   Ë‚/propertygroupËƒ                      enclose the code with #pragma
                                         to suppress specific code like
                                         #pragma warning disable cs1591
                                         public class program { .... }

   enabling xml comments provides debug information for undocumented
   public types and members. undocumented types and members are
   indicated by the warning message.

STEP 2) configure swagger to use the generated xml file.

  public void configureservices(iservicecollection services) {
    services.adddbcontextË‚todocontextËƒ(opt =Ëƒ
        opt.useinmemorydatabase("todolist"));
    services.addmvc()
        .setcompatibilityversion(compatibilityversion.version_2_1);
    services.addswaggergen(c =Ëƒ {             // â†Âºregister swagger generator,Âº
      c.swaggerdoc("v1", new info {           //  Âºdefining 1+ swagger  docs  Âº
          version = "v1",
          title = "todo API",
          description = "...",
          termsofservice = "none",
          contact = new contact {
              name = "shayne boyer",
              email = string.empty,
              URL = "https://..."
          },
          license = new license {
            name = "...", URL = "https:..." }
      });

      var xmlfile = // set comments path for swagger JSON/ui.
        $"{assembly.getexecutingassembly().getname().name}.xml";
      var xmlpath = path.combine(appcontext.basedirectory, xmlfile);
      c.includexmlcomments(xmlpath);
    });
  }

BÂºreflection is used to build an xml file name matching that of theÂº
BÂºweb API project.Âº the appcontext.basedirectory property is used to
  construct a path to the xml file.

QÂº/// Ë‚summaryËƒ                    Âº      â†Â·Â·Â·Â·Â·Â·Â· triple-slash enhances swagger ui
QÂº/// deletes a specific todoitem. Âº               by adding description to section header.
QÂº/// Ë‚/summaryËƒ                   Âº               add
QÂº/// Ë‚param name="id"ËƒË‚/paramËƒ    Âº               swagger ui will translate QÂºË‚summaryËƒÂº to:
  [httpdelete("{id}")]                             the ui is driven by the generated JSON schema:
  public iactionresult delete(long id) {           "delete": {
    var todo = _context.todoitems.find(id);          "tags": [ "todo" ],
    if (todo == null) { return notfound(); }       QÂº"summary": "deletes a specific todoitem.",Âº
    _context.todoitems.remove(todo);                 "operationid": "apitodobyiddelete",
    _context.savechanges();                          ...
    return nocontent();                            }
  }

  /// Ë‚summaryËƒ
  /// creates a todoitem.
  /// Ë‚/summaryËƒ
GÂº/// Ë‚remarksËƒ                   Âº                         â† Add action method documentation,
GÂº/// sample request:             Âº                           supplementing Ë‚summaryËƒ information
GÂº///                             Âº                           text, JSON, or xml allowed
GÂº///     post /todo              Âº
GÂº///     {                       Âº
GÂº///        "id": 1,             Âº
GÂº///        "name": "item1",     Âº
GÂº///        "iscomplete": true   Âº
GÂº///     }                       Âº
  /// Ë‚/remarksËƒ
  /// Ë‚param name="item"ËƒË‚/paramËƒ
  /// Ë‚returnsËƒa newly created todoitemË‚/returnsËƒ
  /// Ë‚response code="201"Ëƒreturns the newly created itemË‚/responseËƒ
  /// Ë‚response code="400"Ëƒif the item is nullË‚/responseËƒ
BÂº/// Ë‚returnsËƒa newly created todoitemË‚/returnsËƒ Âº          â† Âºdescribing response typesÂº
BÂº/// Ë‚response code="201"Ëƒreturns new itemË‚/responseËƒÂº
BÂº/// Ë‚response code="400"Ëƒif the item is nullË‚/responseËƒÂº
  [httppost]
  [producesresponsetype(201)]
  [producesresponsetype(400)]
  public actionresultË‚todoitemËƒ create(todoitem item) {
      _context.todoitems.add(item);
      _context.savechanges();
      return createdatroute("gettodo", new { id = item.id }, item);
  }

-Âºdecorating the model with attributesÂº

  using system.componentmodel;
  using system.componentmodel.dataannotations;  // â† defined attributes providing
                                                //   hints to swagger ui components.
  namespace todoapi.models {
      public class todoitem {
          public long id { get; set; }
          [required]                            // â† atribute
          public string name { get; set; }

          [defaultvalue(false)]                 // â† atribute
          public bool iscomplete { get; set; }
      }
  }
</pre>
</div>
<div groupv>
<span title>Azure Functions</span>
<pre zoom labels="azure,dev_framework,functions,serverless">
<span xsmall>functions</span>
-ÂºOverviewÂº:
  - use-case: run small pieces of code ("functions").
              c#, f#, node.JS, java, or php.
              "unix text utils on the cloud".

                                general
                                storage    â† support BLOB/queue/file/table
                                account      triggers and logging function
                                 â†‘ 1         executions depends on storage.
                                 |
                                 â†“ 1
 â”‚bindingâ”‚   n...0  â†Â·Â·Â·Â·Â·â†’    â”‚functionâ”‚   1 â†Â·Â·Â·Â·Â·â†’   1   â”‚triggerâ”‚
  â””â”€â”€â”¬â”€â”€â”˜                       â””â”€â”€â”€â”¬â”€â”€â”˜                     â””â”€â”€â”¬â”€â”€â”˜
optional declarative way            N                 start code-execution.
to connect to input/output          â”‡                 - Contains trigger.data:
data from within code.              1                            â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                               â”‚function Appâ”‚                  "Ussually", not
                                                               always == payload
                                                               input to funct.
   trigger.data â†’  â”‚ function â”‚ â†’ output data
                                  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
                                  Return value of function,
                                  and/or "out" parameters in
                                  C#/C# script.

 ÂºCONSUMPTION HOSTING-PLANÂº            Â· ÂºAPP SERVICE HOSTING PLANÂº:
(RÂºWARN:Âºâ˜ hosting plan can NOT be changed after Func.App creation)
  --------------------------------- Â·  ------------------------------------
  - hosts added/removed on demad    Â·  -BÂºneeded when function runs forÂº
    based on Âºinput events rateÂº    Â·   BÂºmore than 10 minutesÂº
  - pay for function exec. time.    Â·  - run functions as web apps.
  - execution RÂºstops afterÂº        Â·  - create/reuse App Service from apps
  RÂºconfigurable timeoutsÂº          Â·    at no additional cost.
    - 5 minutes def,                Â·  - can scale between tiers to allocate
    -RÂº10min max                    Â·    different amount of resources.
    - Tunned in functionTimeout@    Â·  -BÂºSupports linux.Âº
      "host.JSON" project file      Â·  -BÂº"always-on" must be enabledÂºsince
  - VMs running functions           Â·  RÂºfun.runtime goes idle after a fewÂº
  RÂºlimited 1.5 GB RAMÂº             Â·  RÂºminutes of inactivityÂº.
                                    Â·    Only http triggers will "wake up"
                                    Â·    the functions.
  - Fun.code is BÂºstored on Âº
  BÂºA.filesÂºshares associated
    to the Fun's main storage account.
                      ^^^^^^^^^^^^^^^
          RÂºwarn:Âºdeleting it deletes
                  the code.
  - all functions in a Func. app     â†BÂºScale controllerÂº monitor and
    share resources within an          applies BÂºtrigger-type heuristicsÂº
    instance and scale                 to determine when to scale
    simultaneously. Distinct           out/in. Ex: In a queue
    Func.Apps scale independently.     storage next tuple is ussed:
                                       (queue size, age-of-oldest-msg)
    - Func.App scales up to Âº200 max.instancesÂº.
    - A single instance may process
     "infinite/no-limit" messages
    -Âºnew instancesÂº will be
      allocatedÂºat most once every 10 secs.Âº

- Existing TriggerÂºTEMPLATESÂº:
  -ÂºhttpTriggerÂº    :
  -ÂºtimerTriggerÂº   :
  -Âºgithub webhookÂº :
  -ÂºCosmosdbTriggerÂº:
  -ÂºBLOBtriggerÂº    : on consumption plan, there can be up to a
                      10-minute delay in processing new BLOBs
                      when function app has gone idle. switch
                      consumption to app service plan +
                      "always on" enabled, or use the event
                      grid trigger.

  -ÂºqueuetriggerÂº (messages arriving to Âºa.storage queueÂº)
  -ÂºeventhubtriggerÂº (events delivered to aÂºa.event hubÂº)
  -ÂºservicebusqueuetriggerÂº
  -ÂºservicebustopictriggerÂº
  -Âºtwilio (sms messages) triggerÂº

BÂºTrigger/Bind ExampleÂº
  - Triggers and Bindings are configured either with:
    -BÂº"function.JSON" fileÂº                    (â† Azure Portal)
    -  code decorator attributes in code params (â† C#/C#Script)
       and functions.

  BÂºExample 1. Using Azure PortalÂº
    -BÂºInput message â†’ function â†’ write row to A.TableÂº
       ^^^^^^^^^^^^^              ^^^^^^^^^^^^^^^^^^^^
       queue-storage              table-storage
       trigger                    output binding
       â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            UseÂªÂºfile 'function.JSON'Âºto define them
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                To edit the file go to A.Portal
                  â†’ ...  â†’ function â†’ "advanced editor"
                                      @"integrate tab"

      â”” Ex: 'fuction.JSON':
        {
          "bindings": [
          {                          â†BÂºInputÂºbinding (== "trigger")
          Â·Âº"type": "queuetrigger",Âº
     â”Œâ†’   Â· "name": "order",         â† Fun.param receiving
     Â·    Â·                            trigger.data input
     Â·    Â· "direction": "in",       â† always 'in' for triggers
     Â·    Â· "queuename": "queue01"   â† queue "ID" to monitor
     Â·    Â· "connection": "storage01"â† pre-setup in app setting
     Â·    Â· "datatype": "string"     â† optional. One of:
     Â·    Â·                            string | byte array | stream
     Â·    Â·                                     ^^^^^^^^^^
     Â·    Â·                                  binary,custom type,
     Â·    Â·                                  deserialize POCO
     Â·    }
     Â·    ,
     Â·    {                          â†BÂºoutputÂºbinding
     Â·    Â·Âº"type": "table",Âº
     Â·    Â· "tablename": "outtable",
    â”ŒÂ·â†’   Â· "name": "$return",       â† how fun.provides output
    |Â·    Â·                            "out" param available in C#/C#Script
    |Â·    Â· "direction": "out",      â† in|out|inOut
    |Â·    Â· "connection": "conn01"   â† pre-setup app setting,
    |Â·    Â·                            avoiding to store secrets in JSON.
    |Â·    }
    |Â·    ]
    |Â·  }
    |Â·
    |Â·â”” Ex. Function (C# script)
    |Â·  #r "newtonsoft.JSON"
    |Â·  using microsoft.extensions.logging;
    |Â·  using newtonsoft.JSON.linq;
    |Â·
    |Â·  public class person {
    |Â·      public string partitionkey { get; set; }
    |Â·      public string rowkey { get; set; }
    |Â·      public string name { get; set; }
    |Â·      public string mobilenumber { get; set; }
    |Â·  }
    |Â·
    |â”” Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· Â· â”
    |   public static person run(jobject order, ilogger log) {
    â””â†’    return new person() {
            partitionkey = "orders",
            rowkey = GUID.newGUID().tostring(),
            name = order["name"].tostring(),
            mobilenumber = order["mobilenumber"].tostring() };
        }

  BÂºExample 2: Using code decorator attributes inÂº
  BÂº           code params and functions (C#/C#Sharp)Âº

    -BÂºInput message â†’ function â†’ BLOB
       ^^^^^^^^^^^^^              ^^^^
       queue-storage              output
       trigger                    binding

      [functionName("queuetrigger")]
      [return: BLOB("output-container/{id}")]
      public static string run(
        [queuetrigger("inputqueue")] workitem input,
        ilogger log)
      {
          string JSON = string.format("{{ \"id\": \"{0}\" }}", input.id);
          log.loginformation($"c# script processed queue message.
      item={JSON}");
          return JSON;
      }

BÂºBINDING EXPRESSIONSÂº
- expressions resolving sources to values:
  ex 1:
  BLOB.path property = container/Âº{queuetrigger}Âº
                                  ^^^^^^^^^^^^^^
                           expression resolves to
                           message.text. Ex. for
                           message "helloworld", BLOB
                           "container/helloworld" is
                           created.

  â”” bindingÂºexpressions typesÂº
    â”” app settings: (secrets, ENV.VARs, ...)
    Â·Âºpercent signsÂº(vs curly braces) used. Ex:
    Â· Âº%env_var01%Âº/newBLOB.txt
    Â·  ^^^^^^^^^^^
    Â·  local-run values come from
    Â· Âº'local.settings.JSON'Âº
    Â·
    â”” trigger filename: BLOB path for trigger
    Â·
    â”” trigger  metadata (vs data payload):
    Â· - It can be used as input C# params|properties
    Â·   in context.bindings object in javascript.
    Â·   ex:
    Â·   a.queue storage trigger supports the following
    Â·   metadata properties:
    Â·   (accessible in "function.JSON")
    Â·   - queuetrigger     - insertiontime
    Â·   - dequeuecount     - nextvisibletime
    Â·   - expirationtime   - popreceipt
    Â·   - id
    â”” JSON payloads:
      - can be referenced in configuration for other
        bindings in the same function and in function
        code.
        - ex: "function.JSON" file for a webhook function
              that receiving
              { QÂº"BLOBname"Âº:"helloworld.txt" }

      function.JSON:
      {
        "bindings": [
          {                                  â† Input trigger
          Â· "name": "info",
          Â·Âº"type": "httptrigger",Âº
          Â· "direction": "in",
          Â· "webhooktype": "genericJSON"
          }
          ,
          {
          Â· "name": "BLOBcontents",
          Â· "type": "BLOB",
          Â· "direction": "in",               â† Input from BLOB
          Â· "connection": "webJobsStorage01" â† PRE-SETUP value
          Â· "path": "strings/{BLOBname}",  â† !!!
          },
          {                               â† Output result binding
          Â· "name": "res",
          Â· "type": "http",
          Â· "direction": "out"
          }
        ]
      }

      {
        "type": "BLOB",
        "name": "BLOBoutput",
        "direction": "out",
        "path": "my-output-container/{datetime}"    â†  2018-02-16t17-59-55z.txt
        "path": "my-output-container/{rand-guid}"
      }
</pre>

<pre zoom labels="azure,functions,serverless,comparative">
<span xsmall>Best Practices</span>
BÂºBEST PRACTICESÂº
  - avoid long running functions
  - cross function communication:
  BÂºdurable-functions and Logic-apps are built to manage stateÂº
  BÂºtransitions and communication between multiple functions. Âº
  BÂºIn any other case, it is generally a best practice to useÂº
  BÂºstorage queues for cross function communicationÂº
    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
  OÂºCHEAPER AND EASIER TO PROVISION!!!Âº
    â˜ Remind: messages limited to 64 KB.
                       â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
    - Use service bus queue for larger sizes
      - up to 256 KB in standard tier
      - up to   1 MB in premium tier
    - service bus topics (vs queue) also allows
      for message filtering before processing.

    - event hubs prefered (to storage queue and service-bus)
      if very-high-volume communications is expected.

  - write functions to be BÂºstateless and idempotentÂº (if possible).
    adding state information with input/output data.

  - write defensive functions:
    - assume exception arise at any time (external involved
      services errors, networking outages, quota limits, ...).
    - design for resiliance to failures.
      ex: function logic:
        - query 10_000 rows in DB.
        - create queue message for each row
        Defensive apporach:
        - track row "completed". If function fail at 5000,
          that will allow to avoid 5000 duplicates.

    - allow no-op for duplicated inputs.

BÂºSCALABILITY BEST PRACTICESÂº
  â””  re-use, share and manage connections.
  â””  don't mix test+production code in the same function.
  â””BÂºif a shared assembly is referenced in multiple .NET functions, Âº
   BÂºput it in a common shared folder. reference the assembly with aÂº
   BÂºstatement similar to (using c# scripts .csx):Âº
   BÂº#r "..\shared\myassembly.dll"Âº in order
     to accidentally deploy multiple versions of same binary.
   â˜ Remember: All functions in a funct. App share the same resources.

  â””  Skip RÂºverbose logging in production codeÂº.

  â””  use async code, avoid (thread-blocking) calls
     -RÂºAvoid referencing result property / calling wait methodÂº
        on a task instance that could lead to RÂºthread exhaustionÂº.

  â””  Use message-batchs when possible (event hub,...)
     Max batch size can be configured in "host.JSON"
     as detailed in reference documentation.

  â””  C#: Use strongly-typed array.

  â””  configure host.JSON behavior (host runtime, trigger behaviors)
     to better handle concurrency.
     - concurrency for a number of triggers can be tunned,
       often adjusting the values in these options.
     â˜ settings apply to all functions within the app.
       within a single instance of the function.
       Ex:
       you have a function app with:
       - 2 http functions, each scaling to 10 instances, sharing
         the same resources.
       - concurrent requests set to 25
       Any incomming request to any http trigger would count towards
       the shared 25 concurrent requests.
       The 2 functions would effectively allow 250 concurrent requests
       (10 instances * 25 concurrent requests per instance).
</pre>

<pre zoom labels="azure,dev_framework,">
<span xsmall>Durable fun.</span>
BÂºOverviewÂº
  - functions and webjobs BÂºextension allowing to writeÂº
  BÂºcomplex stateful functions in a serverless environmentÂº
  BÂºDura.fun. MANAGES STATE, CHECKPOINTS AND RESTARTS transparentlyÂº.
    local state is never lost on process recycle|VM reboots.
  - Can be seen as an "orchestrator of functions".
  - Provides for stateful workflows in an BÂº"orchestrator function"Âº.
  - With Durable Functions, itâ€™s possible to make functions
    depended on each other, with separated billing costs.
    Ex: Function A takes 300 milliseconds to execute,
        but it calls Function B, taking another extra
        2000 milisecs to complete. Function A is only
        charged for 300 millisecs.
    <a target="_new" href="https://www.patrickvankleef.com/2018/08/27/exploring-city-with-azure-durable-functions-and-the-custom-vision-service/">REF</a>
  - orchestrator functions advantages:
    -BÂºdefine workflows in codeÂº(vs JSON schemas or designers)
    - Code can sync/async call other functions.
      Return value can be saved to local variables.
      -BÂºAutomatic progress checkpoint when function awaitsÂº.

BÂºUse CasesÂº:
  â”” pattern #1: function chaining in order.
    often output of one function needs to be applied
    to the input of another function.  (a la Unix Pipeline).
    Ex code:
    public static async taskË‚objectËƒ
      run(BÂºDurableOrchestrationContext ctxÂº) {
        try {
            // â˜ "f1", "f2", ... : names of other functions in
            // funct-app
            var x = awaitBÂºctxÂº.callactivityasyncË‚objectËƒ("f1");
            var y = awaitBÂºctxÂº.callactivityasyncË‚objectËƒ("f2", x);
            var z = awaitBÂºctxÂº.callactivityasyncË‚objectËƒ("f3", y);
            return  awaitBÂºctxÂº.callactivityasyncË‚objectËƒ("f4", z);
                     ^     â””â”¬â”˜
                     â”‚ â˜ BÂºctx allow to invoke other functions   Âº
                     â”‚   BÂºby name, passing params, and returningÂº
                     â”‚   BÂºfunction output.Âº
                     â”‚
                    await triggers a progress-checkpoint instance.
                  BÂºif the process|VM recycles the execution,    Âº
                  BÂºit iwll resumes from theÂºpreviousÂºawait call.Âº
        } catch (exception) {
          ... handling/compensation goes here
        }
    }

    note: subtle differences exists between C#â†â†’C#Script.
    C# requires durable parameters to be decorated with respective
       attributes.
       (Ex [orchestrationtrigger] for durableOrchestrationContext
       param).  Otherwise runtime can not inject variables into
       the function.

  â”” Âºpattern #2: fan-out/fan-inÂº
    execute multiple functions in parallel, then wait
    for all to finish. (often some aggregation work is done on
    individual results).
    - with non-durable functions, fanning out can be done by
      sending multiple messages to a queue while fanning back
      is much more challenging:
      code must track when queue-triggered functions ends, and
      store function outputs.
    - with non-durable functions, fanning out can be done by
                                  custom mechanism (queues,...)
    - with     durable functions, code is much simpler. Ex:
      public static
      async task run(DurableOrchestrationContext ctx) {
          var paralleltasks = new listË‚taskË‚intËƒËƒ();

          // get a list of n work items to process in parallel
          object[] workbatch = await BÂºctxÂº.
                                 callActivityAsyncË‚object[]Ëƒ("f1");

          for (int i = 0; i Ë‚ workbatch.length; i++) {
              TaskË‚intËƒ BÂºtaskÂº = BÂºctxÂº.       //  â† fan-out work distributed
                   callActivityAsyncÂºË‚intËƒ      //     to N f2 instances
                   ("f2", workbatch[i]);
              paralleltasks.add(task);          // â† dynamic list trace tasks
          }

          await taskBÂº.whenAllÂº(paralleltasks); //  â† wait for all called fun. to finish.
                                                //    Similar to <a href="http://www.oficina24x7.com/DevOps/devops_map.html?query=9737647d-58dc-4999-8db4-4cd3c2682edd">barrier shell synchronization</a>
          int sum = paralleltasks.              // â† Aggregate all outputs
                    sum(t =Ëƒ t.result);
          await ctx.                            // â† send result to f3
                callactivityasync("f3", sum);
      }

  â”” Âºpattern #3: async http APIsÂº
     coordinating state of with external clients long-running operations.
     Solution 1:
     - Trigger long-running action in (quick-to-return) http call,
       leaving operation runnning in remote client.
     - poll periodically some end-point to learn when operation completes.

     durable functions simplify this pattern:
     - once an instance is started, Durable functions
       automatically exposes a  webhook http APIs that
       clients can use to query the progress.
       Ex:
       - STEP 1) start an orchestrator/query its status.
         $ BASE_URI="https://myfunc.Azurewebsites.NET/"
       $Âº$ curl -x post ${BASE_URI}/orchestrators/dowork    Âº
       $Âº  -h "content-length: 0" -i                        Âº
           â†’ http/1.1 202 accepted
           â†’ content-type: application/JSON
           â†’ location:
           â†’ https://...
           â†’
           â†’ {"id":Âº"b79b...."Âº, ...}

         $ STATUS=${BASE_URI}/admin
         $ STATUS=${STATUS}/extensions/durabletaskextension/
                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       Automatically exposed by Dura.Fun.engine.
         $ STATUS=${STATUS}/b79b... -i
       - STEP 2) Query status
       $Âº$ curl ${STATUS}                                   Âº
           â†’ http/1.1 202 accepted
           â†’ ...
           â†’ {"runtimestatus":GÂº"running"Âº,"lastupdatedtime":"...", ...}

       - STEP 2) (let time pass)
       $Âº$ curl ${STATUS}                                   Âº
           http/1.1 200 ok
           ...
           {"runtimestatus":BÂº"completed"Âº,"lastupdatedtime":"...", ...}

     - http-triggered function to start a new orchestrator
       function instance wrapping function Âº"wrappedFunction"Âº.
       public static
       async taskË‚httpResponseMessageËƒ run(
         httprequestmessage req,
       BÂºDurableOrchestrationClientÂºstarter,
         stringÂºwrappedFunctionÂº, â† value taken from incoming URL
         ilogger log) {
         // wrapped function name comes from input URL.
         // function input comes from the request content.
         dynamic eventdata = await req.content.
                              readAsyncË‚ObjectËƒ();
         string instanceId = await BÂºstarterÂº. // â† Start Orchestra.
                     startNewAsync(ÂºwrappedFunctionÂº,
                     eventdata);
         return starter
                .createCheckStatusResponse(req, instanceId);
       }

  â””Âºpattern #4: monitoringÂº
    - flexible recurring process in a workflow, ex:
      polling until condition is/are met
    - A regular timer-trigger works for simple scenarios,
      with static intervals. Managing instance lifetimes can
      become complex.
    - durable functions allows for flexible recurrence intervals,
      task lifetime management, and the ability to create multiple
      monitor processes from a single orchestration:
      Ex: reversing pattern #3 async http API scenario.
           instead of exposing an endpoint with status
           wrapping a "wrappedFunction" for external
           clients, make the long-running monitor consume
           an external endpoint, waiting for some state change.
      - multiple monitors can be set observing arbitrary endpoints
        in few lines-of-code.
      - monitors can end execution when some condition is met or
        terminated by the DurableOrchestrationClient.
      - wait interval can be changed based on some condition
        (exponential backoff)
        Ex: C# script
        public static
        async task run(BÂºDurableOrchestrationContext ctxÂº) {
          int jobid           = BÂºctxÂº.getinputË‚intËƒ();
          int pollinginterval = getpollinginterval();
          datetime expirytime = getexpirytime();
          while (BÂºctxÂº.currentutcdatetime Ë‚ expirytime) {
            var jobstatus = await
                BÂºctxÂº.callactivityasyncË‚stringËƒ
                           ("getjobstatus", jobid);
              if (jobstatus == "completed") {
                await BÂºctxÂº.callActivityAsync(
                   "sendalert", machineid);
                break;
              }
              // sleep until this time
              var nextcheck = BÂºctxÂº.currentUTCDatetime
                              .addseconds(pollinginterval);
              await BÂºctxÂº.
                 createTimer(nextcheck, cancellationtoken.none);
          }
          // ... further work here or orchestration end
        }

  â””Âºpattern #5: (slow)human interactionÂº
    People are not highly available and responsive as computers.
    orchestrator will use a durable timer to request approval
    and escalate in case of timeout.
    It waits for an external human generated events.

    Ex C# script:
    public static
    async task run(BÂºDurableOrchestrationContext ctxÂº) {
      awaitBÂºctxÂº.callActivityAsync("requestapproval");
      using (var timeoutcts = new cancellationtokensource()) {
        Datetime duetime = BÂºctxÂº.currentUTCDatetime.addhours(72);
        task durabletimeout = BÂºctxÂº.createtimer(duetime,
               timeoutcts.token);

        taskË‚boolËƒ approvalevent =
            BÂºctxÂº.waitforexternaleventË‚boolËƒ("approvalevent");

        if ( approvalevent ==
                await taskBÂº.whenAnyÂº(approvalevent, durabletimeout)) {
            timeoutcts.cancel();
            await BÂºctxÂº.callactivityasync("processapproval",
             approvalevent.result);
        } else {
            await BÂºctxÂº.callactivityasync("escalate");
        }
      }
    }
    an external client can deliver the human generated event notification
    to a waiting orchestrator function using either the built-in http
    APIs or by using DurableOrchestrationClient.raiseEventAsync API
    from another function:
    public static async task run(string instanceid,
      DurableOrchestrationClient client) {
        bool isapproved = true;
        await client.raiseeventasync(instanceid, "approvalevent", isapproved);
    }

BÂºDURABLE FUNCTION INSIDE TECHNOLOGYÂº
- behind the scenes, durable functions extension is built on top of
  the durable task framework, an open-source library on github for
  building durable task orchestrations.
-BÂºmuch like how Azure functions is the serverless evolution of   Âº
 BÂºAzure webjobs, durable functions is the serverless evolution ofÂº
 BÂºthe durable task framework.                                    Âº
-OÂºIt is heavily within Microsoft and outside as well to automateÂº
 OÂºmission-critical processes.Âº

- It reliably maintain execution state using a design pattern known
  as BÂºevent sourcingÂº:  append-only-store recording full series
  of actions taken by the function orchestration.
  Benefits include:
  - performance, scalability, responsiveness
    compared to "dumping" full runtime state.
  - eventual consistency for transactional data
  - full history and audit trails enabling reliable compensating
    actions.
  - Event sourcing by this extension is transparent. under the
    covers, the await operator in an orchestrator function yields control
    of the orchestrator thread back to the durable task framework
    dispatcher. the dispatcher then commits any new actions that the
    orchestrator function scheduled (such as calling one or more child
    functions or scheduling a durable timer) to storage. this transparent
    commit action appends to the execution history of the orchestration
    instance. the history is stored in a storage table. the commit action
    then adds messages to a queue to schedule the actual work. at this
    point, the orchestrator function can be unloaded from memory. billing
    for it stops if you're using the Azure functions consumption plan.
    when there is more work to do, the function is restarted and its
    state is reconstructed.
  - once an orchestration function is given more work to do
   (for example, response msg received or a durable timer expires),
   the orchestrator wakes up again and re-executes the entire function
   from the start in order to rebuild the local state.
   if during this replay the code tries to call a function
   (or do any other async work), the durable task framework consults
   with the execution history of the current orchestration.
   if it finds that the activity function has already executed
   and yielded some result, it replays that function's
    result, and the orchestrator code continues running. this
    continues happening until the function code gets to a point
    where either it is finished or it has scheduled new async work.

  - the replay behavior creates constraints on the type of code that
    can be written in the function. For example, orchestrator
  BÂºcode must be deterministicÂº, as it will be replayed multiple
    times and must produce the same result each time.

- Âº(2020-03) LANGUAGE SUPPORTÂº
  - C# (functions v1 and v2)
  - F# and javascript (functions v2 only)
  -  support for all languages planned.

- ÂºSTORAGE AND SCALABILITYÂº
  - Durable functions extension transparently uses queues, tables,
    and BLOBs to persist execution history state and trigger function
    execution.
  -RÂºA separate storage account can be needed due to storageÂº
   RÂºthroughput limits.Âº

  - queue messages: Used to schedule activity and receive responses.
    In the consumption plan, these queues are monitored by
    the Azure functions scale out/in compute instances
  - table storage : store execution history for orchestrator accounts.
    Tool like "microsoft Azure storage explorer" can be used to see
    the history.
  -  storage BLOBs: used primarily as leasing mechanism to coordinate
                    the scale-out of orchestration instances across
                    multiple VMs.
                    Also used to hold data for large messages which
                    cannot be stored directly in tables or queues.
</pre>

<pre zoom labels="azure,java,TODO">
<span xsmall>Quarkus Azure Functions</span>
@[https://quarkus.io/guides/azure-functions-http]
Âºquarkus-azure-functions-httpÂº extension allows to write
microservices with RESTEasy (JAX-RS), Undertow (servlet),
Vert.x Web, or Funqy HTTP and make these microservices
deployable to the Azure Functions runtime.

One azure function deployment can represent any number of JAX-RS,
servlet, Vert.x Web, or Funqy HTTP endpoints.
</pre>


<span title>App Automation</span>
<pre zoom labels="azure,aaa,security,devops,runbook,TODO,resource">
<span xsmall>Automation</span>
@[https://docs.microsoft.com/en-us/azure/automation/automation-intro]
- Automation Runbooks (Python/Powershell/GUI) automate resource management.
- Capabilities:
  - Process Automation:  Orchestation
  - Configuration Management: Collect inventory, track changes, desired state
  - Update Management: Assess compliance, scheduled updates
  - Windows/Linux, Azure/On-Premises.

- In addition, PowerShell Desired State Configuration (DSC) is available
<hr/>
<span xsmall bgblue>Runbook Gallery</span>
-@[https://docs.microsoft.com/en-us/azure/automation/automation-runbook-gallery]
 contains runbooks for common tasks (shutting down, deploying VMs...)
</pre>
</div>
</div><!-- App Service -->


<div group> <!-- Storage -->
<span title>Azure STORAGE</span>
<hr/>
<div groupv>
<span title>A.table (NoSQL)</span><br/>
<pre zoom labels="azure,storage,">
<span xsmall>Azure table storage overview</span>
Âºintroduction to table storage in AzureÂº
a.table storage (datastore) service:
- stores large BÂºstructuredÂº RÂºNoSQLÂº (key/attribute) data,
  (RÂºschemaless designÂº)
- access is fast and cost-effective (vs SQL)

- ÂºlimitsÂº:
  - up to the capacity limit of the storage account.
  - any number of entities in a table
  - any number of tables,
  - tables scale on demand.
  - entity size: up to 1mb in table storage
                 up to 2mb in Cosmos DB
                 up to 252 properties.

- Azure Cosmos DB table API: premium offering :
  - throughput-optimized tables
  - global distribution
  - automatic secondary indexes.
  - (Cosmos DB allows also for SQL and other DDBB model

- it  accepts authenticated calls from in/out-side a.cloud.
Âºcommon use cases:Âº
 - storing tbs of structured data
 - datasets that Âºdon't require complex joins,Âº foreign keys,
   or stored procedures and RÂºcan be denormalizedÂº for fast access
 - quickly querying data using a clustered index
 - accessing data using the odata protocol and linq
   queries with wcf

storage account 1 â†â†’ n table â†â†’ 1â†â†’n entity 1â†â†’ 1 property set
                                                  (up to 252 keys)

http://${storage_acc}.table.core.windows.NET/  ${table} â† a.table storage
http://${storage_acc}.table.Cosmosdb.Azure.com/${table} â† a.Cosmosdb table API
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       |
                      direct access with Âºodata protocolÂº.
                      more info at:
                    @[http://www.odata.org/]

- all access to Azure storage   is done through a BÂºstorage   accountÂº
- all access to Azure Cosmos DB is done through a BÂºtable API accountÂº

- each entity also has three system properties that specify:
 -Âºpartition keyÂº:BÂºentities with the same partition key Âº
                  BÂºcan be queried more quickly, and     Âº
                  BÂºinserted/updated in atomic operationsÂº
 -Âºrow key      Âº: unique id within partition.
 -Âºtimestamp    Âº: last-modified timestamp (lmt) used to
                   manage optimistic concurrency.
                   table REST API "etag"  = lmt
                   (used interchangeably)

Âºchoosing table storage or Cosmos DB table APIÂº
"Cosmos-DB-table-API" and "table-storage" SDKs:
-  share  same table data model

           |table storage              | Cosmos DB
           |                           | table API
-----------+---------------------------+---------------------
latency    |fast/but no upper bounds   | Ë‚10-ms reads
           |                           | Ë‚15-ms writes
           |                           | (99th percentile)
           |                           | any scale/worldwide
-----------+---------------------------+---------------------
throughput |variable model             | highly scalable with
           |tables limit: 20,000 op/s  | dedicated reserved throughput
           |                           | per table that's backed by
           |                           | slas. accounts have no upper
           |                           | limit on throughput and
           |                           | support Ëƒ10 million
           |                           | operations/s per table.
-----------+---------------------------+---------------------
global     |single region              | turnkey global distribution
distribu.  |opt: 1 readable secondary  | from 1 to 30+ regions.
           |    read region for HA     | automatic/manual failovers
           |you can't init failover    | any time, anywhere
-----------+---------------------------+---------------------
indexing   |only primary index         | - automatic, complete indexing
           |onÂºpartitionkeyÂºandÂºrowkeyÂº|   on all properties.
           |no secondary idxs.         | - no index management.
-----------+---------------------------+---------------------
query      |pk index can be used.      | queries can take advantage of
           |scans otherwise.           | automatic indexing on
           |                           | properties
-----------+---------------------------+---------------------
consistency|strong   within primary reg| five well-defined consistency
           |eventual within secondary "| levels to trade off
           |                           | availability, latency,
           |                           | throughput, and consistency
           |                           | based on your application
           |                           | needs.
-----------+---------------------------+---------------------
pricing    |storage-optimized.         | throughput-optimized.
-----------+---------------------------+---------------------
slas       |99.99% availability.       | 99.99% availability sla for
           |                           | all single region accounts
           |                           | and all multi-region accounts
           |                           | with relaxed consistency, and
           |                           | 99.999% read availability on
           |                           | all multi-region database
           |                           | accounts industry-leading
           |                           | comprehensive slas on general
           |                           | availability.
-----------^---------------------------^---------------------

- SDKs for .NET (windowsAzure.storage for Table
  microsoft.Azure.Cosmosdb.table for Cosmos DB Table API using
  same APIs and signatures -Cosmos DB Table not yet in .NET core),
- Python (table+Cosmos DB), JAVA
  Node.JS (BÂºclient Browser compatible!!Âº),
  powerShell (mStorageTable module),
  C++, Ruby and PHP.
<hr/>

<span xsmall>table design</span>
Âºdesign to be read-efficient:Âº
- querying/read heavy applications:
  - think about the queries (especially latency sensitive ones)
- specify pk = (partitionkey, rowkey) in queries.
- consider duplicate copies of entities,
 Âºstoring the same entity multiple timesÂº
  (with different keys) for efficient queriesÂº.
- considerÂºdenormalizing dataÂº.
  store summary entities so that queries for aggregate data
  only need to access a single entity.
- use compound key values: the only keys you have are
  (partitionkey/rowkey).
  for example, use compound key values to enable
  alternate keyed access paths to entities.
- use query projection reducing amount of data
  transferred over the net by using queries that
  select only the needed fields.

Âºdesign to be write-efficientÂº:
-Âºdo not create hot partitionsÂº: choose keys that enable to
  spread requests across multiple partitions.
-Âºavoid spikes in trafficÂº: smooth over time.
-Âºdon't necessarily create a separate table for each type of entityÂº.
  when you require atomic transactions across entity types, you
  can store these multiple entity types in the same partition in the
  same table.

Âºdesign scalable and performant tablesÂº
consider factors such as performance, scalability, and cost.
RÂºcounter-intuitive/wrong designs to people familiar with relational DDBBÂº
- design differences reflect  table service target of
  supporting billions of entities ("rows")
  with high transaction volumes.


ex: table storing employee+department entities.
partitionkey rowkey timestamp
marketing    00001  2014-...:32z
                                    firstname lastname age email
                                    don       hall     34  donh@....com

marketing    00002   2014...:34z
                                    firstname lastname age email
                                    jun       cao      47  junc@....com
marketing    dpmt    2014...:30z
                                    departmentname  employeecount
                                    marketing       153

sales        00010   2014...:44z
                                    firstname lastname age email
                                    ken       kwok     23  kenk@....com

^^^^^^^^^^^^^^^^^^^^                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
choice partitionkey,rowkey is       to store complex data types in a
fundamental to good table design.   single property, you must use a
partitiokey rowkey values are idxed JSON/xml/....
to create a clustered index to
enable fast look-ups.
however, the table service does not create any secondary indexes,
soÂºpartitionkey and rowkey are the only indexed properties.Âº

BÂºa solution may consist of a single table that contains all  Âº
BÂºentities organized into partitions, but typically a solutionÂº
BÂºhas multiple tables.                                        Âº
tables help you to logically organize entities, manage access
with acls or drop entire table using a single storage operation.

BÂºTABLE PARTITIONSÂº
  (account name, table name, partitionkey) identify the partition
  within the storage service where the table service stores
  the entity.
  BÂºas well as being part of the addressing scheme for   Âº
  BÂºentities, partitions define a scope for transactions,Âº
  BÂºand form the basis of how the table service scales.  Âº

    node 1 â†â†’ 1+ partitions
           ^^
    table service scales dynamically
    load-balancing partitions across
    nodes.
    table service can split the range of partitions
    serviced onto new different nodes.

  entity group transactions (batch transactions)
  table service entity group transactions (egts):
  - only built-in mechanism for performing atomic updates across
    multiple entities. sometimes referred .
  RÂºwarn:can only operate on entities stored in same partitionÂº
  RÂº(same partition key, different row key)Âº
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   Âºcore reason for keeping multiple entity types in same table/partitionÂº
  - egt limit: at most 100 entities.
  - if simultaneous egts operate on common entities processing
    can be delayed.
  - trade-off:
    more partitions increase scalability/load-balancing
    more partitions limits   atomic transactions and strong consistency

                      /----------------------------------------
                      |capacity
  --------------------+----------------------------------------
  total capacity      |500 tb
  a.storage account   |
  --------------------+----------------------------------------
  num. tables in      |limited by capacity of storage account
  an Azure storage    |
  account             |
  --------------------+----------------------------------------
  # partitions in     | limited by capacity of storage account
  table               |
  --------------------+----------------------------------------
  # of entities       | limited by capacity of storage account
  in partition        |
  --------------------+----------------------------------------
  size of an          | up to 1 mb with 255 properties max
  individual entity   | (including partitionkey/rowkey/timestamp)
  --------------------+----------------------------------------
  partitionkeyize size| string up to 1 kb
  --------------------+----------------------------------------
  size of the rowkey  | string up to 1 kb
  --------------------+----------------------------------------
  size of an entity   | tx can include at most 100 entities and
  group transaction   | payload must be less than 4 mb.
                      | egt can only update an entity once.
  -------------------- ----------------------------------------

BÂºDESIGN FOR QUERYINGÂº
  typically,read scalability design is also also efficient for
  write operations.
  - design queries â†’ design indexes
    note: with table service, it's difficult and expensive to
          change index design (partitionkey,rowkey) later.
          you can not add indexes a posteriory like in DDBBs.

  ex: table storing employee entities (timestamp not shown):
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚column name                     data typeâ”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚partitionkey (department name)  string   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚rowkey       (employee id)      string   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚firstname                       string   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚lastname                        string   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚age                             integer  â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚emailaddress                    string   â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

   fastest lookups:
  -Âºpoint queryÂº (partitionkey, rowkey) used to locate entities.
                 fastest for high-volume (and/or lowest-latency) lookups
                 ex:
                 $filter=(ÂºpartitionkeyÂº eq 'sales') and (ÂºrowkeyÂº eq '2')

  -Âºrange queryÂº (partitionkey, rowkey-range).return 1+ entities.
                 ex:
                 $filter=partitionkey eq 'sales' and
                         rowkey ge 's'           and
                         rowkey lt 't'
               RÂºwarn: using "or" in rowkey filter results
                       in partition scan not range query.
                       $filter=... (rowkey eq '1'RÂºorÂºrowkey eq '3')
  -ÂºpartitionÂº  (partitionkey, non-key filter)
   ÂºscanÂº       ex:
                $filter=partitionkey eq 'sales' and
                        lastname     eq 'smith'

  -Âºtable scanÂº partitionkey RÂºnot used :very inefficientÂº.
                ex:
                $filter=lastname eq 'jones'

  note: results with  multiple entities return them
       Âºsorted in partitionkey and rowkey orderÂº
        to avoid resorting the entities in the client,
        choose a rowkey that defines the most common sort order.
      RÂºwarn:Âº these keys are 'string' values. to ensure that
               numeric values sort correctly, they must be
               string-represented with fixed length padded with
               zeroes: ex: 123  â†’ '00000123'
<hr/>
<span xsmall>authorization in Azure storage</span>
Âºauthorize with shared keyÂº
Âºevery requestÂº against storage service must be authorized.
 exception: public BLOB or container or signed access.

authorizing a request:
- option 1:Âºshared key authorization schemeÂº with the REST API.
  table service ver 2009-09-19+ uses same signature string
  as in previous versions of the table service.

- option 2:  (not detailed here)
  Âºusing a connection stringÂºincluding the authentication information
   required for app to access data in an a.storage account at run time.
   connection strings can be configured to:
   - connect to the Azure storage emulator.
   - access an a.storage account.
   - access specified resources in Azure via
   a shared access signature (sas).

- authorized request required headers:
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 |  http header   |
 |  -----------   |
 | Âºx-ms-dateÂº    | (alternatively 'date' standard http header,
 |                |  with x-ms-date taking preference)
 |                |  coordinated universal time (utc) request timestamp
 |                |  storage servicesÂºensures that a request is no Âº
 |                | Âºolder than 15 minutesÂºby the time it reaches the
 |                |  service.
 |                |- protects from replay attacks and others
 |                |  403 forbidden returned otherwise
 |                |- note: if 'x-ms-date'  != "" constructÂºsignatureÂºwith
 |                |           'date'       == ""
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
 | authorization  | if empty request is considered anonymous:
 |                | only public-access BLOB or container/BLOB/queue/table
 |                | for which a shared access signature has been provided
 |                | for delegated access.
 |                |
 |                | authorization="sharedkey(lite) $accountname:$signature"
 |                |                                              ^^^^^^^^^
 |                |        base64encode(hmacSHA256(request, account_key))
 |                |
 |                |
 |                |
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


Âºsingature how-to:Âº
- build depends on service version authorizing against as well as
  'authorization scheme'. keep in mind:
  - verb portion of string is the uppercase http verb (get, put,...)
  - for shared key authorization for BLOB/queue/file
    each header included in the signature string may appear
    only once.
  - if any header is duplicated, the service returns status
code 400 (bad request).

  - all standard http values must be
    included in the string in the order shown in the signature format,
    headers may be empty if they are not being specified as
    part of the request; in that case, only the new-line character is
    required.

  - all new-line characters (\n) shown are required within sign.string.

  - signature string includes canonicalized headers and
    canonicalized resource strings.

- signature string format for shared key against table service doesn't
  change in any version and it's slightly different from requests
  against BLOB/queue service:
  - it does not include canonicalizedheaders.
  - date headerÂºis never emptyÂº eve if 'x-ms-date' is set.
    'x-ms-date' and 'date' must be the same if both set.
  - stringtosign = verb + "\n" +
                   content-md5 + "\n" +
                   content-type + "\n" +
                   date + "\n" +
                   canonicalizedresource;
  - ver.2009-09-19+: all REST calls must include the
    'dataserviceversion' and 'maxdataserviceversion' headers.

Âºestablishing a stored access policyÂº
an stored access policy:
- provides additional level of control over
  service-level shared access signatures (sas)
  on the server side.
- allows toÂºgroup shared access signaturesÂºand
  to provide additional restrictions for signatures
  that are bound by the policy.
- use-case: change start-time/expiry-time/permissions-for-signature,
            revoke already-issued permissions.

- supported by:
  - queues

  - tables

  - file shares       â† policy can be associated with shared access
                        signature granting perm to:
                        - share     itself           BÂºorÂº
                        - files contained in share

  - BLOB containers   â† policy can be associated with shared access
                        signature granting perm to:
                        - container itself           BÂºorÂº
                        - BLOBs contained in container


Âºcreating/modifying a stored access policyÂº
 -Âºup to 5 access policiesÂºby container/table/queue.
 - call BÂº"set acl"Âºoperation for resource
   - request body specifies terms of the access policy
     Ë‚?xml version="1.0" encoding="utf-8"?Ëƒ
     Ë‚signedidentifiersËƒ
       Ë‚signedidentifierËƒ               â† corresponds 1â†â†’ signed policy
         Ë‚idËƒunique-64-char-valueË‚/idËƒ  â† "custom id", 64chars max
         Ë‚accesspolicyËƒ                 â† (optional) params
           Ë‚startËƒstart-timeË‚/startËƒ
           Ë‚expiryËƒexpiry-timeË‚/expiryËƒ
           Ë‚permissionËƒabbreviated-permission-listË‚/permissionËƒ
         Ë‚/accesspolicyËƒ
       Ë‚/signedidentifierËƒ
     Ë‚/signedidentifiersËƒ

     note: tableÂºentity-range-restrictionsÂº
           Âº(startpk, startrk, endpk, and endrk)Âº
            can not be specified in policy.

ÂºrevokeÂº stored access policy by deleting it or renaming the
         signed identifier (breaking association with existing signature/s)
 - call BÂº"set acl"Âºoperation again, passing only in the set of
   signed identifiers to maintain on the container.
   (empty body to remove all).

Âº(cors) support for the Azure storage servicesÂº
- Azure storage services, ver.2013-08-15 onward.
 (BLOB/table/queue/file services    )
                   ^
                   vr 2015-02-21 onward

- cors allows webapp under one domain to securely access resources into
  another domain, to skip "same-origin policy" restriction.

- cors rules can be set individually for each storage services calling:
    - set BLOB  service properties.
    - set file  service properties.
    - set queue service properties.
    - set table service properties.

 BÂºnote:Âº cors is not an authentication mechanism.

 RÂºwarn:Âº cors is RÂºnotÂº supported for premium storage accounts.
<hr/>
<span xsmall>table service REST API</span>
Âºtable services resourcesÂº
resources available through REST API:
- Âºstorage accountÂº:OÂºparent namespaceÂºfor table service.
   storage account 1 â†â†’ n tables
- ÂºtablesÂº
- ÂºentityÂº

Âºquery timeout and paginationÂº
two types of query operations:
-Âºquery tablesÂº  operation returns the list of tables within the
  specified storage account. it may be filtered.
-Âºquery entitiesÂºoperation returns a set of entities from
  specified table filtered according to request criteria.

Âºlimits:Âº
 - 1_000 max items at one time
 - 5 seconds max execution.
 - query must not cross partition boundary.
 - total time allocated to the request for scheduling
   and processing the query is 30 seconds, including
   5 seconds for query execution.

 BÂºnoteÂº: if limit is passed response headers will provide for
 OÂºcontinuation tokensÂºto resume the query at the
   next item in the result set.

   continuation token header          description
   x-ms-continuation-nexttablename    return in query tables ops.
                                      hash of next table name
   x-ms-continuation-nextpartitionkey return in query entities ops.
                                      next part.key
   x-ms-continuation-nextrowkey       return in query entities ops.
                                      next row key (may be null)
                                      return in query entities ops.

   ms.NET client library manual handling:
   1st) cast result to queryoperationresponse object.
 after) continuation token headers can be accessed
        in headers prop. of instantiated object.

- next "cloned"-queries can "continue" query by using
  request headers:
  - nexttablename
  - nextpartitionkey
  - nextrowkey

RÂºwarnÂº: for tx updates, operation may have succeeded on the
         server despite (30 secs timeout) error being returned.

Âºsample response headers and subsequent requestÂº

  date: mon, 27 jun 2016 20:11:08 gmt
  content-type: application/JSON;charset=utf-8
  server: windows-Azure-table/1.0 microsoft-httpapi/2.0
  cache-control: no-cache
  x-ms-request-id: f9b2cd09-4dec-4570-b06d-4fa30179a58e
  x-ms-version: 2015-12-11
  content-length: ....
BÂºx-ms-continuation-nextpartitionkey:Âº1!8!u21pdgg-
BÂºx-ms-continuation-nextrowkey:Âº1!8!qmvuotk5
  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  next request would be like:
  ...?ÂºnextpartitionkeyÂº=1!8!u21pdgg-&ÂºnextrowkeyÂº=1!12!qmvumtg5oa--

Âºquerying tables and entitiesÂº

BÂºaddressing data-resources in queries follows odata proto.spec.Âº

OÂºbase_url="https://${storage_account}.table.core.windows.NET/Âº

 ${base_url}/tables      â† list tables
                           to create/delete table, refer to
                           the set of tables in the specified
                           storage account.

 ${base_url}/tables(${t})â† return single table


 ${base_url}/mytable()   â† query entities in a table
                           to insert/update/delete an entity,
                           refer to that table directly within
                           the storage account.

                           this basic syntax refers to the
                          Âºset of all entitiesÂºin the named table


ÂºSUPPORTED QUERY OPTIONSÂº
  system             description
  query option
  -----------------------------------------------
  $filter            15 max discrete comparisons
                     eq gt ge lt le ne and not or
  -----------------------------------------------
  $top               return top n results
  -----------------------------------------------
  $select            initial subset.
                     ver 2011-08-18 onward
  -----------------------------------------------

â˜ BÂºquery parameters must be URL encoded:Âº
    - chars / ? : @ â…‹ = + , $ must be scaped.
    - ' must be represented as ''
       Ex:  o'clock â†’ o''clock

  Ex queries:
  - ${base_url}/customers()?$top=10

  - ${base_url}/customers(partitionkey='partition01',rowkey='r_key01')
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€primary key â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                          alt2: specify pk as part of $filter

ÂºCONSTRUCTING FILTER STRINGSÂº
  â˜keep in mind:
  - property name, operator, and constant value must be separated
    by URL-encoded spaces (%20)
  - filter string are case-sensitive.
  - constant value must be of same data type as property
    note: be sure to check whether a property has been explicitly
    typed before assuming it is of a type other than string.
    if property has been explicitly typed, its type is indicated
    within the response or returned entity. otherwise string type applied
  - enclose string constants in single quotes.
  - Example queries:
  ...tab01$filter=partitionkey%20eq%20'partition01'%20and%20rowkey%20eq%20'r_key01'
                  ^^^^^^^^^^^^                              ^^^^^^
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ filter by pk â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  ...tab01()?$filter=lastname%20eq%20'smith'%20and%20firstname%20eq%20'john'
                     ^^^^^^^^   ^^                   ^^^^^^^^^   ^^
                                        ^                               ^
                                        â””â”€â”€â”€â”€â”€ wildcard not supported â”€â”€â”˜
                                               prefix match allowed through comparisions
  ..tab01()?$filter=age%20gt%2030
                             ^^^^
                             do not quote for numeric properties

  ..tab01()?$filter=isactive%20eq%true
                                  ^^^^
                                  boolean

  filtering on datetime properties

  ...tab01()?$filter=activation%20eq%20datetime'2008-07-10t00:00:00z'
                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                       datetime value format

  ...tab01()?$filter=guidvalue%20eq%20guid'a455c695-df98-5678-aaaa-81d3367e5a34'
                                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
                                      guid value format

BÂºINSERTING AND UPDATING ENTITIESÂº
- include with the request anÂºodata atom or odata JSON entityÂº
  specifying the properties and data for the entity.

 note:Âºupdate-entity-operationÂºreplaces current entity.
      Âº merge-entity-operationÂºupdates properties but does not replace it,
                               it creates a new one.

- Ex atom feedÂº
 RÂºwarnÂº: suported up to ver.2015-12-11.
                         ^^^^^^^^^^^^^^
                        JSON for this ver. onward

 ( m:type indicates type value type for a given property)

 Ë‚?xml version="1.0" encoding="utf-8" standalone="yes"?Ëƒ
 Ë‚entry
 xmlns:d="http://schemas.microsoft.com/ado/2007/08/dataservices"
 xmlns:m="http://schemas.microsoft.com/ado/2007/08/dataservices/metadat
 a" xmlns="http://www.w3.org/2005/atom"Ëƒ
   Ë‚title /Ëƒ
   Ë‚authorËƒ
     Ë‚name /Ëƒ
   Ë‚/authorËƒ
   Ë‚id /Ëƒ
   Ë‚content type="application/xml"Ëƒ
     Ë‚m:propertiesËƒ    â†  entity's property defs
       Ë‚d:address                      Ëƒmountain viewË‚/d:addressËƒ
       Ë‚d:age     m:type="edm.int32"   Ëƒ23Ë‚/d:ageËƒ
       Ë‚d:amntdue m:type="edm.double"  Ëƒ200.23Ë‚/d:amountdueËƒ
       Ë‚d:code    m:type="edm.guid"    Ëƒc9da6455-...Ë‚/d:customercodeËƒ
       Ë‚d:since   m:type="edm.datetime"Ëƒ2008-07-10t00:00:00Ë‚/d:customersinceËƒ
       Ë‚d:isactiv m:type="edm.boolean" ËƒtrueË‚/d:isactiveËƒ
       Ë‚d:ordernu m:type="edm.int64"   Ëƒ255Ë‚/d:numofordersËƒ
       Ë‚d:binary  m:type="edm.binary" m:null="true" /Ëƒ
      ÂºË‚d:partitionkeyËƒmypartitionkeyË‚/d:partitionkeyËƒÂº
      ÂºË‚d:rowkeyËƒmyrowkey1Ë‚/d:rowkeyËƒÂº
     Ë‚/m:propertiesËƒ
   Ë‚/contentËƒ
 Ë‚/entryËƒ


Âºexample JSON feedÂº
{
   "address":"mountain view",
   "age"    :23,
   "amntdue":200.23,
   "code@odata.type" :"edm.guid"    ,  "customercode":"c9da6455-..",
   "since@odata.type":"edm.datetime",  "customersince":"2008-07-10t00:00:00",
   "isactive":true,
   "ordernu@odata.type":"edm.int64" ,  "ordernu":"255",
  Âº"partitionkey":"mypartitionkey",Âº
  Âº"rowkey":"myrowkey"Âº
}
</pre>
</div>

<div groupv>
<span title>Cosmos DB</span><br/>
<pre zoom labels="azure,storage,">
<span xsmall>Cosmos DB overview</span>
ÂºAzure Cosmos DBÂº
DDBB service native to Azure providing high-performance database
BÂºregardless of your selected API or data modelÂº offering
multiple APIs and models (key-value, column-family, document, graph)

Âºcore functionalityÂº

Âºglobal replicationÂº
- turnkey global distribution automatically replicates data
  to other Azure datacenters across the globe.

- consistency levels:
-----------|------------------------------------------------
 strong    | reads guaranteed to be visible across replicas
           | before writes is fully committed across all
           | replicas.
           | write operation performed on primary database,
           | replicated to the replica instances.
           | write is committed (and visible) on the primary
           | only after it has been committed and confirmed
           | by all replicas.
-----------|------------------------------------------------
 bounded   | similar to the strong level
 staleness | but you can configure how stale
           | documents can be within replicas.
           | staleness refers to the quantity
           | of time (or the version count) a
           | replica document can be behind the
           | primary document.
-----------|-----------------------------------------------
 session   | it guarantees that all read/write
           | operations are consistent within a user
           | session. within user session, all
           | reads and writes are monotonic and
           | guaranteed to be consistent across
           | primary and replica instances.
-----------|-----------------------------------------------
 consistent| this level has loose consistency but
 prefix    | guarantees that when updates show up in
           | replicas, they will show up in the
           | correct order (that is, as prefixes of
           | other updates) without any gaps.
-----------|-----------------------------------------------
 eventual: | writes are readable immediately, and replicas
           | are eventually consistent with the primary.
           | commits any write operation against the
           | primary immediately. replica
           | transactions are asynchronously handled
           | and will eventually (over time) be
           | consistent with the primary. this tier
           | has the best performance, because the
           | primary database does not need to wait
           | for replicas to commit to finalize it's
           | transactions.

Âºchoose the right consistency level for your applicationÂº
ÂºSQL API and table APIÂº
- for manyÂºreal-world scenarios, session consistency is optimalÂº
- ifÂºstronger consistency that sessionÂºis required,
  andÂºsingle-digit-millisecond latency for writesÂº
  apply, bounded staleness is recomended.
- if eventual consistency is needed, consistent-prefix is
  recomended.
- for less strict consistency guarantees consistent-prefix
  is still recomended.
- if highest availability and lowest latency, then use
  eventual consistency level.

Âºconsistency guarantees in practiceÂº
- stronger consistency guarantees may be obtained in practice.
-BÂºconsistency guarantees for a read operation correspond to        Âº
 BÂºthe freshness and ordering of the database state that you requestÂº
 BÂºread-consistency is tied to the ordering and propagation ofÂº
 BÂºthe write/update operations.Âº

 - inÂºbounded-stalenessÂº, Cosmos DB guarantees that
  Âºclients always read the value of a previous writeÂº,
   with aÂºlag bounded by the staleness windowÂº.
 -Âºstrong ==  staleness window of zeroÂº:
   clients are guaranteed to read latest committed value
   of the write operation.
 - for remaining three consistency levels, staleness window
   is largely dependent on app workload. for example,
BÂº if there are no write operations on the database, a readÂº
BÂº operation with eventual, session, or consistent prefix  Âº
BÂº consistency levels is likely to yield the same results  Âº
BÂº as a read operation with strong consistency level.      Âº

   you can find out the probability that clients get strong
   and consistent reads for workloads by looking at the
 BÂºprobabilistic bounded staleness (pbs)Âº metric
   exposed in the BÂºAzure portalÂº:
   - it shows how eventual is Cosmosdb configured eventual consistency.
     providing insights into how often clients get a stronger
     consistency than the consistency level currently.
    Âºprobability (measured in milliseconds) of getting   Âº
    Âºstrongly consistent reads for a combination of writeÂº
    Âºand read regionsÂº

ÂºCONSISTENCY LEVELS AND Azure Cosmos DB APIsÂº
Âºfive consistency modelsÂº natively supported by the a.Cosmos DBÂºSQL APIÂº
(SQL API is default API):
- native support for wire protocol-compatible APIs for
  popular databases is also provided including
 Âºmongodb, cassandra, gremlin, and Azure table storageÂº.
  RÂºwarn:Âº these databases don't offer precisely defined consistency
    models or sla-backed guarantees for consistency levels.
    they typically provide only a subset of the five consistency
    models offered by a.Cosmos DB.
- for SQL API|gremlin API|table API default consistency level
  configured on theÂºa.Cosmos DB accountÂºis used.

comparative cassandra vs Cosmos DB:
cassandra 4.x       Cosmos DB           Cosmos DB
                    (multi-region)      (single region)
one, two, three     consistent prefix   consistent prefix
local_one           consistent prefix   consistent prefix
quorum, all, serial bounded stale.(def) strong
                    strong in priv.prev
local_quorum        bounded staleness   strong
local_serial        bounded staleness   strong

comparative mongodb 3.4 vs Cosmos DB

mongodb 3.4         Cosmos DB           Cosmos DB
                    (multi-region)      (single region)
linearizable        strong              strong
majority            bounded staleness   strong
local               consistent prefix   consistent prefix

ÂºAzure Cosmos DB supported APIsÂº
BÂºthe underlying data structure in Azure Cosmos DB is a data modelÂº
BÂºbased on atom record sequences that enabled Azure Cosmos DB to  Âº
BÂºsupport multiple data models.                                   Âº
because of the flexible nature of atom record sequences,
Azure Cosmos DB will be able to support many more models and
APIs over time.

-Âºmongodb APIÂº acts massively scalable mongodb service.
  it is compatible with existing mongodb libraries, drivers, tools, and apps.

-Âºtable APIÂº acts as a key-value database service with premium
  capabilities (automatic indexing, guaranteed low latency,
  global distribution).

-Âºgremlin APIÂº acts as a fully managed, horizontally scalable graph
  database service to build and run applications that work with
  highly connected datasets supporting open graph APIs
 (based on apache tinkerpop spec, apache gremlin).

-Âºapache cassandra APIÂº acts as a tglobally distributed apache
  cassandra service compatible with existing apache cassandra
  libraries, drivers, tools, and applications.

-ÂºSQL APIÂº is a JS+JSON native API providing query capabilities rooted
  in the familiar SQL query language.
  it supports the execution of javascript logic within the database
  in the form of stored procedures, triggers, and user-defined functions.


Âºmigrating from nosqlÂº
cassandra API supports cqlv4
mongodb   API supports mongodb v5.

for successful migration keep in mind:
- do not write custom code. use native tools (cassandra shell,
  mongodump, and mongoexport).

- Cosmos DB containers should be allocated prior to the
  migration with the appropriate throughput levels set.
  many of the tools will create containers for you with
  default settings that RÂºare not idealÂº.

- prior to migrating, you should increase the container's
  throughput to at least 1,000 request units (rus) per second
  so that import tools are not throttled.
  throughput can be reverted back after import is complete.

BÂºCONTAINERS â…‹ ITEMSÂº

  account                                          ÂºRESOURCE HIERARCHYÂº
  1
  â””â†’n database
      1
      â””â†’n collection container == collection|graph|table *1
          1
          â”” n â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
             â”Œâ”€â”€â”¼â”€â”€â”€â” â”Œâ”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†“â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â†“â”€â”€â”€â”€â”€â”
             â”‚items â”‚ â”‚specs  triggers   userâ”€defined  â”‚    â”‚conflictsâ”‚
             â””â”€â”€â”€â”€â”€â”€â”˜ â”‚                  functions     â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  *1:you can also scale workloads across collections,
    if you have a workload that needs to be partitioned,
    you can scale that workload by distributing its
    associated documents across multiple collections.
   Âºcollection escalability types can beÂº
  ( can be defined at creation in Azure portal)
   - fixed    : max.limit of 10 gb and 10_000 ru/s throughput.
   - unlimited: to create it, you   must specify abÂºpartition keyÂº
                and a minimum throughput of 1_000 ru/s.

                (logical)         (physical)          logical
                collection 1 â†â†’ n partitions  1 â†â†’ n  partition
                                  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜          ^^^^^^^^^
                                      â”‚               data store
                                      â”‚               associated with
                                      â”‚               partition key val
                                      â”‚
             - fixed amount of reserved solid-state drive (ssd)
               combined with a variable amount of compute resources
               (cpu and memory), replicated for high availability.
               it is an internal concept of Azure Cosmos DB.
               they are transient.
             - number of physical partitions is determined by Cosmos DB
               based on the storage size and throughput provisioned
               for a container or set of containers.
               (similar to sharding pattern)

  account    it is associated with a set of databases and a fixed amount
             of large object (BLOB) storage for attachments.
            Âºone or more database accounts can be created per Azure subscription.Âº

  database:  logicalÂºcontainer of document storage partitioned across collectionsÂº.
             it is also aÂºusers containerÂº

  collection container of JSON documents and associated javascript
  container  application logic.  collections can span one or more
             partitions or servers and can scale to handle practically
             unlimited volumes of storage or throughput.

  document   user-defined JSON content. by default, no schema needs to
  (item)     be defined nor do secondary indexes need to be provided for
             all the documents added to a collection.

  stored     application logic written in JS and registered with a
  procedure  collection and executed within the database engine as a
  (sproc)    transaction.

  trigger    application logic written in JS executed
             before or after either an insert/replace/delete op

  user       application logic written in JS.  enabling developers to model a
  defined    custom query operator and thereby extend the core SQL API query
  function   language.


BÂºCollections in Cosmos DB SQL APIÂº

  â””â˜ÂºDDBBs are essentially containers for collections.Âº
    collections: place for individual documents.
                 it automatically grows and shrinks.
  â”” each collection is assigned a maximum Âºthroughput valueÂº
  â””  alternatively, you can assign the throughput
    at the database level and share the throughput values
    among all collections.
  â”” if a set of documents needs throughput beyond the
    limits of an individual collection, they can be
    distributed among multiple collections.
    each collection has its own distinct throughput level.

  â”” you can also scale workloads across collections,
    if you have a workload that needs to be partitioned,
    you can scale that workload by distributing its
    associated documents across multiple collections.

  â”” Cosmos DB SQL API includes a client-side partition
    resolver that allows you to manage transactions and
    point them in code to the correct partition based on
    a partition key field.

  â””Âºcollection typesÂº
    (can be defined at creation in Azure portal)
    - fixed    : max.limit of 10 gb and 10_000 ru/s throughput.
    - unlimited: to create it, you must specify a BÂºpartition keyÂº
                 and a minimum throughput of 1_000 ru/s.
                 (otherwise it will not automatically scale)
      ^^^^^^^^^
    - to migrate the data fixed â†’ unlimited, you need to use the
      data migration tool or the change feed library.

    -  Cosmos DB containers can also be configured to share throughput
       among the containers in a database.
<hr/>
 <span xsmall>C# how-to</span>
Âºmanage collections and documents by using the microsoft .NET SDKÂº
Cosmos DB SQL API pre-setup:
-Âºmicrosoft.Azure.documentdb.coreÂº nuget package

using microsoft.Azure.documents;        â† imports
using microsoft.Azure.documents.client;
...
documentclient docclient01 =
   new documentclient(new URI("[endpoint]"), "[key]");
                                ^
                               Cosmos DB
                               account
                               endpoint


-any resource reference in the SDK  needs a URI.
 urifactory static helper methods will be used
 for common Cosmos DB resources. ex. collection URI:
 URI collectionuri = urifactory.
                     createDocumentCollectionURI(
                        databasename, collectionname
                     );

 var document = new {  // â† any c# type allowed in SDK
   firstname = "alex",
   lastname = "leh"
 }

 await docclient01.
 createdocumentasync(collectionuri, document); // â† insert

 to query the DB:
   // alt 1: sqlqueryspec:
   var query = client.createdocumentqueryË‚familyËƒ(
     collectionuri,
     new sqlqueryspec() { // â† perform SQL query
       querytext = "select * from f where (f.surname = @lastname)",
       parameters = new sqlparametercollection() {
           new sqlparameter("@lastname", "andt")
       }
     },
     defaultoptions
   );
   var families = query.tolist(); // â† result

   // alt 2: c# language-integrated query (linq)
   // linq expressions will be automatically translated
   // into the appropriate SQL query:

   var query = client.createdocumentqueryË‚familyËƒ(collectionuri)
       .where (d =Ëƒ d.surname = "andt")
       .select(d =Ëƒ new { name = d.id, city = d.address?.city)
       .asdocumentquery();

   var families = query.tolist();
</pre>

<span title>SQL ddbb</span>
<pre zoom labels="azure,storage,SQL,saas,IaaS,">
<span xsmall>Azure SQL overview</span>
- supports relational data, JSON, spatial, and xml.
- columnstore indexes for extreme analytic analysis and reporting
- in-memory oltp for extreme transactional processing.
- dynamically scalable performance within
  two differentÂºpurchasing models:Âº
  - Âºvcore-basedÂº:
  - Âºdtu-basedÂº  :
-BÂºcode base shared with microsoft SQL server database engineÂº
 ( newest capabilities of SQL server released first to
   SQL database, then to SQL server itself)

GÂºWORKLOADS OPTIONSÂº:
GÂºâ”” SQL server on VMs (IaaS):Âº
    - Cons: patching, backups, ... is manual.
      Pros: full control over the database engine,
            (switch recovery model simpler|bulk-logged,
             pause/start engine at will,...)
    - Pice options include:
      -Âºpay-as you-goÂº:
        - SQL server license included with SQL server image
        -Âºreuse existing licenseÂº.

GÂºâ”” hosted service (PaaS) Azure SQL database:Âº
    - fully-managed based on latest stable
      enterprise edition of SQL server.
    - built on standardized hardware and software
      owned, hosted, and maintained by microsoft.
    - Âºpay-as-you-goÂº
    - options to scale up or out
    - additional features not available in SQL server.
      (built-in intelligence and management)

    BÂºDEPLOYMENT OPTIONSÂº
    BÂºâ””  logical serverÂº:
        - managed by aÂºlogical serverÂº:
        - most of database-scoped features of SQL server are available.
        - Single database or elastic database pool
          share resources.

    BÂºâ””  managed instancesÂº:
        (part of a collection of ...)
        - shared resources for databases  and additional
          instance-scoped features.
        - support forâ˜Âºdatabase migration from on-premisesÂº.
        - all of the paas benefits of Azure SQL database but
          adds capabilities of SQL on VMs:
          - native virtual network (VNET)
          - nearÂº100% compatibility with on-premises SQL serverÂº.

   SQL server on VM        Azure SQL database     Azure SQL database
                           (managed instance)     (logical server)
   -----------------------------------------------------------------
                               PROS
   -----------------------------------------------------------------
   full control of         high on-premises       most common SQL server
   SQL server engine       compatibility          features available.

   up to 99.95%            99.99% availa.         99.99% availa.
   availability.           guaranteed             guaranteed

   full parity with        built-in backups,      built-in backups,
   on-premises             patching, recovery.    patching, recovery.
   SQL server.

   fixed, well-known       latest stable          latest stable
   engine version.         engine                 engine

   easy migration from     easy migration from    resources (cpu/storage)
   on-premises.            SQL server             assigned individually to
                                                  each DDBB.
   private ip address      private ip address
   within Azure vnet.      within Azure vnet.

                           built-in advanced      built-in advanced
                           intelligenceâ…‹securit   intelligenceâ…‹securit

                           online (cpu/storage)   online (cpu/storage)
                           change                 change

   Can share VM resources
   with application code

   ---------------------------------------------------------------
                            CONTS
   ---------------------------------------------------------------
   manual manage backup    minimal number of       migration from
   and patches             SQL-server features     SQL server might be hard
                           not available.
   manually implement                              some SQL server features
   HA solution.                                    are not available.

                           compatibility with      compatibility with
   downtime while changing SQL server version can  SQL server version can
   resources(cpu/storage)  be achieved only using  be achieved only using
                           database compatibility  database compatibility
                           levels.                 levels.

                           no guaranteed exact     no guaranteed exact
                           maintenance time        maintenance time
                           (nearly transparent)    (nearly transparent)

                                                   private ip address
                                                   cannot be assigned
                                                   (firewall rules
                                                    still available).

BÂºDDBB: Transactionally Consistent copy HOW-TOÂº
BÂºâ”” copy target params include:Âº
    - same/different destination server
    - same/different service tier
    - same/different compute size

  â˜ NOTE: automated database backups are used when creating a DDBB copy.

BÂºâ”” FIXING LOGIN/USERS IN DDBB COPYÂº
  GÂºâ”” Copying to same logical serverÂº:
      - same logins persists.
      - ÂºSecurity principal used to run the copy becomes theÂº
        Âºowner of the new DDBBÂº
      - all DDBB users, user-permissions, and user-security-identifiers
        (SIDs) are copied to DDBB copy.

  GÂºâ”” Copying to different logical serverÂº:
      - ÂºSecurity principal used to run the copy becomes theÂº
        Âºowner of the new DDBBÂº and is assigned a
        new security identifier (SID).
      -  if using contained-in-DDBB-users for data access,
         ensure that both primary and secondary DDBBs always
         have the same user credentials, warranting access
         with same credentials.
      -  if using A.AD, managing credentials in the copy is
         not needed, RÂºhowever, login-based access might not workÂº
       RÂºin new copy because logins do not exist on destination server:Âº.
         Only the login initiating the copy (new owner in copy) can work
         before remapping users. To resolve logis:
         - after copy is online, use "alter user" statement to remap
           the users from the new database to logins on the destination
           server.
         - all users in the new database retain permissions.

BÂºâ”” DDBB copy from A.PortalÂº
     portal â†’ ... open database page â†’ click "copy"

BÂºâ”” DDBB copy with PowerShellÂº
     $ new-Azurermsqldatabasecopy
         -resourcegroupname "myresourcegroup" `
         -servername $sourceserver `
         -databasename "mysampledatabase" `
         -copyresourcegroupname "myresourcegroup" `
         -copyservername $targetserver `
         -copydatabasename "copyofmysampledatabase"
</pre>

<pre zoom labels="azure,storage,SQL,saas,IaaS,csharp">
<span xsmall>Entity Framework "CRUD"</span>
- object-relational mapperÂºlibrary for .NETÂº
- Reduce relationalâ†â†’OO impedance|mismatch
- goal: - enable developers interact with relational databases
          using strongly-typed .NET objects representing
          the application's domain.
        - eliminate repeated "plumbing" code.

BÂºentity framework core vs entity frameworkÂº
- Entity Framework core (EF core) is a rewrite of
  Entity Framework ("full") targeting .NET standard.
  â˜ recomended over the "full" non-core old one.

BÂºentity framework providersÂº (2020-03)
  - SQL server    - mySQL/mariaDB        - DB2
  - SQLite        - myCat server         - Informix
  - PostgreSQL    - SQL server compact   - Oracle
  - firebird      - MS access
  also:GÂºin-memory provider: useful to test componentsÂº

  - SQL server provider:
    OOSS project, part of Entity Framework Core
  @[https://github.com/aspnet/entityframeworkcore]

  - MySQL:
    - MySQL official plus third-party groups:
      - mysql.data.entityframeworkcore
       (https://www.nuget.org/packages/mysql.data.entityframeworkcore).
      - pomelo.entityframeworkcore.mysql
       (https://www.nuget.org/packages/pomelo.entityframeworkcore.mysql/).

  - PostgreSQL:
    - multiple third-party libraries:
      - npgsql.entityframeworkcore.postgresql
      @[https://www.nuget.org/packages/npgsql.entityframeworkcore.postgresql/]

    - devart.data.postgresql.efcore
    @[https://www.nuget.org/packages/devart.data.postgresql.efcore/]

BÂºModeling a DDBB with EF-coreÂº
  â”” conventions:
    - based on the shapes of entity model(classes).
    - providers might also enable a specific configuration

  â”” 1) create model mapping:
       (entities,relationships) â†â†’ DDBB tables
       Ex.:
       â†’ Initial DDBB table:
       Â· BLOGS TABLE
       Â· -------------------------------------
       Â·ÂºblogidÂºOÂºURLÂº         QÂºdescriptionÂº
       Â· ------   ------------   -----------
       Â· 1        /first-post    first post ...
       Â· 2        /second-post   null
       Â·
       â”” â†’ Initial OÂºPOCOÂº: (No Framework Related)
         Â· public class OÂºBlogÂº {
         Â·   public intÂºblogIDÂº { get; set; }
         Â·    public stringOÂºURLÂº { get; set; }
         Â·    public stringQÂºdescriptionÂº { get; set; }
         Â· }
         Â·
         â”” â†’ Initial In-memory "Database":
           Â· public class blogDataBase {
           Â·      public IEnumerableË‚BlogËƒ  â† GÂºDBSetË‚ËƒÂº methods allows to query the DDBB using
           Â·           blogs { get; set; }   Âºlanguage-integrated queryÂº(LINQ), by implementing
           Â· }                                IEnumerableË‚Ëƒ interface, giving access to many of
           Â·                                  the existing LINQ queries.
           Â·
           â”” â†’ Mark classes as models of database,
               by including a type metadata that
               entity framework can interpret:

               â”” â†’BÂºalt 1: fluent APIÂº
               Â·    - â˜ no need to modify entity classes.
               Â·    - Âºhighest precedenceÂº overriding conventions and
               Â·       data annotations.
               Â·
               Â·    - override "onModelCreating" in derived context class
               Â·    - use BÂº"modelBuilder API"Âº to configure the model.
               Â·
               Â·      protected override void
               Â·      onModelCreating(BÂºmodelBuilder MBÂº)    â† â˜ GÂºConventionÂº: types mentioned in
               Â·      {                                            "onModelCreating" are included
               Â·        BÂºMBÂº.entityË‚OÂºBlogÂºËƒ()                    in the model
               Â·             .haskey    (c =Ëƒ c.ÂºblogIDÂº)
               Â·             .property  (b =Ëƒ b.OÂºURLÂº)
               Â·             .isrequired()
               Â·             .property  (b =Ëƒ b.QÂºdescriptionÂº);
               Â·      }
               Â·
               â”” â†’BÂºalt 2: data annotationsÂº
                    - higher precedence over conventions. ex:
                      public class blog {
                          [key]
                          public int ÂºblogIDÂº          { get; set; }
                          [required]
                          public string OÂºURLÂº         { get; set; }
                          public string QÂºdescriptionÂº { get; set; }
                      }

BÂºsystem.data.entity.DBContext implementationÂº
BÂºâ”” primary interaction point with frameworkÂº
     (also known as "context class"). used to:
     - write, then execute queries.
     - convert query-results to entity instances.
     - track and persist-back changes made objects.
     - bind entities in memory to UI controls.

BÂºâ”” PRE-SETUP:Âº
    - create a model (Previous section)

BÂºâ”” recommended way to work:Âº
    - define a GÂºderived class from DBContextÂº
     Âºexposing DBSet-properties representing collectionsÂº
      of entities in the context.
      Ex:
      public class blogContext : GÂºDBContextÂº {
        public GÂºDBSetÂºOÂºË‚blogËƒÂº                       â† â˜ GÂºConventionÂº, types exposed in
               blogs { get; set; }                         DBSet props are included in
      }                                                    the model

   â˜ GÂºConventionÂº: any types found recursively exploring
                 the navigation properties of discovered
                 types are also included in the model.


BÂºQuerying DDBBsÂº
  - Ex: load all data from table:
    listË‚BlogËƒ allBlogs =
        context.GÂºblogsÂº.tolist();             â† get all table

    IEnumerableË‚blogËƒ someblogs =
        context.GÂºblogsÂº.where                 â† filter
           (b =Ëƒ b.OÂºURLÂº.contains("dotnet"))

    Blog blog01 = context.GÂºblogsÂº.single      â† get single instance
           (b =Ëƒ b.ÂºblogidÂº == 1);               matching a filter

  -BÂºwhen calling LINQ operators, Entity Framework builds up an in-memoryÂº
   BÂºrepresentation of the queryÂº.
     - query is sent to the database on-demand, when results are
       being consumed. (iterating result, calling tolist,toarray,
       single, or count).

BÂºdata binding the results of a query to a uiÂº
  - EF protects from  SQL injection.
  - EFRÂºdoes not validate inputÂº.
</pre>
</div>

<div groupv>
<span title>BLOB storage</span>
<pre zoom labels="azure,storage,blob,">
<span xsmall>BLOB overview</span>
- optimized for storingÂºmassive amounts of UNstructured dataÂº
  (files, logs, backups, ...).
- object access via (http/https) with A.Storage REST API,
  cli/PowerShell, client library (.NET, Java, node.JS,
  python, go, php, and ruby).

-ÂºA.DATA LAKE Storage gen2Âº
  - Microsoft's enterprise big data analytics solution for cloud.
  - It offers a hierarchical file system as well as the
    advantages of BLOB storage (low-cost, tiered storage,
    HA, strong consistency, disaster recovery).

BÂºBLOB storage resources typesÂº

  â”‚storageâ”‚ 1 â†-----â†’ N   â”‚containerâ”‚   1 â†-------------â†’ N  â”‚BLOBâ”‚
  â”‚accountâ”‚                ^                                  ^
   ^                       â”‚                                  â”‚
 - unique namespace        - names are lowercase              â”‚
   in Azure for data       - organize set of BLOBs            â”‚
   endpoint for stor.        in hierachies                    â”‚
   account will be:          "a la fileâ”€system"               â”‚
   https://${sto_acct}.BLOB.core.windows.net                  â”‚
                                                              â”‚
  GÂºTypesÂº  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
-GÂºBLOCK BLOBÂº:
  text and binary data, up to about 4.7 TB, â† In A.storage emulator BLOBs
-GÂºAPPEND BLOBSÂº:                             are limited to 2 gb max.
  - made up of blocks (like block BLOBs)      â”‚
  - optimized for append operations.          â”‚
    (logging,..)                              â”‚
-GÂºPAGE BLOBSÂº:                               â”‚
  - optimized random access up to 8 TB.   â†â”€â”€â”€â”˜
  - Used to store Virtual HD(VHD) files
    in VMs.

- all BLOBs types reflect committed changes immediately.
- each version of the BLOB has aÂºunique "etag"Âº,
  that can be used to assure new changes applied to
  a specific instance of the BLOB.
-Âºall BLOBs can be leased for exclusive write accessÂº.
  only calls that include the current lease ID will
  be allowed to modify the (block in block BLOBs) BLOB.

BÂºMoving data to A.Storage.Âº
BÂºâ”” storage data movement .NET libraryÂº:
    - move data between A. storage services.
BÂºâ”” azcopyÂº:
    - .NET cli tool
    â”€ copy  to/from:  â”‚storageâ”‚ â†â”€â†’ â”‚containerâ”‚ â†â”€â†’ â”‚BLOBâ”‚ â†â†’ â”‚File-shareâ”‚
                      â”‚accountâ”‚

    $ CORE=core.windows.net

    $ azcopy /source:$source /dest:$destination [options]  â† Copy BLOB

    $ azcopy                                               â† Download BLOB
        /source:https://myaccount.BLOB.${CORE}/mycontainer
          /dest:c:\myfolder /sourcekey:key /pattern:"abc.txt"
                                           ^^^^^^^^^^^^^^^^^^
                                         - match prefix
                                         - if abstent download all BLOBs
                                           (add /s flag)
                                         â˜ remember, no folder hierarchy exists.
                                           entire BLOB-path constitutes the name.


    $ azcopy \                                           â† copy BLOBs
      /source:https://myaccount.BLOB.${CORE}/container01   from container01
        /dest:https://myaccount.BLOB.${CORE}/container02     to container02
      /sourcekey:key /destkey:key \
      /pattern:abc.txt

    $ azcopy                                             â† Copy BLOB/s
      /source:https://account01.BLOB.${CORE}/containerA    from account01
        /dest:https://account02.BLOB.${CORE}/containerB      to account02
      /sourcekey:key1 /destkey:key2
      /pattern:abc.txt
    $ azcopy                                             â† Copy BLOB/s
      /source:https://account01.file.${CORE}/myfileshare/  from file-share
        /dest:https://account02.BLOB.${CORE}/mycontainer/    to BLOB-container
      /sourcekey:key1 /destkey:key2
      /s


    - NOTE: azcopy is done asynchronously by default.
      "/synccopy" option ensures that the copy
      operation gets a consistent speed by
      downloading the BLOBs to copy from the
      specified source to local memory and
      then uploading them to the BLOB storage
      destination
BÂºâ”” A.Data factoryÂº:
    â”€ copy data to/from â”‚BLOBâ”‚ by using account-key,
      shared-access-signature, service-principal or managed identities
      for A.resource authentication.
BÂºâ”” BLOBfuseÂº: linux vfs driver for BLOB storage.

BÂºâ”” A.DATA BOX DISKÂº:
    â”‚ On premise â”‚ â†’ â”‚SSDsâ”‚ â†’ Microsoft â†’ â”‚ Storage â”‚
    â”‚ Data       â”‚    ^        Upload     â”‚ Account â”‚
                      provided
                      by Microsoft
BÂºâ”” A.IMPORT/EXPORTÂº:
    â”‚ Storage â”‚ â†’ Microsoft â†’ â”‚ HDs â”‚ â†’ â”‚ On premise â”‚
    â”‚ Account â”‚   Download      ^       â”‚ Data       â”‚
                              provided
                              by Client

GÂºA.BLOB STORAGE TIERSÂº                              BÂºSTORAGE ACCOUNTS SUPPORTING TIERING:Âº
GÂºâ”” hot  storage:Âº                                   BÂºâ”” BLOB storage              account
    - optimized for   frequently accessed data.      BÂºâ”” general purpose v2 (gpv2) account
GÂºâ”” cool storage:Âº                                       - new pricing structure for BLOBs, files,
    - optimized for infrequently accessed data.            and queues, and other new storage
    - accessed and stored for at least 30 days.            features as well.
      (short-term backup/recovery datasets)              RÂºWARNÂº: some workloads can be more
    - must tolerate slightly lower availability                   expensive on gpv2 than gpv1.
      but still requires high durability and similar              Check before switching.
      time-to-access and throughput as hot.              - previous accounts types allow to specify
    - slightly lower availability SLA and                  default storage tier at account level
      lower storage cost and RÂºhigher access costsÂº        as hot or cool.
GÂºâ”” archive storage:Âº
    - optimized for  rarely accessed and stored for
      at least 180 days with flexible latency
      requirements ("hours").
    - lowest storage cost andRÂºhighest access costsÂº
    RÂºonly available at BLOB(vs storage account) levelÂº
    - data here is offline/cannot be read, copied,
      overwritten, modified or "snapshoted" (metadata is
      online and can be read). change to another tier to
      make use of it. "rehydratation" can take up to 15 hours.
      large BLOB sizes recommended for optimal performance.

â˜ to manage and optimize costs it's helpful to organize data
  based on attributes like:
  - frequency-of-access
  - planned retention period.


BÂºblock BLOBsÂº
  - efficient upload of large BLOBs.
  - each block is identified by "block id".
  - Block Commiting:
    - Each commit creates a new BLOB version.
    - Blocks need to be commited (after upload) to be part
      of the BLOB version Until commited|discarded
      block-status is "uncommited".
    - commitment-list-order (vs upload time order) determines
      the real order of blocks in block.
    - uncommitted blocks can be replaced by new blocks without
      affecting the BLOB version.
    - uncommited expiration time: 1 week, discarded otherwise.
    - Blocks not included in commitment are automatically discarded.
    - commit-lists with repeated blocks will insert the same block
      in different offsets in final BLOB.
    - commitments operation fails in any commit-listed block is not found.
    - commitments overwrites the BLOB's existing properties and metadata.

  - BLOB Limits:
    - blocks can be of different size.
      up to 100 MB max    per Block  (up to 4 MB using REST API Ë‚= 2016-05-31)
    x Up to 50,000 blocks per BLOB.
    -----------------------------------
      Up to  ~ 4.75 TB TOTAL BLOB size

  - Upload Limits:
    - BLOBs less than 256 MB (64 MB Ë‚2016-05-31),
      can be uploaded  with a single write operation.
      - Storage clients default to 128 MB max single upload,
        tunnable in BLOBRequestOptions.singleBLOBUploadThresholdInBytes
        (larger BLOBs require to split file into blocks)
      - BLOBRequestOptions.ParallelOperationThreadCount
        tune the number of parallel threads/uploading-blocks
      - Up to 100_000 uncommitted blocks per BLOB,
        Total size of uncommitted blocks Ë‚= 200_000? MB.

  - optional md5-hash for block-verification available in API.

  -ÂºBlock IDsÂº:equal-length strings defined in client.
    - usually base-64 encoding used normalize strings into
      equal lengths (pre-encoded string must Ë‚= 64 bytes).

  - Writing a block to a non-existing BLOB creates a new zero-lengh
    BLOB with uncommitted BLOBs.
    - Discarded automatically after 1 week in no commit is done.

BÂºpage BLOBsÂº
  - collection of 512-byte pages optimized for random read/write.
  - Commit is automatic on page-write.
  - maximum page size is setup and BLOB creation.
  - writes/updates is done uploading 1+(page,512-byte-aligned offset)
  - overwrite is limited to 4 MB maximum. (4 x 1024 x 2 pages)
  - max size: 8 TB.

BÂºappend BLOBsÂº
  - Comprised of blocks optimized for append-operations
    adding to the end of the BLOB only.
  -RÂºupdating/deleting existing blocks is not supportedÂº.
  - unlike a block BLOB, block-ids are not exposed.
  - Limits:
    - each block can be of different size
      up to 4 MB          per Block
      up to 50_000 blocks per BLOB
      ----------------------------
      Up to  =~ 195 GB

BÂºshared access Sign.(SAS)Âº
  - BÂºSAS:URI granting restricted access rights containers,Âº
      BÂºBLOBs, queues, and tables for a specific time intervalÂº
      BÂºincorporatingÂº GÂºall grant-information necessaryÂº
        -GÂºvalidity intervalÂº.
        -GÂºpermissions grantedÂº.
        -GÂºtargeted (shared) resourceÂº.
        -GÂºsignature(to be validatedÂº
         GÂºby storage services)Âº.

      EX: SAS URI providing read/write permissions to a BLOB.
      base_url=https://myaccount.BLOB.core.windows.NET/sascontainer/
      $base_url/sasBLOB.txt?           â† BLOB URI (https highly recommended)
          sv=2012-02-12â…‹               â† storage services version
          st=2013-04-29t22%3a18%3a26zâ…‹ â† start      time (iso-8061 format)
                                                         (empty == "now")
          se=2013-04-30t02%3a23%3a26zâ…‹ â† expiration time
          sr=bâ…‹                        â† resource type = BLOB.
          sp=rwâ…‹                       â† permissions granted
          sig=z%2frhix5xcg0mq2rqi3olwtjeg2tykboxr1p9zuxdtkk%3
          ^^^
          base64(sha256-hmac(message_string, shared_key)

     Âº"...by providing a client with a SAS, it canÂº
     Âºaccess targeted resourceÂº
    BÂºWITHOUT sharing storage-account key with itÂº
      â”” Valet key pattern:
        lightweight service authenticates the client as needed,
        then generates an SAS.

  - BÂºSAS stored access policiesÂº
    - SAS can GÂºtake one of two formsÂº:
      -ÂºAD hoc SASÂº: start/expiration time and permissions
                     are all specified on the sas URI.
                     - This SAS can be created on a container,
                       BLOB, table, or queue.
      -ÂºSAS with a stored access policyÂº:
        -  stored-access-policy is defined on aÂºresource-containerÂº
           for BLOBs/Tables/Queues, and re-used to manage constraints
           for 1+ SASs/resources.
        - When associating SAS to stored-access-policy,
          the former inherits the constraints â€”start/expiration time,
          and permissions- of the later.
          - It allows to revocate permissions for non-expired SASs.
          RÂºThis is not possible for AD hoc SASsÂº. SASs can only
            be invalidated by regenerating the storage-account keys.
</pre>
<pre zoom labels="azure,storage,blob,reactive,event,event_grid,">
<span xsmall>BLOB events</span>
REF:
@[https://docs.microsoft.com/en-us/Azure/storage/blobs/storage-blob-event-overview]
BÂºA.storage eventsÂº
BÂºâ””ÂºOÂºeventsÂº triggered on BLOB creation/deletion,
    (reliably) pushed OÂºA.Event GridÂº, that (reliable) delivers
    to OÂºsubscribersÂº(Functions, Logic apps, custom http listeners,...)
    with rich retry policies and dead-letter delivery.

    ÂºBLOB storageÂº     â”       â”Œâ”€â”€â”€â”€â”€â” event      â”‚ A.Functions
     resource groups   â”¤topics â”‚eventâ”‚ subscript. â”‚ A.Logic apps
     Azure subscriptionâ”¼â”€â”€â”€â”€â”€â”€â†’â”¤grid â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”¤ A.Automation
     event hubs        â”¤       â””â”€â”€â”€â”€â”€â”˜            â”‚ Webhooks
     custom topics     â”˜
     ^^^^^^^^^^^^^^^^^                              ^^^^^^^^^^^^
     event publishers                               event handlers

BÂºâ”” Available in account-types:Âº
    -Âºgeneral-purpose v2Âº storage
    -ÂºBLOBÂº               storage:
      - Specialized storage account for BLOBs.
      - similar to general-purpose storage accounts and
        share all durability, availability, scalability,
        and performance features, including 100% API
        consistency for block-BLOBs and append-BLOBs.

BÂºâ”” BLOB storage event types:Âº
    - microsoft.storage.BLOBcreated: fired on BLOB creation/replacement
                                     ('putBLOB', 'putBlockList', 'copyBLOB' OPs)

    - microsoft.storage.BLOBdeleted: fired on BLOB deletion
      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜             ('deleteBLOB' OP.)
      Univocally identify the
      event as BLOB-storage

BÂºâ”” filtering events:Âº
  - event subscriptions can be filtered based:
    - Âºevent typeÂº  microsoft.storage.(BLOBcreated|BLOBdeleted)
    - Âºcontainer nameÂº
       â˜Remember: â”‚storageâ”‚ 1 â†â†’ N â”‚containerâ”‚ 1 â†â†’ N â”‚BLOBâ”‚
                  â”‚accountâ”‚         ^
                                    organize BLOBs in hierachies
                                    a la F.S.

    - ÂºBLOB nameÂº

  - filters can be applied at event-subscription or later on.
  - Event grid GÂºsubject filtersÂº work based on
  GÂº"begins with"Âº and GÂº"ends with"Âº matches.
  - BLOB storage events GÂºsubjet-formatÂº:
  GÂº/BLOBservices/default/containers/$container/BLOBs/${name}.jpg

    â””-------match all*1     storage events
    â””-------match container storage events ---------â”˜
    â””-------match blob      storage events ----------------â”˜
            ^
          and applying ÂºsubjectBeginsWithÂºfilter
          *1: all = all in storage-account

        â˜ Use somethingÂºsubjectEndsWith (".log") to
          refine filtered event

BÂºPractices for consuming eventsÂº
 - multiple subscriptions can be configured to route events to
   the same event handler, it is important Âºnot to assumeÂº
  Âºthat events are from a particular sourceÂº
   but toÂºcheck the topicÂºof the message to ensure
   that it comes from the storage-account you are expecting.
 - check eventType is the expected one.
   (do not assume defaults)
 - messages can arrive out-of-order:
   - use "etag" field to understand if information about
     objects is still up-to-date.
     REF:
    [https://docs.microsoft.com/en-us/Azure/storage/common/storage-concurrency?toc=%2fAzure%2fstorage%2fblobs%2ftoc.JSON#managing-concurrency-in-blob-storage]
   - use sequencer fields to understand the order of
     events on any particular object.
   - use 'BLOBtype' field (blockBLOB|pageBLOB) to identify
     operations allowed.
   - The 'URL' field in cloud(block|append)BLOB constructors.
   - ignore non-known (reserved) fields.
</pre>

<pre zoom labels="azure,storage,blob,">
<span xsmall>daily use</span>
BÂºset/get properties and metadata using RESTÂº
BÂºâ””Âº(custom) metadata is represented as HTTP headers, that
    can be set along a container|BLOB create-request,
    or explicitely on an independent request for an
    existing resource.
BÂºâ””Âºmetadata header format
    x-ms-meta-name:string-value
    (case-insensitive)
    ver.2009-09-19+ must adhere to C# identifiers naming rules
    - repeated metadata headers returns HTTP error code 400
    - metadata is limited to 8 KB max size.

BÂºâ””ÂºOperations supported:
  - Metadata can overwrite existing keys.
    URI syntax:
    - GET/HEAD
      https://myaccount.BLOB.core.windows.NET/mycontainer?restype=container
        "          "      "    "     "          "        /myBLOB?comp=metadata

    - PUT  â†  RÂºWARNÂº: without any headers clears all existing metadata
      https://myaccount.BLOB.core.windows.NET/mycontainer?comp=metadata?restype=container
      https://myaccount.BLOB.core.windows.NET/mycontainer/myBLOB?comp=metadata

    - Standard HTTP headers supported:
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      CONTAINERS                    BLOBs
    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    Â· etag             Â· etag            Â· content-encoding
    Â· last-modified    Â· last-modified   Â· content-language
                       Â· content-length  Â· cache-control
                       Â· content-type    Â· origin
                       Â· content-md5     Â· range



BÂºmanipulating properties/metadata in .NETÂº
    cloudBLOBclient client = cloudStorageAccount.          â† client allows access to file shares
                             createcloudBLOBclient();

  cloudBLOBcontainer container = client                    â† reference an specific container
                        .getContainerReference("images");
  container.createIfNotExists();                           â† ensure that it exists (hydratated reference)

  await container.fetchAttributesAsync();                  â† retrieve properties and metadata
  BLOBContainerProperties props = container.properties;    â† props can be used now to set/changed
                                                             etag            Used for optimistic concurrency.
                                                                             (Continue if etag is not old)
                                                             lastmodified
                                                             publicaccess    level of public access allowed to container
                                                                             (:= BLOB | container | off | unknown)
                                                             hasLegalHold    container has an active legal hold?
                                                                             it help ensure that BLOBs remain
                                                                             unchanged until the hold is removed.
                                                             hasImmutabilityPolicy  helps to ensure BLOBs are stored
                                                                             for a minimum amount of retention time.

  BLOBcontainermetadata? metadata = container.metadata;   â† metadata can now be set/changed
                         metadata.add("doctype", "textdocuments");
                         metadata["category"] = "guidance";
                         ...
  await container.setMetadataAsync(); // â† persist new metadata

BÂºLease BLOB operationsÂº
  - sets/manages lock on a BLOB for write/delete operations.
  - lock duration:Âº15 to 60 secs or infiniteÂº
                  (60 secs ver Ë‚2012-02-12)

  - Lease Operations:
    -ÂºAcquireÂº: request new lease
    -ÂºRenew  Âº: renew existing lease
    -ÂºChange Âº: change ID of existing lease.
    -ÂºReleaseÂº: free lease /lock
    -ÂºBreak  Âº: end the lease but ensure that another client cannot
                acquire a new lease until the current lease period
                has expired.

  - HTTP/S REQUEST:
    PUT
    https://myaccount.BLOB.core.windows.NET/mycontainer/myBLOB?comp=lease
            â…‹timeout=... â† optional
    http/1.1
    (http://127.0.0.1:10000/devstoreaccount1/mycontainer/myBLOB?comp=lease
     for the emulated storage service)

    request header     description
    --------------------------------------------------------------------------------
    authorization      required. authentication scheme, account name, and signature.
    --------------------------------------------------------------------------------
    date or            required. specifies the coordinated universal
    x-ms-date          time (utc) for the request.
    --------------------------------------------------------------------------------
    x-ms-version       optional. specifies the version of the operation to
                       use for this request.
    --------------------------------------------------------------------------------
    x-ms-lease-id:     id required to renew, change, or release the
                       lease.
                       the value of x-ms-lease-id can be specified in any valid
                       guid string format.
    --------------------------------------------------------------------------------
    x-ms-lease-action  acquireÂ¦renewÂ¦changeÂ¦releaseÂ¦break
    --------------------------------------------------------------------------------
    x-ms-lease-duration -1 if never  expires Â¦ n
    --------------------------------------------------------------------------------
    x-ms-proposed-lease-id "id" optional for acquire, required for change.
                        guid string format.
    --------------------------------------------------------------------------------
    origin              optional. (for cross-origin resource sharing headers)
    --------------------------------------------------------------------------------
    x-ms-client-request-id  optional. provides a client-generated, opaque
                        value with a 1 kb character limit recorded in analytics
                        logs.
    --------------------------------------------------------------------------------
  -ÂºEx. LEASE REQUESTÂº
    â†’ PUT
    â†’ https://myaccount.BLOB.core.windows.NET/mycontainer/myBLOB?comp=lease
    â†’ http/1.1
    â†’
    â†’ request headers:
    â†’ x-ms-version: 2015-02-21
    â†’ x-ms-lease-action: acquire
    â†’ x-ms-lease-duration: -1
    â†’ x-ms-proposed-lease-id: 1f812371-a41d-49e6-b123-f4b542e851c5
    â†’ x-ms-date: $date
    â†’ authorization: sharedkey
    â†’ testaccount1:esskmoydk4o+ngtutyeolbi+xqnqi6abmiw4xi699+o=

    RESPONSE STATUS CODE
    â†     acquire: "OK" status: 201 (created).
    â†     renew:   "OK" status: 200 (ok).
    â†     change:  "OK" status: 200 (ok).
    â†     release: "OK" status: 200 (ok).
    â†     break:   "OK" status: 202 (accepted).

    RESPONSE HEADERS
    â†    etag
    â†    last-modified
    â†    x-ms-lease-id: id
    â†    x-ms-lease-time: seconds
    â†    x-ms-request-id
    â†    x-ms-version
    â†    date
    â†    access-control-allow-origin
    â†    access-control-expose-headers
    â†    access-control-allow-credentials
    â†    authorization

</pre>
<pre zoom labels="azure,code_snippet,js,storage,blob,">
<span xsmall>Browser JS API</span>
<a target="_new" href="https://www.patrickvankleef.com/2018/08/27/exploring-city-with-azure-durable-functions-and-the-custom-vision-service/">REF</a>
Browser Web Cam â†’ upload to Azure Blob storage how-to:

- PRE-SETUP:
  - azure-storage NPM: maintained by Microsoft , it allows
    to upload the blob directly in web JS client code:

    const img      = this.canvas.current.
                     toDataURL('image/jpeg', 1.0).split(',')[1];
    const buffer   = new Buffer(img, 'base64');
    const fileName = `${new Date().getTime()}.jpg`;

    const BASE_URL = 'https://*.blob.core.windows.net/user-images/`;
   ÂºAzureStorage.Blob.Âº
      createBlobService('UseDevelopmentStorage'). \
      createBlockBlobFromText(
        'user-images',
        fileName,
        buffer,
        {contentType:'image/jpeg'},
        (error, result, response) =Ëƒ {
          if (error) {
            // ... handle error
            return;
          }
          const url = BASE_URL + '/${fileName}';
          this.canvas.current.toBlob((blob: Blob) =Ëƒ {
              this.props.closeModal(url, blob);
          });
        }
    );
</pre>

</div>
</div> <!-- Storage -->


<div group> <!-- AAA -->
<span title>implement authentication</span>
<hr/>
<div groupv>
<span title>Active Directory</span>
<pre zoom bgorange labels="azure,AD,101,comparative,diagram,billing,TODO">
<a xsmall href="https://docs.microsoft.com/en-us/azure/active-directory/">Azure Active Directory</a>
PRICE: Free for less than 500_000 objects.

REF: Microsoft cloud identity for enterprise architects
@[https://docs.microsoft.com/en-us/office365/enterprise/microsoft-cloud-it-architecture-resources#identity]
@[https://go.microsoft.com/fwlink/p/?LinkId=524586]

  SaaS                  Azure PaaS           Azure IaaS (VMs,...)
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”Œâ”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
 â”‚ Office â”‚MS Intuneâ”‚â”‚ â”‚â”‚LOB appâ”‚â”‚mob.appâ”‚â”‚ â”‚â”‚LOB appâ”‚ â”‚LOB app@VMâ”‚â”‚
 â”‚ 365    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”˜â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
 â”‚                   â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€^â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€^â”€â”€â”€â”€â”€â”˜
 â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚  LOB: Line of business     â”‚          â”‚
 â”‚        â”‚Dynam.CRMâ”‚â”‚                            â”‚          â”‚
 â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚                            â”‚          â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”
â”‚ÂºAzure AD IntegrationÂº                     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”  â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”â”‚
â”‚                                           â”‚ Domain â”‚  â”‚Extend     â”‚â”‚
â”‚                                           â”‚Servicesâ”‚  â”‚onâ”€premisesâ”‚â”‚
â”‚                                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚directory  â”‚â”‚
â”‚                                                       â”‚services toâ”‚â”‚
â”‚                                                       â”‚your Azure â”‚â”‚
â”‚                                                       â”‚VM         â”‚â”‚
â”‚                                                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
ÂºAZURE ACTIVE DIRECTORY IDaaSÂº              â”‚ÂºLICENCES TYPESÂº
â”œâ”€Âºonâ”€premises infra integrationÂº           â”‚ (Free, Basic, Premium)
â”‚  â”œâ”€ Synch. or federation of identities    â”‚ - Synchronization or federation with
â”‚  â”œâ”€ Self_services password reset with     â”‚   on-premises directories through Azure
â”‚  â”‚  write back to on_premises direc.      â”‚   AD Connect (sync engine)
â”‚  â”œâ”€ Web App Proxy for auth. against       â”‚ - Directory objects
â”‚  â”‚  on_premises web based apps.           â”‚ - User/group management
â”œâ”€ÂºUser AccountsÂº                           â”‚   (add/update/delete), user-based
â”‚  â”œâ”€ MyApps Panel                          â”‚   provisioning, device registration
â”‚  â”œâ”€ Multi_facto Authentication            â”‚ - Single sign-on (SSO)
â”‚  â”œâ”€ Conditional access to resources       â”‚ - Self-service password change for
â”‚  â”‚  and apps                              â”‚   cloud users
â”‚  â”œâ”€ Behaviour and risk_based access       â”‚ - Security and usage reports
â”‚  â”‚  control with ID protection            â”‚
â”œâ”€ÂºDevicesÂº                                 â”‚ (Basic, Premium only)
â”‚  â”œâ”€ Mobile device management with Intune  â”‚ - Group-based access management and
â”‚  â”œâ”€ Windows 10 Azure AD Join and SSO      â”‚   provisioning
â”‚  â””â”€ Device Registrartion and management   â”‚ - Self-service password reset for cloud
â”‚     for non_Win.devices (Android, ...)    â”‚   users
â”œâ”€ÂºPartner CollaborationÂº                   â”‚ - Company branding (logon pages, Access
â”‚  â””â”€ Secure collaboration with business    â”‚   Panel customization)
â”‚     partners using Azure AD B2B           â”‚ - Application Proxy
â”‚     collaboration                         â”‚ - Enterprise SLA of 99.9
â”œâ”€ÂºCustomer Account ManagementÂº             â”‚
â”‚  â””â”€ Self_registration for customers using â”‚ (Premium only)
â”‚     a unique ID or existing social        â”‚ - Self-service group and app
â”‚     identity with Azure AD B2C            â”‚   management, self-service application
â”œâ”€ Application Integration                  â”‚   additions, dynamic groups
â”‚  â”œâ”€ Pre_integrated with 1000s of SaaS     â”‚ - Self-service password reset, change,
â”‚  â”œâ”€ Deep Integra. with Office365          â”‚   unlock with on-premises writeback
â”‚  â”œâ”€ Cloud App Discovery                   â”‚ - Multi-factor authentication (cloud
â”‚  â”œâ”€ PaaS app. integration                 â”‚   and on-premises, MFA Server)
â”‚  â”œâ”€ Domain Services                       â”‚ - MIM CAL + MIM Server
â”‚  â””â”€ Integration with AWS, ....            â”‚ - Cloud App Discovery
â””â”€ Administration                           â”‚ - Connect Health
   â”œâ”€ Reporting                             â”‚ - Automatic password rollover for group
   â”œâ”€ Global Telemetry and                  â”‚   accounts
   â”‚  machine learning
   â”œâ”€ Enterprise scale
   â”œâ”€ Worldwide availability
   â””â”€ Connect Health

<hr/>
<span xsmall>A.Accounts vs Tenant Subs.</span>
@[https://marckean.com/2016/06/01/azure-vs-azure-ad-accounts-tenants-subscriptions/]
</pre>

<pre zoom labels="azure,AD,csharp,development,code_snippet">
<span xsmall>.Net/C# usage</span>
OÂºSystem.Security.Claims.ClaimsPrincipalÂº mscorlib (4.5+)
  Set of attributes that defines the (authenticated) user
       in the context of your application:
       name, assigned roles, ... .

  - An app can implement authentication with different:
    - protocols
    - token formats
    - providers
    - consumption
    - development stacks

OÂºClaimsPrincipal represent the outcome of the auth independently of any methodÂº

- ASP.NET:
  - Usually ClaimsPrincipal is "fetch" from:
    - Thread.CurrentPrincipal
    - HttpContext.Current.User.
    - Custom source (ClaimsPrincipalSelector)
  - example:
    â”‚ ClaimsPrincipal cp = ClaimsPrincipal.Current;
    â”‚ string givenName = cp.FindFirst(ClaimTypes.GivenName).Value;
    â”‚ string   surName = cp.FindFirst(ClaimTypes.Surname  ).Value;
</pre>

<pre zoom labels="azure,AD,aaa,ADFS,security,federation,TODO">
<span xsmall>AD Federation Services</span>
- A Guide to Claims-Based Identity and Access Control (2nd Edition)
@[https://docs.microsoft.com/en-us/previous-versions/msp-n-p/ff423674(v=pandp.10)]

- REF: @[https://www.youtube.com/watch?v=xwWb6S6OvFI]
Prerequisites:
- AD domain controller
- join ADFS server to Domain
- Certificates for both ADFS and claims aware App Server

SETUP:
- Add trust amongst ADFS an App Server certificates


- TODO: Using ADFS as ID Provider for Azure AD B2C
@[https://blogs.msdn.microsoft.com/azuredev/2017/06/23/using-adfs-as-an-identity-provider-for-azure-ad-b2c/]
</pre>

<pre zoom labels="azure,AD,aaa,vm,LDAP,NTLM,Kerberos,TODO">
<span xsmall>AD Domain Services</span>
ADDS allows VMs to join a domain avoiding new domain controllers,
by reusing Azure AD:
User â†’ VM: login
VM   â†’ Corporate_AD: login
...
ADDS feautes:
- domain join
- LDAP
- NTLM
- Kerberos authentication
</pre>
</div>

<div groupv>
<pre zoom labels="azure,AAA">
<span xsmall>Microsoft identity platform</span>
BÂºAzure AD main use-casesÂº:
  - single-page application (SPA)
  - web browser to web application
  - native application to web API
  - web application to web API
  - daemon or server application to web API

BÂºApp  parameters for registration in ADÂº:
  -Âºapplication ID URIÂº: (authentication phase)
    Identifying the targeted(third party) app we want
    access (=="token") for.
    This URI will also be included in tokens during AA phase.
  -Âºreply URL, redirect URI:Âº
    -Âºreply URLÂº   : Webhook receiving auth token
    -Âºredirect URIÂº: unique ID to which Azure AD
                     will redirect the user-agent in an
                     OAuth 2.0 request.
  -Âºapplication IDÂº: (returned during registation).
                     used in requests for  new
                     OÂºauthorization-codeÂº or OÂºtokenÂº
  -ÂºkeyÂº           : sent back in AA along Âºapplication IDÂº
                     on new Authorization requests

BÂºApps classification regarding Identity:Âº
  -Âºsingle tenantÂº: App look in its own directory for a user
                    tenant endpoint might be like:
                  @[https://login.microsoftonline.com/contoso.onmicrosoft.com]
  - Âºmulti-tenantÂº: App needs to identify a specific user from
                    all the directories in Azure AD.
                    common authentication endpoint exists:
                  @[https://login.microsoftonline.com/common]

  - Azure AD uses same signing key for all tokens in all
    directories, single or multi-tenant application.

BÂºApps roles regarding Identity:Âº
-ÂºclientÂº         role : consume (third-party) resources
-Âºresource serverÂºrole : exposes APIs to be consumed by AA app-clients
-Âºclient+reso.srvÂºrole :


BÂºExample Multitenant LayoutÂº:
    adatum Tenant  : used by "HR APP" Resource Provider
    contoso        : used by contoso org ("HR APP" consumer)

adatum â”€ 1 â”€â†’  "HR app" srv.principal   "HR APP"-keys will be used
    â”‚â†-------  @HOME-TENANT             by "HR APP" app
    â”‚                                   ^
    â””â”€â”€â”€â”€â”€â”€â”€â†’  "HR app"                 â”‚
               registration             â”‚
               @CONTOSO-TENANT          â”‚
                  â”‚                     â”‚
                  â”‚2 admin completes    â”‚
                  â”‚ consent             â”‚
                  v                     â”‚
 contoso â”€3â”€â†’  add "HR APP" srv.ppal â”€â”€â”€â”˜
               (@CONTOSO-TENANT)
               it represents the use of an instance
               of the application at runtime, governed
               by the permissions consented by the
               the admin of the CONTOSO-TENANT.
               (What privileges the third-party
               "HR app" is granted in local tenant).
               CONTOSO-TENANT will send OÂºaccess tokenÂº
               to other apps with claims describing
               permissions(scopes) granted to
               (third service) "HR app".

BÂºScopes (=="permissions") defined in Azure ADÂº:
  -BÂºdelegated permissionsÂº:
     present-user (in session?) delegates to registered
     third-party app.
     (directly or through intermediate admin consents)
  -BÂºapplication permissionsÂº:
     background/daemons services can only be consented
     by an admin.

BÂºpermission attributes in A.AD adminÂº
  -ÂºID       Âº: App GUID.
                - ex: 570282fd-fa5c-430d-a7fd-fc8dc98a9dca
  -ÂºisEnabledÂº: is available for use?
  -Âºtype     Âº: user|admin consent
  -Âºvalue    Âº: string used to identify permission during
                OAuth 2.0 authorize flows.
               ÂºIt may also be combined with app ID URIÂº
               Âºto form fully qualified permission.    Âº
                - ex: mail.read
  -ÂºadminconsentdescriptionÂº: shown to admins
  -ÂºadminconsentdisplaynameÂº: friendly name
  -Âºuserconsentdescription Âº: shown to users
  -Âºuserconsentdisplayname Âº: friendly name_of_app

BÂºtypes of consentÂº
  -BÂºSTATIC USER CONSENTÂº: occurs automatically during the
    OAuth 2.0 authorize flow specifying the targeted resource
    that registered app wants to interact with.
    - registered app must  have previously specified all
      needed permissions (In the A.Portal,...).
  -BÂºDYNAMIC USER CONSENTÂº: Azure AD app model v2.
    app requests permissions that it needs in the OAuth 2.0
    authorize flow for v2 apps.
    note: ALWAYS set the static permissions (those specified in
    application AD registration) to be the superset of the
    dynamic (codified as query-param) permissions requested at
    runtime to avoid admin-consent flow errors.
  RÂºWARNÂº: dynamic consent presents a big challenge for
           permissions requiring admin consent
           since the admin consent experience
           doesn't know about those permissions at consent
           time. if you require admin privileged permissions
           or if your app uses dynamic consent, you must
           register all of the permissions in the Azure
           portal (not just the subset of permissions that
           require admin consent). this enables tenant
           admins to consent on behalf of all their users.

 -BÂºADMIN CONSENTÂº: required for certain high-privilege permissions.
   it ensures that admins have additional controls

BÂºresource/API BEST PRACTICESÂº
  - resources should follow the naming pattern
    subject.permission[.modifier], where:
    - subject : type of data available
    - permission: action that user may take upon that data
    - modifier: describe specializations of  permission
</pre>
<pre zoom labels="azure,AAA,OpenID,Identity">
<span xsmall>OpenID connect</span>
BÂºOAuth "vs" OpenID connectÂº
  - OAuth 2.0  : protocol to obtain OÂºaccess-tokensÂº to
                 protected resources.
  - OpenID con.: - built on top of OAuth to provide
                   in form of an OÂºid_tokenÂº verifying end-user identity
                   basic profile information.
                 - BÂºrecommended for web applicationd accessed via browserÂº

BÂºPRE-SETUPÂº:
  - register "application ID" within App-Tenant)
    portal â†’ top right account â†’ switch Directory
    â†’ choose Azure AD tenant (ignorefor single A.AD tenant users)
      â†’ Click app registrations â†’ new application:
        fill in data:
        - sign-on URL   â† Âºweb applicationsÂº    URL  where users can sign in
        - redirect URI  â† Âºnative applicationsÂº URL to return tokens to
                                                 ex: http://myfirstaadapp.
      â†’ Write down application id.

BÂºOpenID configurationÂº:
    - JSON required to perform sign-in is Located at:
      https://login.microsoftonline.com/Âº${tenant}Âº/.well-known/OpenID-configuration
    - The JSON includes:
      - URLs to use
      - location of the service's public signing keys.
      Ex:
      {
        "authorization_endpoint":
          "https://login.microsoftonline.com/common/OAuth2/authorize",
        "token_endpoint":
          "https://login.microsoftonline.com/common/OAuth2/token",
        "token_endpoint_auth_methods_supported": [
            "client_secret_post",
            "private_key_JWT",
            "client_secret_basic"
        ],
        "jwks_uri":
          "https://login.microsoftonline.com/common/discovery/keys"
        "userinfo_endpoint":
          "https://login.microsoftonline.com/{tenant}/OpenID/userinfo",
        ...
      }

BÂºOpenID flowÂº:
  user     â†’ browser: sign-in page
  browser  â†’ browser: redirect to /authorize endpoint.
  browser  â†’      AD: GET https://login.microsoftonline.com/
                        ${tenant}/OAuth2/authorize
                        ?client_id=6731de76-14a6-49ae-97bc-6eba6914391e
                        â…‹response_type=id_token              â† alt 1. get only id_token
                        â…‹response_type=id_tokenoÂº+codeÂº      â† alt 2. get also access_token
                        â…‹resource=https%3a%2f%2fssrv.a.com%2fâ† targeted resource to get access to
                        â…‹redirect_uri=http%3a%2f%2flocalhost%3a12345
                        â…‹response_mode=form_post
                       Âºâ…‹scope=OpenIDÂº
                        â…‹state=12345
                       Âºâ…‹nonce=123423412Âº                    â† avoid replay attacks

  browser  â†    AD: (successful response example)
                    POST /myapp/ http/1.1
                    HOST: localhost
                    CONTENT-TYPE: application/x-www-form-URLencoded

                  OÂºid_token=eyj0exai...Âº
                    â…‹state=12345â…‹
                  OÂºâ…‹code=awabaâ…‹   â† alt 2: access_token also requested.
                  OÂºid_token=Âºyj0exaioi...
                              ^^^^^^^^^^^^
                          JWT (async signature). Can be validated in
                          browser, but ussually validation is left to
                          backend servers (vs web clients). Browser
                          will just keep in charge of keeping it safe.

 â”” once JWT signature validated -- backend servers will  need  to verify
   next claims in JWT/Tenant:
   - ensure user has signed up for the app.
   - ensure user has proper authorization/privileges
   - ensure certain strength of authentication has occurred
            â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
            (multi-factor,...)

 â”” once OÂºid-tokenÂº validated, GÂºapp user session can startÂº.
   - GÂºClaims in JWT OÂºid_tokenÂº will be used to retrieve user-infoÂº
     GÂºfor the appÂº

BÂºSign-out requestÂº
 - it is not sufficient to clear app's cookies.
   redirect the user to Âºend_session_endpointÂº (listed in JSON metadata)
   otherwise RÂºuser can reauthenticate to the appÂº without
   entering their credentials again, since it still have
   a valid single sign-on session with the Azure AD endpoint.

   GET https://.../OAuth2/logout?
       post_logout_redirect_uri=http%3a%2f%2flocalhost%2fmyapp%2f
   â””â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
  ÂºSingle sign-outÂº:
   AD clears the user's session from the browser. it will also send
   an http get request to the registered logouturl of all the
   applications that the user is currently signed in to.
   (and registered by this tenant).
   applications must respond to this request by clearing any
   session that identifies the user and returning a 200 response.

   to setup the logout url in app code:
   - set logouturl in
     â†’ a.portal â†’ select AD for account
       â†’ choose app registrations
         â†’ choose app â†’ settings â†’ properties â†’ logout URL text box.
</pre>
<pre zoom labels="azure,AAA,OAuth">
<span xsmall>OAuth.Grant</span>
- Provides the Access in "AAA"
RÂºWARNÂº: Longest list of security concerns in the OAuth2 specification
- approach implemented by ADAL JS
- recommend for SPA applications.

- authorization grant that uses BÂºtwo separate endpointsÂº:.
  (no need to authenticate the -JS SPA- client )
  -BÂºauthorization URLÂº: used for user-interaction phase.
                         generates GÂºauthorization codeÂº.
  -BÂºtoken         URLÂº: used by JS SPA client to exchange
                       GÂºauthorization codeÂº by OÂºaccess tokenÂº
                                                OÂºid     tokenÂº (in OpenID)
                       - web apps are required to present their
                       GÂºown app. credentialsÂº to allow
                         authorization server authenticate the
                         JS SPA client.

     ^^^^^^^^^^^^^^^^^
   BÂºcross origin calls are eliminatedÂº

- in original OAuth2 specification, tokens are returned in a URI
  fragment making them available to JS code, also warrants that they
  will not be included in redirects toward the server.

- BÂº:OAuth2 implicit grant never returnÂº GÂºrefresh tokensÂº
  to the client mostly for security reasons.
  refresh token less narrowly scoped granting more privileges.
  hence inflicting far more damage in

- app will use a hidden iframe to perform new token requests
  against the A.AD authorization endpoint.
  - TheÂºartifact that makes the silent renewal possible,Âº
    theÂºAzure AD session cookie,Âº is managed outside of
    the application.
  - Signing out from Azure AD autoamtically disable renew-process.
  - the absence of the a.AD session cookie in native clients
    discourages this method.
<hr/>
<span xsmall>Authorization Code Grant</span>
- OAuth spec  section 4.1
- perform authentication and authorization in most application types.
  (web/native apps,...)
BÂºPRE-SETUP:Âº
  - register your app with Azure AD.
  - get OAuth 2.0 authorization endpoint for your Azure AD tenant
    tenant by selecting app registrations â†’ endpoints

BÂºauthorization flowÂº
  user â† client: redirect to the /authorize
                 - permissions needed from user
  client   â†’ AD: https://login.microsoftonline.com/{tenant}/OAuth2/authorize?
                 client_id=6731de76-14a6-49ae-97bc-6eba6914391e
                Âºâ…‹response_type=codeÂº
                 â…‹redirect_uri=http%3a%2f%2flocalhost%3a12345 â† (opt) authen resp
                                                     must match registered
                                                     URLenc(redirect_uris)
                                                     urn:ietf:wg:OAuth:2.0:oob
                                                     for native/mob apps
                Âºâ…‹response_mode=queryÂº         â† (opt)query*|fragment|form_post
                 â…‹state=12345
               GÂºâ…‹resource=https%3a%2f%2fsrv.a.com%2fÂº â† target web API
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           portal â†’ Azure AD â†’ application registrations
                           â†’ application's settings page â†’ properties.
                 â…‹prompt=login (opt) user interaction type :=
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â””â†’â”œâ”€         login: user should be prompted to reauthenticate.
              â”œâ”€select_account: user prompted to select an account,
              â”‚                 interrupting single sign on.
              â”‚                 â”€ user may select existing signedâ”€in account,
              â”‚                   enter credentials for a remembered account  or
              â”‚                 â”€ use a different account altogether.
              â”œâ”€      consent: user consent has been granted, but needs to
              â”‚                 be updated.  user should be prompted to consent.
              â””â”€ admin_consent: dmin should be prompted to consent on
                                behalf of all users in their org
                 â…‹login_hint=... (opt) pre-fill the email ...
                â…‹domain_hint=... (opt) tenant hint
                                 if federated to on-premises AD,
                                 aad redirects to specified
                                 tenant federation server.
      â…‹code_challenge_method=... (recommened)  method used to encode
                                 code_verifier for code_challenge
                                 parameter.
                                 := plain | s256.
            â…‹code_challenge=... (recommened)
                                - secure authorization code
                                  grants via proof key for code exchange
                                  (pkce) from a native or public client.
                                - required if code_challenge_method on
  user_cli â† AD: form
  user_cli â†’ AD: form response
  user_cli â† AD: get  http/1.1 302 found
                 location: http://localhost:12345/?
                OÂºcode=kaplreqdfsbzjq...Âº    â† app request OÂºaccess tokenÂº with it
                 â…‹session_state=7b29111d-..  â† opaque guid
                 â…‹state=12345                â† avoids cross-site request forgery
                                               (csrf) attacks against the client.

  user_cli â†’ AD: post /{tenant}/OAuth2/token http/1.1
                 host: https://login.microsoftonline.com
                 content-type: application/x-www-form-URLencoded
               OÂºgrant_type=authorization_codeÂº
                Âºâ…‹client_id=...Âº
               OÂºâ…‹code=....Âº
                 â…‹redirect_uri=https%3a%2f%2flocalhost%3a12345
                 â…‹resource=https%3a%2f%2fservice.contoso.com%2f
                 â…‹client_secret=p@ssw0rd

  user_cli â† AD: {
                GÂº"access_token": "eyj0..."Âº, â† JWT used to authenticate
                  "token_type": "bearer",       to resource
                  "expires_in": "3600",
                  "expires_on": "1388444763",
                  "resource"  : "https://service.contoso.com/",
                  "refresh_token": "...",
                  "scope"     : "https%3a%2f%2fgraph.microsoft.com%2fmail.read",
                  "id_token": "..."
                 }
  user_cli â†’ call resource
             resource: get /data http/1.1
             host: service.contoso.com
           GÂºauthorization: bearer ${JWT}Âº

  user_cli â† resource: http/1.1 200
                      www-authenticate: bearer
                      authorization_uri=
                        "https://login.microsoftonline.com/.../OAuth2/authorize",
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         client must validate this in response
                    - resource_id="unique identifier of the resource."
                      user_cli can use this identifier as the value of
                      the resource parameter when it requests a token
                      for the resource.
                      - recommended strategy to prevent attack:
                        verify resource_id startswith web API URL being accessed.
                      - client application must reject a resource_id
                        not  beginin with base URL


  user_cli â†’ AD: refresh token
                 post /{tenant}/OAuth2/token http/1.1
                host: https://login.microsoftonline.com
                content-type: application/x-www-form-URLencoded

                client_id=6731de76-14a6-49ae-97bc-6eba6914391e
                â…‹refresh_token=...
                â…‹grant_type=refresh_token
                â…‹resource=https%3a%2f%2fservice.contoso.com%2f
                â…‹client_secret=jqqx2pno9bpm0ueihupzyrh

  user_cli â† AD: {
                   "token_type": "bearer",
                   "expires_in": "3600",
                   "expires_on": "1460404526", â† expiration timestamp
                   "resource"  : "https://service.contoso.com/",
                 GÂº"access_token": "...",Âº
                   "refresh_token": "..."
                 }
  note: refresh tokens are valid for all resources that your client has
        already been given consent to access - thus, a refresh token issued
        on a request for resource=https://graph.microsoft.com can be
        used to request a new access token for resource=https://contoso.com/API.
        - in occassion refresh tokens expire/revoked/lack privileges.
          app must handle such errors by restarting autho.flow.

<hr/>
<span xsmall>client credentials grant</span>
- OAuth 2 spec. Section 4.4
- client credentials grant flow permits a web service
  (confidential client) to use its own credentials instead of
  impersonating a user, to authenticate when calling another web
  service.  (machine-to-machine)
- Azure AD also allows the calling service to use a
  certificate (instead of a shared secret) as a credential.

BÂºPRE-SETUPÂº
  - both client app and targeted resource must be registered to Azure AD.

BÂºFlow diagramÂº
  service â†’ Azure AD : authenticate
                         +request token
                          https://login.microsoftonline.com/
                          ${tenant}/OAuth2/token
  service â† Azure AD : access token
  service â†’ resource : request ( access token)
                        post /contoso.com/OAuth2/token http/1.1
                        host: login.microsoftonline.com
                        content-type: application/x-www-form-URLencoded

                        - alt 1 : shared secret
                          ?grant_type=client_credentials
                          â…‹client_id=...
                          â…‹client_secret=..  (shared_secre)
                          â…‹resource=app_id_uri targeted service

                        - alt 2 : certificate
                          ?grant_type=client_credentials
                          â…‹client_id=...
                          â…‹resource=app_id_uri targeted service
                          â…‹client_assertion_type=
                           urn:ietf:params:OAuth:client-assertion-type:JWT-bearer
                          client_assertion=      JSON web token app needs to create
                                                 and sign with registered cert

  service â† resource : {
                         "access_token":"...",
                         "token_type":"bearer",
                         "expires_in":"3599",
                         "expires_on":"1388452167",
                         "resource":"https://service.contoso.com/"
                       }
</pre>

<pre zoom labels="azure,AAA">
<span xsmall>VM/... managed identities</span>
- Azure AD feature
- GÂºfree with Azure AD for Azure subscriptionsÂº
  GÂºno additional costÂº
- formerly known as managed service identity (msi)

-ÂºproblemÂº: how to manage credentials in your code for authenticating
  to cloud services and keep credentials secure.
  -  Azure key vault provides a way to securely store
     credentials, secrets, and other keys,
   RÂºbut your code has to authenticate to key vault to retrieve themÂº

-Âºmanaged identities solutionÂº:
  app use the (AD) manage identity to authenticate to any service
  that supports Azure AD authentication, including key vault,
  without any credentials in your code.

BÂºterminologyÂº
-Âºclient idÂº   : UID generated by Azure AD, tied to an application
                 and service principal during its initial provisioning.

-Âºprincipal idÂº: Object ID of service principal object for
                 your managed identity used to grant RBAC
                 to an Azure targeted resources.

BÂºAzure instance metadata service (IMDS)Âº:
  accessible to all IaaS VMs created via ARM at local URL:
  http://169.254.169.254/...

  - Types of GÂºmanaged-identitiesÂº:
    -GÂºsystem-assignedÂº: enabled directly on an VM instance.
      an identity for the instance is created in Azure AD.
      as well as credentials with lifecycle == VM lifecycle

    -GÂºuser-assignedÂº:
      - created as standalone resource with associated
        AD identity in a (trusted-by-subscription) tenant .
      - The identity can be re-assigned to 1+VMs/... instances.

    - app code can use the managed identity to request (OAuth)
      access tokens for services supporting Azure AD authentication.

     - RBAC in Azure AD is used to assign the role to the
       VM service principal.
       "Key-vault" grant-access must be set.

  BÂºAzure VM system-assigned flowÂº
    (ARM = Azure Resource Manager)
    admin â†’ ARM: enable system-assigned GÂºmanaged-identityÂº on a VM.
    ARM   â†’ ARM: create VM service-principal in (trusted-by-subscription)
                 AD.
    ARM   â†’ VM : - updates Azure instance metadata service identity endpoint
                  with the service principal client id and certificate.
                - provisions VM extension (planned for deprecation
                  january 2019), and service principal client ID/Cert.
                  (planned for deprecation.)
    --- requesting tokens --
    app@VM â†’ VM: http://169.254.169.254/metadata/identity/OAuth2/token
                 ?API-version=2018-02-01 (or greater)     â† IMDS version
                 â…‹resource=https://management.Azure.com/
                                   â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€
                                      target resource (ARM)
                 optional params:
                 â…‹object_id   object_id of managed identity the token is for.
                              required, if VM has multiple identities
                 â…‹client_id   client_id of managed identity token is for.
                              required, if VM has multiple identities
                 http/1.1 metadata: true   â† required as mitigation against
                                             server side request forgery
                                             (ssrf) attack.


    app@VM â† VM: JSON with OÂºJWT access tokenÂº
                 â† http/1.1 200 ok
                 â† content-type: application/JSON
                 â† {
                 â† GÂº"access_token": "eyj0exai...",Âº
                 â†   "refresh_token": "",
                 â†   "expires_in": "3599",
                 â†   "expires_on": "1506484173",
                 â†   "not_before": "1506480273",
                 â†   "resource": "https://management.Azure.com/",
                 â†   "token_type": "bearer"
                 â† }

  BÂºUser-assigned flowÂº
    admin â†’ ARM : request create a user-assigned managed identity.
    ARM   â†’ AD  : create service principal for user-assigned
                GÂºmanaged identityÂº.
    AD    â†’ AD  : update A.VM instance metadata service
                  identity endpoint with user-assigned
                  managed identityÂºservice principal client idÂº
                  and ÂºcertificateÂº.
    ARM   â† AD  : request to configure the user-assigned
                  managed identity on a VM
    ARM   â†’ VM  : provisions VM extension
    ARM   â†’ VM  : (planned for deprecation)
                  add user-assigned managed identity service
                  principal client id and certificate.

BÂºenable system-assigned managed identityÂº
  â”” PRE-REQUISITES:
    admin account needs theÂºVM contributorÂºrole set.
    (no other role is needed)
  $ az login
  $ az group create  \               â† create a resource group
    --name group01 --location westus
  $ az VM create \                   alt1: assign at VM creation time
    --resource-group group01
    --name myvm
    --image win2016datacenter
    --generate-ssh-keys
  OÂº--assign-identityÂº
    --admin-username Azureuser
    --admin-password mypassword12

  $ az VM identity assign \          â† alt2:  assign for existing VM
    -g group01 -n myvm

  $ az VM update \                  â† de-assign option 1:
    -n myvm                           no longer needs system-assigned ID.
    -g group01                        butÂºstill needs user-assigned IDsÂº
    --set \                           use the following command:
    identity.type='userassigned' â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


  $ az VM update                    â† de-assign option 2:
    -n myvm
    -g group01 \                      no longer needs system-assigned id.
    --set identity.type="none"      â† it has no user-assigned identities.

  $ az VM identity \                â† remove managed id VM extension
    --resource-group group01 \        (planned for deprecation)
    --VM-name myvm \
    -n managedidentityextensionforwindows
       managedidentityextensionforlinux

BÂºenable user-assigned managed identityÂº
  â”” PRE-REQUISITES:
    admin account needs theÂºVM contributorÂºrole set.
    (no other role is needed)

  $ az identity create -g group01 -n myuserassignedidentity
    {
      "clientid": "73444643..",
      "clientsecreturl": "https://control-westcentralus.identity.../subscriptions/
                          $ubscripton id/resourcegroups/$resource group/providers/
                          microsoft.managedidentity/userassignedidentities/
                          $myuserassignedidentity/credentials
                         ?tid=5678
                         â…‹oid=9012
                         â…‹aid=73444643-...",
    GÂº"id": "/subscriptions/$subscripton_id/resourcegroups/Âº â† resource id
    GÂº        $resource_group/providers/microsoft.managedidentity/ Âº
    GÂº        userassignedidentities/$user assigned identity name",Âº
      "location": "westcentralus",
      "name": "$user assigned identity name",
    GÂº"principalid": "e5fdfdc1-ed84-4d48-8551-fe9fb9dedfll",Âº
      "resourcegroup": "$resource group",
      "tags": {},
      "tenantid": "733a8f0e-...",
      "type": "microsoft.managedidentity/userassignedidentities"
    }

BÂºtoken cachingÂº
  ""we recommend to implement token caching in your code.
    prepare for scenarios where the resource indicates that
    the token is expired.""

BÂºEx. allow VM access to a storage-accountÂº
  $ az login

  $ SPID=$(az resource list \
              -n myvmOrScaleSet01 \
              --query [*].identity.principalid \
              --out tsv)
    â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    get service principal for VM (or scaleSet) named 'myvmOrScaleSet01'

  $ az role assignment create \      â† Grant reader role
    --assignee $SPID                   in storageAccount in Resource Group
    --role 'reader'
    --scope /subscriptions/$subid/resourcegroups/$group
            /providers/microsoft.storage/storageaccounts/${account01}Âº
</pre>

<pre zoom labels="azure,AAA">
<span xsmall>certificate, form/token authentication</span>
BÂºCERTIFICATE-BASED AUTHENTICATIONÂº
  - in the context of microsoft Azure, certificate-based
    authentication enables to be authenticated by Azure AD
    with a client certificate on a windows/mobile device
    when connecting to different services, including
    (but not limited to):
    - Custom services authored by your organization
    - Microsoft Sharepoint online
    - Microsoft Office 365 (or Microsoft exchange)
    - Skype for business
    - Azure API management
    - Third-party services deployed in your organization

  Useful  where an organization has multiple front-end apps
  communicating with back-end services.
  Traditionally, the certificates are installed on each server,
  and the machines trust each other after validating
  certificates.
  For example, mutual authentication can be enabled to
  restrict client-access.

BÂºForms-based authenticationÂº
  - It isRÂºNOTÂº an internet standard.
  - appropriate only for web APIs that called from a web app
    so that the user can interact with the html form.
  - Disadvantages:
    - it requires a browser client to use the html form.
    - it requires measures to prevent cross-site request forgery (csrf).
    - user credentials are sent in plaintext as part of an http request.
      (Citation needed).

BÂºWindows-based authenticationÂº
  - integrated windows authentication enables users to log in with their
    windows credentials usingÂºKerberos or NTLMÂº.
    client sends credentials in the authorization header.
    windows authentication is best suited for an intranet environment.
  -RÂºdisadvantagesÂº:
    - difficult to use in internet applications without exposing
      the entire user directory.
    - it canâ€™t be used in bring-your-own-device (byod) scenarios.
    - it requires kerberos or integrated windows authentication (NTLM)
      support in the client browser or device.
    - the client must be joined to the active directory domain.

BÂºclaims-based authentication in .NETÂº
BÂºâ”” asp.NET identity
    - unified identity platform for asp.NET
    - target enviroments:  web, phone, store, or hybrid applications.
    - core features to match token-based authentication:
      - implements a provider model for logins.
        (today local AD, tomorrow Azure AD, social networks)
      - support for Âºclaims-based authenticationÂº
     â˜QÂºclaims allow developers to be a lot more expressive in Âº
      QÂºdescribing a user's identity than roles allowÂº.
      QÂºwhereas role membership is just a boolean Âº
      QÂºvalue (member or non-member), a claim can include rich informationÂº
      QÂºabout the user's identity and membership. most social providersÂº
      QÂºreturn metadata about the logged-in user as a series of claims.Âº

BÂºâ”” app service authentication and authorizationÂº
    - built-in authentication and authorization
    - sign-in users and access data byÂºwriting minimal/no code in appÂº
    - authentication+authorization module runs in
     Âºsame sandbox as your application codeÂº.
    -Âºwhen enabled, every incoming http request passes through itÂº
      before being handled by your application code.

    - all authn/authz logic, including cryptography for token validation
      and session management,Âºexecutes in the worker sandbox and outside ofÂº
     Âºweb app codeÂº.
      - module is configured using app settings.
      - no SDKs/code-changes required.

   - identity information flows directly into the application code.
     - for all language frameworks, app service makes the user's
       claims available to your code by injecting them into the
       request headers.
       -Âº.NETÂºapplications:
         -ÂºclaimsPrincipal.currentÂºis populated with authenticated
           user's claims following the standard .NET code pattern.
           including the [authorize] attribute.
       -ÂºphpÂº:
         - _server[â€˜remote_userâ€™] is populated.

BÂºâ”” built-in token storeÂº
    - repository of tokens associated with app-users, APIs.
      automatic refresh.
<hr/>
<span xsmall> implement multi-factor authentication </span>
  BÂºin the world of security, it is often recommended to have         Âº
  BÂºtwo of the following factors:                                     Âº
  BÂº   knowledge â€“ something that only the user knows (security       Âº
  BÂº               questions, password, or pin).                      Âº
  BÂº   possession â€“ something that only the user has (corporate badge,Âº
  BÂº               mobile device, or security token).                 Âº
  BÂº   inherence â€“ something that only the user is (fingerprint, face,Âº
  BÂº               voice, or iris).                                   Âº

  Azure multi-factor authentication (MFA) built in to Azure AD.

  administrators can configure approved authentication methods.

  - two ways to enable MFA:
    -Âºenable each user for MFAÂº. users will perform two-step verification
      each time they sign in (exceptions: trusted ip addresses,
      remembered devices feature is turned on).

    - set up aÂºconditional access policyÂºthat requires two-step
      verification under certain conditions.
      it uses the Azure AD identity protection risk policy to
      require two-step verification based only on the sign-in risk
      for all cloud applications.

  -OÂºMS authenticator appÂº(available in Android/iOS public Stores)
     â”” can be used either as:
       - second verification method
       - replacement for a password when using phone sign-in.
     â”” for verification in MFA it supports:
       - verification code
       - notification methods

  -Âºmulti-factor authentication SDKÂº:
    - Allows to build two-step verification directly into
      the sign-in or transaction processes of applications
      in your Azure AD tenant.
    - Available for c#, visual basic (.NET), java, perl,
      php, and ruby.
</pre>
</div>

<div groupv>
<span title>Access Control</span><br/>
<pre zoom labels="azure,AAA">
<span xsmall>Claims-based authorization </span>
- claim: (String)name/(String)value pair that represents what the
         subject is and not what the subject can do.

BÂºclaims-based authorizationÂº
  â”” approach:
   Âºgrant/deny authorization  decision  based on Âº
   Âºarbitrary logic that uses data available in claims as input.Âº
    at its simplest, checks the value of a claim and
    decide based on that value.
  â”” claim-basedÂºauthorization checks are declarativeÂº
    (embedded in controller|controller-action code)

  â”” claims maps to policies in asp.NET. Example

    .../Âºstartup.csÂº:
    public void
    configureServices(iServiceCollection services)
    {
      services.addMVC();
      services.addAuthorization(
        options =Ëƒ {
           options.addPolicy                  â† 1) Add policy
             (GÂº"withIDOnly"Âº, policy =Ëƒ
               policy.requireClaim("ID")); // â† 2) map claim to policy
            // policy.requireClaim("ID", "1", "2")); // only 1/2 values for claim
      });
    }

    [authorize(policy = GÂº"withIDOnly"Âº)]   // â† 3) Apply policy to all class
    public class
    vacationcontroller : controller {
      public ActionResult
      vacationbalance()  { ... }

      [allowanonymous]                      // â† 4) For this method
      public ActionResult                           allow anyone in
      vacationpolicy() { ...  }
    }
</pre>
<pre zoom labels="AAA,ARM,RBAC,azure">
<span xsmall>RBAC authorization </span>
BÂºOverviewÂº
  - (RBAC) helps you manage who has access to ÂºAzure resourcesÂº
    (versus controller code), what can be done, and what areas
    they have access to.
  - Âºbuilt on ARMÂº (A.Resource Manager)
  - â˜ best practice:
    GÂºgrant users the least privilege to get work doneÂº
  - control access:
    role assignments:
      â”‚security â”‚ N â†Â·Â·Â·Â·Â·â”‚   role   â”‚Â·Â·Â·Â·â†’ M â”‚scopeâ”‚
      â”‚principalâ”‚         â”‚definitionâ”‚
             ^                ^                  ^
    user, group,        - collection          Ex.: resourceGroup01
    service principal,    of permissions ...  scope level:
    managed identity                          - management group
    requesting access                         - subscription
    to Azure resources.                       - resource group
                                              - resource

  -BÂºbuilt-in rolesÂº
     -BÂºownerÂº        : full access to all resources (+delegation)
     -BÂºcontributorÂº  : can create/manage all types of Azure resources
     -                  but Âºcan't grantÂº access to others.
     -BÂºreaderÂº       : can view existing Azure resources.
     -BÂºuser accessÂº  : manage user access to a.resources.
     -BÂºadministratorÂº:
    (other built-in roles exists to manage specific A resources,
     ex: VM contributor, ...)

  -BÂºDeny assignmentsÂº
     RBAC supports deny assignments in a limited way.
    Âºcurrently, deny assignments are read-onlyÂºand
     can only be set by Azure.

  -BÂºRBAC steps to deny/grant accessÂº:
     - User (service principal) request a token to
       A.AD requesting access to ARM. Returned token
       includes the user's group memberships
       (and transitive group dependencies).
     - User calls ARM REST API with the token attached.
     - ARM retrieves all role/deny assignments applying to
       the resource upon which the action is being taken.
     - ARM determines what roles the user has for
       this resource.
     - ARM checks if requested REST API action is
       included in the user RBAC list for this resource.
       - If check fails access is not granted. (END)
     - ARM checks if a deny assignment applies.
       - If check applies access is not granted. (END)
     - At this point access is granted.

  -BÂºAD administrator rolesÂº:
   -Âºclassic subscription admin rolesÂº
    Âºaccount admin, service admin, and co-adminÂº
     - full access to the Azure subscription managing resources
       using the portal, resource manager APIs, and the classic
       deployment model APIs.
     -ÂºAzure sign up accountÂº: automatically set as Âº
       Âºaccount + service adminÂº
        == user with  'owner' role @ scope:subscription
   -ÂºAzure RBAC rolesÂº
   -ÂºAzure ADÂºadministrator roles

   - classic subscription admin
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚             â”‚ limit       â”‚ permissions
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚account      â”‚ 1 per Azure â”‚ access the Azure account center
     â”‚admin        â”‚ account     â”‚ manage all subscriptions in an account
     â”‚"billing act"â”‚             â”‚ create new subscriptions
     â”‚             â”‚             â”‚ cancel subscriptions
     â”‚             â”‚             â”‚ change the billing for a subscription
     â”‚             â”‚             â”‚ change the service administrator
     â”‚             â”‚             â”‚ subscription owner
     â”‚             â”‚             RÂºno access to the Azure portalÂº
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚service      â”‚ 1 per Azure â”‚ manage services in the portal
     â”‚admin        â”‚ subscriptionâ”‚ assign users to coâ”€admin role
     â”‚             â”‚             BÂºfull access to portalÂº
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
     â”‚coâ”€admin     â”‚ 200 per     â”‚ same as service admin, but can't
     â”‚             â”‚ subscriptionâ”‚Âºchange the association of         Âº
     â”‚             â”‚             â”‚Âºsubscriptions to Azure directoriesÂº
     â”‚             â”‚             â”‚ assign users to coâ”€administrator role,
     â”‚             â”‚             â”‚ butrÂºcannot change service adminÂº
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

   - Azure AD adminÂºrolesÂºcontrol permissions to manage
     Azure AD resources.
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚ Azure RBAC roles                   â”‚ Azure AD administrator roles  â”‚
     â”‚ manage access to AzureÂºresourcesÂº  â”‚ manage access to Azure        â”‚
     â”‚                                    â”‚ÂºAD resourcesÂº                 â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ supports custom roles              â”‚ cannot create custom roles    â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ÂºscopeÂºcan be specified atÂºmultipleÂºâ”‚ scope is at therÂºtenant levelÂºâ”‚
     â”‚ÂºlevelsÂº(management group,          â”‚                               â”‚
     â”‚ subscription, resource group,      â”‚                               â”‚
     â”‚ resource)                          â”‚                               â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ role info can be accessed in       â”‚ role info can not be accessed â”‚
     â”‚ portal, cli, powershell            â”‚                               â”‚
     â”‚ resource manager templates,        â”‚                               â”‚
     â”‚ REST API                           â”‚                               â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  -BÂºREST API: list RBAC role assignementsÂº:
     - PRE-SETUP: roles needed:
      Âºmicrosoft.authorization/roleassignments/readÂº  permission role
       (at the specified scope to Âºcall the APIÂº)
     GET
     https://management.Azure.com
     /{scope}/providers/microsoft.authorization
       ^     /roleassignments?API-version=2015-07-01â…‹$filter={filter}
       |
   - subscriptions/{subscriptionid}
   - subscriptions/{subscriptionid}/resourcegroups/myresourcegroup1
   - subscriptions/{subscriptionid}/resourcegroups/myresourcegroup1/
                   providers/microsoft.web/sites/mysite1

   - {filter}: (optional ) condition to apply to role assignment list:
      $filter=atscope()                       do not including subscopes.
      $filter=principalid%20eq%20'{objectid}' list role assignament
                                              for  user/group/service principal.
      $filter=assignedto('{objectid}')        list role assign. for a
                                              specified user, including ones
                                              inherited from groups.
  -BÂºREST API: grant RBAC accessÂº
     PUT params:
     - security principal
     - role definition
     - scope.
     - pre-setup:
       - Âºmicrosoft.authorization/roleassignments/write operationÂº enabled.
         (at the specified scope to Âºcall the APIÂº)
       - Âºnew uuid for role assignmentÂº.
          00000000-0000-0000-0000-000000000000
          8           4    4    4           12

      note: use 'https://docs.microsoft.com/en-us/REST/API/
                  authorization/roledefinitions/list'
            list all the identifier for the role definition you want to assign.
      PUT
      https://management.Azure.com/{scope}/providers/microsoft.authorization
      /roleAssignments/${roleAssignmentName}?API-version=2015-07-01

      {
        "properties": {
          "roledefinitionid":
            "/subscriptions/{subscriptionid}/providers/microsoft.authorization
             /roledefinitions/{roledefinitionid}",
          "principalid": "{principalid}"
        }
      }

  -BÂºREST API: remove RBAC accessÂº
   DELETE

   https://management.Azure.com/
   {scope}/providers/microsoft.authorization
          /roleassignments/{roleassignmentname}?API-version=2015-07-01
<hr/>
<span xsmall>asp.NET RBAC</span>
- Declarative role-based authorization in controller.
- roles exposed through ÂºclaimsPrincipal::isInRole(..) methodÂº.

 Âº[authorize(roles = "hrmanager,finance")]Âº    // â† one of the roles
  public class
  salarycontroller :ÂºcontrollerÂº{ ...  }

 Âº[authorize(roles = "poweruser")]Âº       // â†â”¬â”€ all of the roles
 Âº[authorize(roles = "controlpaneluser")]Âº// â†â”˜
  public class
  controlpanelcontroller
  Âº: controllerÂº {
   Âº[authorize(roles = "administrator")]Âº // â† add required role for
    public actionresult shutdown()        //   method
    { ...  }

   Âº[allowanonymous]Âº                     // â† anonymous access
    public actionresult login()
    { ...  }
  }

- usingÂºrole-policy-syntaxÂº, developerÂºregisters a policyÂº
 Âºat startupÂºas part of the authorization service configuration.

  .../Âºstartup.csÂº
  public void
  configureServices(IServiceCollection services) {
    services.addMVC();
    services.addAuthorization(
      options =Ëƒ {Âº
        options
          .addPolicy("requireAdminRole",
            policy =Ëƒ policy.BÂºrequireRoleÂº("administrator"));
        options
          .addPolicy( "elevatedRights", p
            olicy =Ëƒ policy.BÂºrequireroleÂº("backup", "root"));
     Âº});Âº
  }

  .../*Controller.cs:
  ...
 Âº[authorize(policy = "requireAdminRole")]Âº
  public iActionResult shutdown() { ...  }

â˜ claims-based authorization and role-based authorization
  can be put together.
  is it typical to see the role defined as a special claim.
  the role claim type is expressed using the following
  URI:Âºhttp://schemas.microsoft.com/ws/2008/06/identity/claims/roleÂº
</pre>

<span title>Secure Data</span><br/>
<pre zoom labels="AAA,azure">
<span xsmall>Encryption options</span>
BÂºEncryption at REST:Âº
  - encoding (encryption) of dataÂºwhen it is persisted.
  - attacks against data at REST include attempts to obtain
    physical access to the hardware on which the data is stored and to
    then compromise the contained data.

  - also required for data governance and compliance efforts.
    industry and government regulations, such as the
    health insurance portability and accountability act (hipaa),
    pci dss, and federal risk and authorization management program
    (fedramp), lay out specific safeguards regarding data protection
    and encryption requirements.
  - Azure use symmetric encryption.
    - data may be partitioned, and different keys may be used
      for each partition.
    - keys must be stored in a security-enhanced location with access
      control policies limiting access to certain identities and logging
      key usage.
      data encryption keys are often encrypted with asymmetric
      encryption to further limit access.

  - Azure storage encryption
    - all Azure storage services (BLOB storage, queue storage, table
      storage, and Azure files) support server-side encryption at REST,
      with some services supporting customer-managed keys and client-side
      encryption.
    - all Azure storage services enable server-side encryption
      by default using service-managed keys, which is transparent to the
      application.

    - storage service encryption is enabled for all new and existing
      storage accounts andÂºcannot be disabledÂº. because your data is
      security enhanced by default, you don't need to modify your code or
      applications to take advantage of storage service encryption.

BÂºAzure SQL database encryptionÂº
  - support for microsoft-managed-encryption at REST for:
    - server-side provided throughBÂº"transparent data encryption"(TDE)Âº:
      - enabled by default at creation.
      - keys are automatically created and managed by default.
        RSA 2048-bit customer-managed keys in Azure key-vault supported.
      - it can be enabled at levels:
        - database
        - server
    - client-side

BÂºAzure Cosmos DB encryptionÂº
- Cosmos DB automatically encrypts all databases,
  media attachments and backups.
<hr/>
<span xsmall>end-to-end encryption</span>
BÂºTransparent Data Encryption(TDE)Âº encrypts SQL server, A.SQL, and
A SQL data warehouse data files.
BÂºTDEÂº performs real-time I/O en/de-cryption of data and log files.
Encryption of the database file is performed at the page level.
- TDE protect sensitive data (Credit Cards,...):
  - at REST on the server
  - during movement between client and server
  - while the data is in use

- encryption uses a DDBB Âºencryption keyÂº(DEK), stored
  in the database boot record for availability during recovery.
 ÂºDEKÂºis either:
  -Âº symmetric keyÂºsecured with a certificate stored in
    the master database of the server with
    AES/3DES encryption algorithms supported
  -ÂºAsymmetric keyÂºprotected by an
    extensible-key-management (EKM) module.

- It also Âºallows clientsÂº to encrypt data inside
  client apps without revealing encryption-keys
  to the DDBB engine.

-OÂºIt  helps  to ensure that on-premises database administrators,Âº
 OÂºcloud database operators, or other highly privileged butÂº
 OÂºunauthorized users cannot access the encrypted data.Âº
 OÂºallowing to delegate DDBB administrationÂº
 OÂºto third parties and reduce security clearanceÂº
 OÂºrequirements for database administrators.Âº

 â˜it requires a specialized driver installed on
  the client computer to automatically encrypt and decrypt sensitive
  data in the client application.
  - For many applications, this does require some code changes.
    this is in contrast to TDE, which only requires a change to
    the applicationâ€™s connection string.
<hr/>
<span xsmall>A.Confidential computing</span>
- set of features available in many Azure services that encrypt
 Âºdata in useÂº
- designed for scenarios where data is processed in the cloud
  while still protecting it from being viewed in plaintext.
- collaborative project between hardware vendors like intel and
  software vendors like microsoft.
- it ensures that when data is "in the clear," it is
  protected inside a OÂºtrusted execution environment (TEE)Âº.
  TEEs ensures that there is no way to view data or
  operations inside from the outside,Âºeven with a debuggerÂº
  and thatÂºonly authorized code is permitted to access dataÂº
  if the code is altered or tampered with, operations are
  denied and the environment disabled.

 â˜ note: TEEs are commonly referred to asBÂºenclavesÂº.

- TEEs are exposed in multiple ways:
  -Âºhardware  Âº: Intel SGX technology
  -Âºsoftware  Âº: Intel SGX SDK and third-party enclave APIs
  -Âºservices  Âº: many Azure services, such as Azure SQL database,
                 already execute code in TEEs.
  -ÂºframeworksÂº: the microsoft research team has developer
                 frameworks, such as the confidential consortium
                 blockchain framework, to help jumpstart new
                 projects that need to run in TEEs.

<span xsmall>A.Key vault:</span>
Security-enhanced secrets store.
- vaults are backed by hardware security modules (hsms).
- vaults also control and log the access to anything stored
  in them.
- designed to support any type of secret:
  (passwords, credential, API keys, certificate,...).
- handling and renewing of and lifecycle management tls certificates.

 $ az keyvault create \             â† create new vault
   --name contosovault \
   --resource-group securitygroup \
   --location westus
   (write down the vault URI)

 $ az keyvault \                     â† add a secret
  Âºsecret setÂº\
   --vault-name contosovault \
   --name databasepassword
   --value 'pa5w.rd'

 $ az keyvault \                     â† view value.
  Âºsecret showÂº\
   --vault-name contosovault \
   --name databasepassword

<span TODO>Manage Certificates</span>
  $ az keyvault certificate delete --name
  $ az keyvault certificate purge  --name
</pre>
</div>
</div> <!-- AAA -->

<div group> <!-- Monitor,tune,troubleshoot -->
<span title>monitor, troubleshoot, and optimize</span>
<hr/>
<div groupv>
<span title>Azure monitor</span>
<pre zoom labels="azure,monitoring,troubleshoot,">
<span xsmall>summary</span>
ÂºAzure monitorÂº: single integrated experience for
   - log analytics          - Azure resources
   - application insights   - hybrid environments
                   â”Œâ”€ Azure monitor â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚              â”Œ â”Œâ”€insightsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                   â”‚              â”‚ â”‚-application     -VM        â”‚â”‚
                   â”‚              â”‚ â”‚-container       -monitoringâ”‚â”‚
                   â”‚              â”‚ â”‚                  solutions â”‚â”‚
                   â”‚              â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                   â”‚              â”‚ â”Œâ”€visualizeâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
    application â”  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚ -dashboard     -powerBI    â”‚â”‚
             OS â”¤  â”‚  â”‚ metricsâ”‚  â”‚ â”‚ -views         -workbooks  â”‚â”‚
                â”‚  â”‚  â”‚ DDBB   â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
    a.resources â”¤  â”‚  â”‚        â”‚  â”‚ â”Œâ”€analyzeâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                â”œâ”€â”€â”€â”€â†’â”¤        â”‚â”€â†’â”¤ â”‚ -metrics       -log        â”‚â”‚
 a.subscription â”¤  â”‚  â”‚        â”‚  â”‚ â”‚ -analytics     -analytics  â”‚â”‚
                â”‚  â”‚  â”‚ logs   â”‚  â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
       a.tenant â”¤  â”‚  â”‚        â”‚  â”‚ â”Œâ”€respondâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                â”‚  â”‚  â””â”€^â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚ -alerts        -autoscale  â”‚â”‚
 custom sources â”˜  â”‚    |         â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                   â”‚    |         â”‚ â”Œâ”€integrateâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
                   â”‚    |         â”‚ â”‚ -event -logics -ingestâ…‹    â”‚â”‚
                   â”‚    |         â”‚ â”‚ -hubs  -apps    export APIsâ”‚â”‚
                   â”‚    |         â”” â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
                   â””â”€â”€â”€â”€|â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     ÂºMETRICSÂº                       ÂºLOGSÂº:
    - numerical values describing   - different kinds of data organized into records
      some aspect of a system atÂºa    with different sets of properties for each type.
      particular point in time.Âº    - telemetry (events, traces,...) are stored
    - lightweight (near real-time     as logs in addition to performance data so that
      scenarios)                      it can all be combined for analysis.
                                    - stored in log analytics:
                                     Âºrich query languageÂºto quickly retrieve,
                                      consolidate, and analyze collected data.

- extend the resource-collected-data into the actual operation by:
  1) enabling diagnostics
  2) adding an agent to compute resources.
  this will collect telemetry for the internal operation of
  the resource and allow you to configure different data sources to
  collect logs and metrics from windows and linux guest OSes.

BÂºApplication InsightsÂº(web applications)       BÂºAzure monitor VM insightsÂº
  - addÂºinstrumentation packageÂº                 - analyze performance and health of Win/Linux VMs
  - monitors availability , performance, usage     including:
  - cloud orÂºon-premisesÂº                          - processes
  - integrates with visual studio                  - interconnected dependencies on other resources
                                                     and external processes.
                                                 - Cloud andÂºon-premisesÂº

BÂºAzure monitor for containersÂº                 BÂºMonitoring SolutionsÂº
  - automatically collected through               - packaged sets of logic that provide insights for
    (linux)log-analytics-agent.                     a particular application or service.
  - monitor the performance of managed-AKS        - available from Microsoft and partners.
    container workloads
  - collects:
    - memory/processor metrics from
      controllers+nodes+containers
    - container logs are also


BÂºAlertsÂº                                          BÂºAutoscaleÂº
  - alertÂºrules based on metricsÂº: near real time  - autoscale rules use Azure monitor metrics
    alerting based on: numeric values              - you specify a minimum/maximum number of instances
  - alertÂºrules based on logsÂº: complex logic        and the logic for when to increase or decrease resources.
    based on: data from multiple sources.
                                                   - data sources that can be organized into tiers:
  - alert rules useÂºaction groupsÂº:                  highest tiers: application and OSs
    - they contain unique sets of recipients and     lower   tiers: Azure platform components.
      actions that can be shared across multiple
      rules.
      ex actions:
      - using webhooks to start external actions
      - integrate with external itsm tools.

BÂºSOURCESÂº:
- Azure tenant  : Azure active directory like data.

- Azure platform: Azure subscription data, audit-logs from
                  Azure active directory.

- guest OS      : (need agent-install for on-premises machines)

- applications  : (application insights)

- custom sources: log dataÂºfrom any REST clientÂº
                  usingÂºdata collector APIÂº

- APM Service   : Application Performance Management (APM)
                 Âºfor web developersÂº.

BÂºLIVE MONITOR WEB APPSÂº
  - target users:Âºdevelopment teamÂº
  - request rates.
  - response times.
  - failure rates
  - popular pages
  - day peaks
  - dependency rates
  - exceptions: analyse aggregated statistics or
                specific instances
    - dump stack trace.
    - both serverÂºand browserÂºexceptions are reported.
  - load performance reported byÂºclient browsersÂº.
  - OS perf.counters (cpu, memory, network usage)
  - diagnostic trace logs from your app to
   Âºcorrelate trace events with requestsÂº
  - custom events from client/server Âºto track business eventsÂº
    such as items sold,...
</pre>

<pre zoom labels="azure,monitoring,troubleshoot,">
<span xsmall>Alerts in Azure</span>
Azure monitorÂºunifiedÂºalert: (vs classic alerts)
- it includesÂºLog analytics and application InsightsÂº

    â”ŒÂºAlert ruleÂº â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚â”€ Separated from alerts and actions                          â”‚
    â”‚  (target_resource,alerting criteria)                        â”‚
    â”‚â”€ Attributes:                                                â”‚
    â”‚  â”€ÂºstateÂº                : enabledâ”‚disabled                 â”‚
    â”‚                                                             â”‚
    â”‚  â”€Âºtarget resource signal: Observed VMs, container          â”‚
    â”‚                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                          â”‚
    â”‚  â”€Âºcriteria/Logic test   : â”Œâ”€â”€â”€ emitted signal â”€â”€â”€â”€â”        â”‚
    â”‚                            percentage cpu            Ëƒ 70%  â”‚
    â”‚                            server response time      Ëƒ 4 ms â”‚
    â”‚                            result count of log query Ëƒ 100  â”‚
    â”‚                            log search query          ...    â”‚
    â”‚                            activity log events       ...    â”‚
    â”‚                            A.platform health         ...    â”‚
    â”‚                            web site availability     ...    â”‚
    â”‚                                                             â”‚
    â”‚  â”€ alert name     : userâ”€configured                         â”‚
    â”‚  â”€ alert descript :                                         â”‚
    â”‚  â”€ severity       : 0 to 4                                  â”‚
    â”‚  â”€ action         : a specific action taken when the alert  â”‚
    â”‚                     is fired. (see action groups).          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”Œâ”€â”€â”€vâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€vâ”€â”€â”€â”€â”€â”
  â”‚ACTION GROUP   â”‚  â”‚ condition   â”‚
  â”‚               â”‚  â”‚ monitoring  â”‚
  â”‚- actions to doâ”‚  â”‚Âºalert stateÂºâ†- state changes stored in alert-history.
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

OÂºAlert States:Âº
- set and changed by the user (vsÂºmonitor conditionÂº, set and cleared by system)
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚state       â”‚description             â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚new         â”‚issue detected, not yet â”‚
  â”‚            â”‚reviewed                â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚acknowledgedâ”‚admin reviewed alert andâ”‚
  â”‚            â”‚started working on it   â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚closed      â”‚issue resolved. it can  â”‚
  â”‚            â”‚be reopen               â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

create an alert rule
 1) pick the target for the alert.
 2) select a signal from available ones for target.
 3) specify logic to be applied to data from the signal.
</pre>

<span title>Disaster Recovery</span>
<pre zoom labels="azure,troubleshoot,TODO">
<span xsmall>Site Recovery</span>
@[https://www.infoq.com/news/2019/01/azure-migrate-site-recovery]
Site Recovery service helps ensure business continuity
by keeping business apps and workloads running during outages. Site
Recovery replicates workloads running on physical and virtual
machines (VMs) from a primary site to a secondary location. When an
outage occurs at your primary site, you fail over to secondary
location, and access apps from there. After the primary location is
running again, you can fail back to it.


</pre>
<pre zoom labels="azure,troubleshoot,backups,TODO">
<span xsmall>Backup Serv.</span>
Backup service: The Azure Backup service keeps your data safe and
recoverable by backing it up to Azure.
</pre>

</div>
<div groupv>
<span title>apps and services scalability</span>
<pre zoom labels="azure,monitoring,troubleshoot,">
<span xsmall>common autoscale patterns</span>
BÂºmonitor autoscale currently applies only to virtualÂº
BÂºmachine scale sets, cloud services, app service -  Âº
BÂºweb apps, and API management services.             Âº


- autoscale settings can be set to be triggered based on:
  - metrics of load/performance.
  - scheduled date and time.

Âºautoscale setting schemaÂº
 -BÂº1 profileÂº
 -  2 metric rules in profile:
    - scale out (avg. cpu% Ëƒ 85% for past 10 minutes)
    - scale in  (avg. cpu% Ë‚ 60% for past 10 minutes)
   {
   | "id": "/subscriptions/s1/resourcegroups/rg1/providers/microsoft.insights/autoscalesettings/setting1",
   | "name": "setting1",
   | "type": "microsoft.insights/autoscalesettings",
   | "location": "east us",
   | "properties": {
   | Â· "enabled": true,
   | Â· "targetresourceuri": "/subscriptions/s1/resourcegroups/rg1/providers/microsoft.compute/virtualmachinescalesets/vmss1",
   | BÂº"profiles":Âº[
   | Â· | {
   | Â· | Â· "name": "mainprofile",
   | Â· | Â· "capacity": {
   | Â· | Â· Â· "minimum": "1",
   | Â· | Â· Â· "maximum": "4",
   | Â· | Â· Â· "default": "1"
   | Â· | Â· },
   | Â· | Â·Âº"rules":Âº[
   | Â· | Â· | {
   | Â· | Â· | Â·Âº"metrictrigger":Âº{
   | Â· | Â· | Â·   "metricname": "percentage cpu",
   | Â· | Â· | Â·   "metricresourceuri": "/subscriptions/s1/resourcegroups/rg1/providers/microsoft.compute/virtualmachinescalesets/vmss1",
   | Â· | Â· | Â·   "timegrain": "pt1m",
   | Â· | Â· | Â·   "statistic": "average",
   | Â· | Â· | Â·   "timewindow": "pt10m",
   | Â· | Â· | Â·   "timeaggregation": "average",
   | Â· | Â· | Â·   "operator": "greaterthan",
   | Â· | Â· | Â·   "threshold": 85
   | Â· | Â· | Â· },
   | Â· | Â· | Â· "scaleaction": {
   | Â· | Â· | Â·   "direction": "increase",
   | Â· | Â· | Â·   "type": "changecount",
   | Â· | Â· | Â·   "value": "1",
   | Â· | Â· | Â·   "cooldown": "pt5m"
   | Â· | Â· | Â· }
   | Â· | Â· | },
   | Â· | Â· | {
   | Â· | Â· | Â·Âº"metrictrigger":Âº{
   | Â· | Â· | Â·   "metricname": "percentage cpu",
   | Â· | Â· | Â·   "metricresourceuri": "/subscriptions/s1/resourcegroups/rg1/providers/microsoft.compute/virtualmachinescalesets/vmss1",
   | Â· | Â· | Â·   "timegrain": "pt1m",
   | Â· | Â· | Â·   "statistic": "average",
   | Â· | Â· | Â·   "timewindow": "pt10m",
   | Â· | Â· | Â·   "timeaggregation": "average",
   | Â· | Â· | Â·   "operator": "lessthan",
   | Â· | Â· | Â·   "threshold": 60
   | Â· | Â· | Â· },
   | Â· | Â· | Â· "scaleaction": {
   | Â· | Â· | Â·   "direction": "decrease",
   | Â· | Â· | Â·   "type": "changecount",
   | Â· | Â· | Â·   "value": "1",
   | Â· | Â· | Â·   "cooldown": "pt5m"
   | Â· | Â· | Â· }
   | Â· | Â· | }
   | Â· | Â· ]
   | Â· | }
   | Â· ]
   | }
   }

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ section      â”‚ element name     â”‚ description                                                         â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ setting      â”‚ id               â”‚                                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ setting      â”‚ name             â”‚                                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ setting      â”‚ location         â”‚ location can be different from the location of resource being scaledâ”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ properties   â”‚ targetresourceuriâ”‚ resource id  being scaled. 1 autoscale setting max per resource     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ properties   â”‚  profiles        â”‚ 1â”¼ profiles. autoscale engine runs/executes on one profile          â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ profile      â”‚ name             â”‚                                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ profile      â”‚ capacity.maximum â”‚ it ensures that when executing this profile, does not scale         â”‚
â”‚              â”‚                  â”‚ resource above number                                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ profile      â”‚ capacity.minimum â”‚ it ensures that when executing this profile, does not scale         â”‚
â”‚              â”‚                  â”‚ resource below number                                               â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ profile      â”‚ capacity.default â”‚ if there is a problem reading the resource metric  and current      â”‚
â”‚              â”‚                  â”‚ capacity is below the default, scales out to default.               â”‚
â”‚              â”‚                  â”‚ if current capacity is higherit does not scale in.                  â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ profile      â”‚  rules           â”‚ 1...n per profile                                                   â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ rule         â”‚ metrictrigger    â”‚                                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ metrictriggerâ”‚ metricname       â”‚                                                                     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ metrictriggerâ”‚metricresourceuri â”‚ resource id of resource that emits the metric. (probalby same that  â”‚
â”‚              â”‚                  â”‚ resource being scaled)                                              â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ metrictriggerâ”‚timegrain         â”‚ metric sampling duration. ex "pt1m" = aggregated every 1 minute     â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ metrictriggerâ”‚statistic         â”‚ aggregation method used in timegrain. ex: "average"                 â”‚
â”‚              â”‚                  â”‚ average|minimum|maximum|total                                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ metrictriggerâ”‚timewindow        â”‚ amountâ”€ofâ”€time to lookâ”€back for metrics.                            â”‚
â”‚              â”‚                  â”‚ ex: "pt10m" == "every time autoscale runs, query metrics for past   â”‚
â”‚              â”‚                  â”‚      10min"(it avoids reacting to transient spikes)                 â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ metrictriggerâ”‚ timeaggregation  â”‚ aggregation method used to aggregate the sampled metrics.           â”‚
â”‚              â”‚                  â”‚ "average" == aggregate sampled metrics taking the average.          â”‚
â”‚              â”‚                  â”‚ average|minimum|maximum|total                                       â”‚
â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ rule         â”‚ scaleaction      â”‚ action to take when (metrictrigger of) the rule is triggered.       â”‚
â”‚ scaleaction  â”‚ direction        â”‚ "increase"(scale out)â”‚"decrease"(scale in)                          â”‚
â”‚ scaleaction  â”‚ value            â”‚ how much to in/deâ”€crease resource capacity                          â”‚
â”‚ scaleaction  â”‚ cooldown         â”‚ timeâ”€toâ”€wait after a scale operation before scaling again.          â”‚
â”‚              â”‚                  â”‚ ex: "pt10m" := "do not attempt to scale again for another 10min".   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Âºautoscale profiles typesÂº

- regular profile: you donâ€™t need to scale your resource based on
                   date, day-of-week...
                   youÂºshould only have one regular profile definedÂº

- fixed date profile: ex:  important event coming up on
                   december 26, 2017 (pst).

- recurrence profile:  day of the week, ...
  - they only have a start time.  and run until the next
     recurrence profile or fixed date profile is set to start

    "profiles": [
    Â· { "name": " regularprofile",
    Â·   "capacity": { ...  }, "rules": [{ ...  }, { ...  }]
    Â· },
    Â· { "name": "eventprofile",
    Â·   "capacity": { ...  },
    Â·   "rules": [{ ...  }, { ...  }],
    Â·   "fixeddate": {
    Â·       "timezone": "pacific standard time",
    Â·          "start": "2017-12-26t00:00:00",
    Â·            "end": "2017-12-26t23:59:00"
    Â·   }
    Â· },
    Â· { "name": "weekdayprofile",
    Â·   "capacity": { ...  },
    Â·   "rules": [{ ...  }],
    Â·   "recurrence": {
    Â·       "frequency": "week",
    Â·       "schedule": { "timezone": "pacific standard time", "days": [ "monday" ], "hours": [0], "minutes": [0] }
    Â·   }
    Â· },
    Â· { "name": "weekendprofile",
    Â·   "capacity": { ...  },
    Â·   "rules": [{ ...  }]
    Â·   "recurrence": {
    Â·       "frequency": "week",
    Â·       "schedule": { "timezone": "pacific standard time", "days": [ "saturday" ], "hours": [0], "minutes": [0] }
    Â·   }
    Â· }
    ]

Âºautoscale profile evaluationÂº
1) looks for any fixed date profile that is configured to run now.
   if multiple fixed date profiles that are supposed to run,
   first one is selected.
2) looks at recurrence profiles.
3) runs regular profile.



â†’ Azure portal â†’ Azure monitor icon (left nav.pane)
  â†’ click "autoscale setting"
    â†’ open "autoscale blade"
      â†’ select a resource to scale
        â†’ click "enable autoscale"
          â†’ scale setting for new web app
            fill in "name" and click
            â†’ add a rule.
              â†’ change default metric source to "application insights"
                (web app with application insights has
                been configured previously)
                â†’ select the app insights resource in the dropdown
                  â†’ select custom metric based
                    â†’ ...

- a resource can have only one autoscale setting
- all autoscale failures are logged to the activity log.
  activity log alert can be configured to notify via
  email, sms, or webhooks whenever there is an autoscale success/failure.

Âºbest practicesÂº
- ensure the max/min values are different and with adequate margin
- manual scaling is reset by autoscale min and max

- let's look at an example of what can lead to a behavior that may seem
  confusing. consider the following sequence.
  assume there are two instances to begin with and then the
  average number of threads per instance grows to 625.
 - autoscale scales out adding a third instance.
 - next, assume that the average thread count across instance falls to 575.
 - before scaling down, autoscale tries to estimate what the final
   state will be if it scaled in. for example, 575 x 3 (current instance
   count) = 1,725 / 2 (final number of instances when scaled down) =
   862.5 threads. this means autoscale would have to immediately
   scale-out again even after it scaled in, if the average thread count
   remains the same or even falls only a small amount. however, if it
   scaled up again, the whole process would repeat, leading to an
   infinite loop.
 - to avoid this situation (termed "flapping"), autoscale does not
   scale down at all. instead, it skips and reevaluates the condition
   again the next time the service's job executes. this can confuse many
   people because autoscale wouldn't appear to work when the average
   thread count was 575.

 - estimation during a scale-in is intended to avoid â€œflappingâ€
   situations, where scale-in and scale-out actions continually go back
   and forth. keep this behavior in mind when you choose the same
   thresholds for scale-out and in.


- autoscale will post to the activity log if any of the following
  conditions occur:
  - autoscale issues a scale operation
  - autoscale service successfully completes a scale action
  - autoscale service fails to take a scale action.
  - metrics are not available for autoscale service to make a scale decision.
  - metrics are available (recovery) again to make a scale decision.
<hr/>
<span xsmall>singleton app instances patterns</span>
querying resources using Azure cli
$ az ...
  --query $jmespath
          ^^^^^^^^^
          JSON query language

jmespath queries areÂºexecuted on the JSON outputÂºbefore they perform
any other display formatting.

- but there's never an order guarantee from the Azure cli.
- to make multivalue array outpus easier to query,
  jmespathÂº[] operatorÂº can be used to flatten  output

  ex:
  $ az VM list
  â†’ [ ... hundreds of lines ...  ]

  $ az VM list --query \
    '[].{name:name, image:storageprofile.imagereference.offer}'
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             Âº{...} projection operatorÂº
  â†’ [
  â†’     { "image": "ubuntuserver", "name": "linuxvm" },
  â†’     { "image": "windowsserver", "name": "winvm" }
  â†’ ]

  $ az VM list --query
   "[?starts_with(storageprofile.imagereference.offer, 'windowsserver')]"
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 Âº[...] filter operatorÂº: filter result set
                        by comparing JSON properties values

  $ az VM list --query
    "[?starts_with(storageprofile.imagereference.offer, 'ubuntu')].{name:name, id:vmid}"
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  [..] filter and {..} projection combined
  â†’ [
  â†’     {
  â†’         "name": "linuxvm",
  â†’         "id": "6aed2e80-64b2-401b-a8a0-b82ac8a6ed5c"
  â†’     }
  â†’ ]

// step 1. create authenticated client
   Azure Azure01 = Azure.authenticate("Azure.auth").withdefaultsubscription();
                         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
            static method returning    authorization
                 an object that can    file, contains
           fluently query resources    info for
          and access their metadata.   subscription
                                       service-principal.
                                           |
    "Azure.auth" example:                  v
    | {
    |     "clientid": "b52dd125-9272-4b21-9862-0be667bdf6dc",
    |     "clientsecret": "ebc6e170-72b2-4b6f-9de2-99410964d2d0",
    |     "subscriptionid": "ffa52f27-be12-4cad-b1ea-c2c241b6cceb",
    |     "tenantid": "72f988bf-86f1-41af-91ab-2d7cd011db47",
    |     "activedirectoryendpointurl": "https://login.microsoftonline.com",
    |     "resourcemanagerendpointurl": "https://management.Azure.com/",
    |     "activedirectorygraphresourceid": "https://graph.windows.NET/",
    |     "sqlmanagementendpointurl": "https://management.core.windows.NET:8443/",
    |     "galleryendpointurl": "https://gallery.Azure.com/",
    |     "managementendpointurl": "https://management.core.windows.NET/"
    | }

      note: you can generate a (a)ctive (d)irectory (s)ervice (p)rincipal like:
      $ az AD sp create-for-RBAC --SDK-auth Ëƒ "Azure.auth"

step 2) use the API:
   var VMs = Azure01.virtualmachines;                // alt 1.  sync
   var VMs = await Azure01.virtualmachines.listasync();// alt 2. async

   foreach(var VM in VMs) {
       console.writeline(VM.name);
   }

   // filter out using linq:
   ivirtualmachine targetvm01 = VMs.where(
      VM =Ëƒ VM.name == "simple").singleordefault();
   console.writeline(targetvm?.id);

   inetworkinterface targetnic01 =
       targetvm01.getprimarynetworkinterface();
   inicipconfiguration targetipconfig01 = \
       targetnic01.primaryipconfiguration;
   ipublicipaddress ipaddr01 =
       targetipconfig01.getpublicipaddress();
   console.writeline($"ip address:\t{ipaddr01.ipaddress}");
<hr/>
<span xsmall>transient faults patterns</span>
apps can handle transient errors followingoÂºstrategiesÂº:
-OÂºcancelÂº: failure isn't transient

-OÂºretryÂº: unusual/rare failure. (ex: network packet corrupted,...)
           retry immediately

-OÂºretry after delayÂº: connectivity/busy errors,...
         for more common transient failures, the period between retries
         should be chosen to spread requests from multiple instances of
         the application as evenly as possible.

if request is unsuccessful after a predefined number of attempts,
the application should treat the fault as an exception and handle it
accordingly.

c# ex. implementation:
private int retrycount = 3;
private readonly timespan delay = timespan.fromseconds(5);

/// invokes external service asynchronously through
/// the transientoperationasync method.
public async task operationwithbasicretryasync() {
    int currentretry = 0;
    for (;;) {
/*^^^^^^^^^*/try {
        await transientoperationasync();
        break;
/*vvvvvvvvv*/} catch (exception ex) {
        trace.traceerror("operation exception");
        currentretry++;
        if (currentretry > this.retrycount || !istransient(ex)) { throw; }
/*---------*/}
        await task.delay(delay);
    }
}
private bool istransient(exception ex) {
    if (ex is operationtransientexception) return true;
    var webexception = ex as webexception;
    if (webexception != null) {
        return new[] {
            webexceptionstatus.connectionclosed,
            webexceptionstatus.timeout,
            webexceptionstatus.requestcanceled
        }.contains(webexception.status);
    }
    return false;
}
</pre>
</div>
<div groupv>
<span title>instrument for monitâ…‹log</span><br/>
<pre zoom labels="azure,monitoring,troubleshoot,">
<span xsmall>App.Insights: config instrumentation</span>
application insights can be used with any web pages

insert the following script into each page you want to track.
place this code immediately before the closing Ë‚/headËƒ tag,
and before any other scripts. your first data will appear
automatically in just a few seconds.

Ë‚script type="text/javascript"Ëƒ
var appinsights=window.appinsights||function(a){
  function b(a){
    c[a]=function(){var
    b=arguments;
    c.queue.push(function(){c[a].apply(c,b)})}
  }
  var c={config:a},d=document,e=window;
  settimeout(function(){
    var b=d.createelement("script");
    b.src=a.URL||"https://az416426.vo.msecnd.NET/scripts/a/ai.0.JS",
    d.getelementsbytagname("script")[0].parentnode.appendchild(b)
  });
  try{c.cookie=d.cookie}catch(a){}
  c.queue=[];
  for(
    var f=["event","exception","metric","pageview","trace","dependency"];
    f.length;
  ) { b("track"+f.pop()); }
  if(
    b("setauthenticatedusercontext"),
    b("clearauthenticatedusercontext"),
    b("starttrackevent"),
    b("stoptrackevent"),
    b("starttrackpage"),
    b("stoptrackpage"),
    b("flush"),
    !a.disableexceptiontracking) {
     f="onerror",b("_"+f);
     var g=e[f];
     e[f]=function(a,b,d,e,h){
          var i=g&&g(a,b,d,e,h);
          return!0!==i&&c["_"+f](a,b,d,e,h),i
     }
    }
    return c
}({
      instrumentationkey:my_key
  });

window.appinsights=appinsights,appinsights.queue&&0===appinsights.queu
e.length&&appinsights.trackpageview();
Ë‚/scriptËƒ

if your website has a master page, you can put it there.
for example in an asp.NET mvc project, put in:
view/shared/_layout.cshtml.

Âºdetailed configurationÂº
- optional parameters that can be set:
  - disable or limit the number of ajax calls reported per page view
   (to reduce traffic).
  - set debug mode to have telemetry move rapidly through the pipeline
    without being batched.

code snippet to set these parameters:
})({
  instrumentationkey: "..."
  enabledebug: boolean,
  disableexceptiontracking: boolean, // don't log browser exceptions.
  disableajaxtracking: boolean, // don't log ajax calls.
  maxajaxcallsperview: 10, // limit ajax calls logged, def: 500
  overridepageviewduration: boolean, // time page load up to execution of first trackpageview().
  accountid: string, // set dynamically for an authenticated user.
});

â†’ Azure portal â†’ create application insights resource.
                 application type: choose general.
  â†’ take copy of "instrumentation key"
    (placed in "essentials" drop-down of just created resource)
    â†’ install latest "microsoft.applicationinsights" package.
      â†’ set the instrumentation key in your code before tracking any
        telemetry (or set appinsights_instrumentationkey environment
        variable).
        after that, you should be able to manually track telemetry
        and see it on the Azure portal:

        telemetryconfiguration.active.instrumentationkey = my_key;
        var telemetryclient = new telemetryclient();
        telemetryclient.tracktrace("hello world!");

        â†’ install latest version of
          "microsoft.applicationinsights.dependencycollector" package
          it automatically tracks http, SQL, or some other external
          dependency calls.

          â†’ you may initialize and configure application insights from
            the code or using applicationinsights.config file.
          BÂºinitialization  must happens as early as possibleÂº

    RÂºwarnÂº:instructions referring to RÂºapplicationinsights.configÂº are
            only applicable to apps that are targeting the .NET framework,
          RÂºdo not apply to .NET core applicationsÂº

          - configuring telemetry collection from code

app start-up)

    var module = new dependencytrackingtelemetrymodule();
        ^^^^^^
        singleton that must be preserved for application lifetime.

    // prevent correlation id to be sent to certain endpoints.
    // you may add other domains as needed.
    module.excludecomponentcorrelationhttpheadersondomains.add("core.windows.NET");

    ...

    // enable known dependency tracking, note that in future versions,
    // we will extend this list. please check default settings in
    // https://github.com/microsoft/applicationinsights-dotnet-server/BLOB/develop/src/dependencycollector/nuget/applicationinsights.config.install.xdt#l20
    module.includediagnosticsourceactivities.add("microsoft.Azure.servicebus");
    module.includediagnosticsourceactivities.add("microsoft.Azure.eventhubs");
    ...
    module.initialize(configuration); // initialize the module

    // add common telemetry initializers)

    // stamps telemetry with correlation identifiers
    telemetryconfiguration.active.telemetryinitializers.
      add(new operationcorrelationtelemetryinitializer());

    // ensures proper dependencytelemetry.type is set for Azure restful API calls
    telemetryconfiguration.active.telemetryinitializers.
      add(new httpdependenciesparsingtelemetryinitializer());

for .NET framework windows app, you may also install and
initialize performance counter collector module.

full example:

using microsoft.applicationinsights;
using microsoft.applicationinsights.dependencycollector;
using microsoft.applicationinsights.extensibility;
using system.NET.http;
using system.threading.tasks;

namespace consoleapp {
    class program {
        static void main(string[] args) {
            telemetryconfiguration configuration =
              telemetryconfiguration.active;

            configuration.instrumentationkey = "removed";
            configuration.telemetryinitializers.add(new
              operationcorrelationtelemetryinitializer());
            configuration.telemetryinitializers.add(new
              httpdependenciesparsingtelemetryinitializer());

            var telemetryclient = new telemetryclient();
            using (initializedependencytracking(configuration)) {
                // run app...
                telemetryclient.tracktrace("hello world!");
                using (var httpclient = new httpclient()) {
                    // http dependency is automatically tracked!
                    httpclient.getasync("https://microsoft.com").wait();
                }
            }
            telemetryclient.flush();
            task.delay(5000).wait(); // flush is not-blocking. wait a bit
        }

        static dependencytrackingtelemetrymodule
            initializedependencytracking(
               telemetryconfiguration configuration) {
            var module = new dependencytrackingtelemetrymodule();
            // prevent correlation id to be sent to certain endpoints. you may add other domains as needed.

           module.excludecomponentcorrelationhttpheadersondomains.add("core.windows.NET");
           module.excludecomponentcorrelationhttpheadersondomains.add("core.chinacloudapi.cn");
           module.excludecomponentcorrelationhttpheadersondomains.add("core.cloudapi.de");
           module.excludecomponentcorrelationhttpheadersondomains.add("core.usgovcloudapi.NET");
           module.excludecomponentcorrelationhttpheadersondomains.add("localhost");
           module.excludecomponentcorrelationhttpheadersondomains.add("127.0.0.1");

            // enable known dependency tracking, note that in future versions, we will extend this list.
            // please check default settings in
            // https://github.com/microsoft/applicationinsights-dotnet-server/BLOB/develop/src/dependencycollector/nuget/applicationinsights.config.install.xdt#l20
            module.includediagnosticsourceactivities.add("microsoft.Azure.servicebus");
            module.includediagnosticsourceactivities.add("microsoft.Azure.eventhubs");
            module.initialize(configuration); // initialize the module
            return module;
        }
    }
}


<hr/>
<span xsmallËƒanalyze â…‹ troubleshoot with A.monitor</span>
Âºapplication map: triage distributed applicationsÂº

- application map helps youÂºspot performance bottlenecks or failureÂº
  hotspotsÂºacrossÂºall components of yourÂºdistributed applicationÂº
  each node on the map represents an application component or its
  dependencies; and has health kpi and alerts status. you can click
  through from any component to more detailed diagnostics, such as
  application insights events. if your app uses Azure services, you can
  also click through to Azure diagnostics, such as SQL database advisor
  recommendations.

ÂºcomponentÂº:
- independently deployable part of distributed/microservices
  application with independent telemetry.
- deployed onny number of server/role/container instances.
- they can be separate application insights instrumentation
  keys (even if subscriptions are different) or different roles
  reporting to a single application insights instrumentation key.

Âºcomposite application mapÂº
- full application topology across multiple levels of
  related application components.

- components could be different application insights resources,
  or different roles in a single resource.

-Âºthe app map finds components by following http dependencyÂº
 Âºcalls made between servers with the application insights SDKÂº
 ÂºinstalledÂº

- this experience starts with progressive discovery of the components.
  when you first load the application map, a set of queries are
  triggered to discover the components related to this component. a
  button at the top-left corner will update with the number of
  components in your application as they are discovered.

- if all of the components are roles within a single application
  insights resource, then this discovery step is not required. the
  initial load for such an application will have all its components.

- key objective 1:  visualize complex topologies with hundreds of components.

- application map uses theÂºcloud_rolenameÂºproperty to identify the
  components on the map (automatically added by  application insights SDK
  to the telemetry emitted by components).
  to override the default value:
  using microsoft.applicationinsights.channel;
  using microsoft.applicationinsights.extensibility;

  namespace custominitializer.telemetry {
    public class mytelemetryinitializer : itelemetryinitializer {
      public void initialize(itelemetry telemetry) {
        if (string.isnullorempty(telemetry.context.cloud.rolename)) {
           Âºtelemetry.context.cloud.rolename = "rolename";Âº //set custom role
        }
      }
    }
  }

  instantiate the initializer in code, ex:
  ex 1: global.aspx.cs:
  using microsoft.applicationinsights.extensibility;
  using custominitializer.telemetry;
     protected void application_start()
     {
         // ...
         telemetryconfiguration.active.
           telemetryinitializers.
             add(new mytelemetryinitializer());
     }

  ex 2: index.JS  alt 1)
  var appinsights = require("applicationinsights");
  appinsights.setup('instrumentation_key').start();
  appinsights.defaultclient.context.tags["ai.cloud.role"] = "role name";
  appinsights.defaultclient.context.tags["ai.cloud.roleinstance"] =
      "your role instance";

  ex 2: index.JS  alt 2)
  var appinsights = require("applicationinsights");
  appinsights.setup('instrumentation_key').start();
  appinsights.defaultclient.
    addtelemetryprocessor(envelope =Ëƒ {
      envelope.tags["ai.cloud.role"] = "your role name";
      envelope.tags["ai.cloud.roleinstance"] = "your role instance"
  });

  ex 3: client/browser-side javascript
  appinsights.queue.push(() =Ëƒ {
    appinsights.context.
      addtelemetryinitializer((envelope) =Ëƒ {
        envelope.tags["ai.cloud.role"] = "your role name";
        envelope.tags["ai.cloud.roleinstance"] = "your role instance";
    });
  });

- multi tile visualizing data from multiple resources across
  different resource groups and subscriptions how-to:

to create a new dashboard:
â†’ dashboard pane â†’ "new dashboard" â†’ type "dashboard-name"
  â†’ add tile from tile gallery by draging
  â†’ pin charts/.. from application insights to dashboard

  â†’ add health overview by draging  "application insights" tiles

  â†’ add custom metric chart
    metrics panel allows to graph a metric collected by
    application insights over time with optional filters and grouping.
    to add to the  dashboard a little customization is needed first.

  â†’ select "application insights" resource in home screen.
    â†’ select metrics
      (empty chart pre-created)
    â†’ in add a metric prompt:
       add a metric to the chart
      Âºoptionally add a filter and a groupingÂº
    â†’ select pin to dashboard on the right.

Âºadd analytics queryÂº

use application insights analytics Âºrich query languageÂº
- since Azure applications insights analytics is a separate service,
  you need to share your dashboard for it to include an analytics
  query.
  when you share an Azure dashboard, you publish it as an Azure
  resource which can make it available to other users and resources.

  â†’ dashboard screen top â†’ click "share"
                          (keep dashboard name)
    â†’ select "subscription name" to share the dashboard.
      â†’ click publish.
        (dashboard is now available to other services and
         subscriptions)
      â†’ (optionally) define specific users access

        â†’ select your "application insights" resource in home screen.
          â†’ click "analytics" at the top of screen
            to open theÂºanalytics portalÂº
            â†’ type next query (return top 10 most requested pages
                               and their request count)
             ÂºrequestsÂº
              | summarize count() by name
              | sort by count_ desc
              | take 10
              â†’ click "run" to validate
                â†’ click "pin icon" and select dashboard

through activity logs, it's possible to see:
- what operations were taken on the resources in your subscription
- who initiated the operation (although operations initiated by a
  backend service do not return a user as the caller)
- when the operation occurred
- the status of the operation
- the values of other properties that might help to
  research the operation

-Âºactivity log contains all write operations (put, post, delete)Âº
performed on your resources. it does not include read operations
(get).
- useful to find error when troubleshooting or to monitor
  how a user in your organization modified a resource.
- retained for 90 days.
  (from portal, powershell, Azure cli, insights REST API,
   or insights .NET library)
 ÂºpowershellÂº
  $ get-Azurermlog \
      -resourcegroup group01 \
      -starttime 2015-08-28t06:00   â† if start/end not provided,
      -endtime 2015-09-10t06:00       last hour is returned.

  $ get-Azurermlog \
      -resourcegroup group01 \
      -caller someone@contoso.com          â† only this user
      -status failed                       â† filter by status
      -starttime (get-date).adddays(-14) | â† date funct ("last 14 days")
      where-object operationname           â† filter
        -eq microsoft.web/sites/stop/action
  â†’ authorization     :
  â†’ scope     : /subscriptions/xxx/resourcegroups/group01/providers/microsoft.web/sites/examplesite
  â†’ action    : microsoft.web/sites/stop/action
  â†’ role      : subscription admin
  â†’ condition :
  â†’ caller            : someone@contoso.com
  â†’ correlationid     : 84beae59-92aa-4662-a6fc-b6fecc0ff8da
  â†’ eventsource       : administrative
  â†’ eventtimestamp    : 8/28/2015 4:08:18 pm
  â†’ operationname     : microsoft.web/sites/stop/action
  â†’ resourcegroupname : group01
  â†’ resourceid        : /subscriptions/xx/resourcegroups/group01/providers/microsoft.web/sites/examplesite
  â†’ status            : succeeded
  â†’ subscriptionid    : xxxxx
  â†’ substatus         : ok

    to focus on an output filed:
    (
      (
       get-Azurermlog \
         -status failed \
         -resourcegroup group01
       Âº-detailedoutputÂº
      ).properties[1].content["statusmessage"] |
      convertfrom-JSON
    ).error
    â†’ returning:
    â†’
    â†’ code             message
    â†’ ----             -------
    â†’ dnsrecordinuse   dns record dns.westus.cloudapp.Azure.com is
    â†’                  already used by another public ip.


ÂºAzure cliÂº

 $ az monitor \
   activity-log list \
   --resource-group $group

ÂºREST APIÂº
- REST operations for working with the activity log are part of the
  insights REST API. to retrieve activity log events, see list the
  management events in a subscription.
- application insights sends web requests to your application at
  regular intervals from points around the world.
  it alerts you if your application doesn't respond, or responds slowly.
  for any http or https endpoint that accessible from the public internet.

- it can be third REST API service on which our app depends.

- availability tests types:
  -ÂºURL ping testÂº      : simple test  (created in Azure portal)
  -Âºmulti-step web testÂº: (created in v.s. enterprise
                           and  uploaded to portal)

  - up to 100 availability tests per application resource.

 pre-setup: configure application insights for a web app
 portal â†’ open "application insights"
 create a URL ping test:
 â†’ open "availability" blade and add a test.
   fill internet-public URL
   (up to 10 redirects allowed).
   â†’ parse dependent requests: if checked, test will also requests images,
     scripts, style files, ...
     recorded response time includes time taken to get these files.
     test fails if any resource fails within the timeout.
   â†’ enable retries: if checked, falied test retried after short
     interval up to 3 times.
     (retry is suspended until the next success).
   â†’ test frequency: default to 5min + 5 test locations
     (min. 5 locations recomended, up to 16 ).
   note:
   """ we have found that optimal configuration is:
        number of test locations = alert location threshold + 2."""

   success criteria:
   - test timeout  not exceeded.
   - http response is 200
   - content match match expected one
     (plain string, without wildcards)
   - alert location threshold: minimum of 3/5 locations recomended.

Âºmulti-step web testsÂº
- test sequence of URLs.
- record the scenario by using visual studio enterprise.
- upload recording to application insights.
- coded functions or loops not allowed.
- tests ust be contained completely in the .webtest script.
- only english characters supported.
  update the web test definition file to
  translate/exclude non-english characters.
</pre>
</div>
<div groupv>
  <span title>integrate cachingâ…‹cdns</span>
<pre zoom labels="azure,troubleshoot,cache">
<span xsmall>Azure cache for redis</span>
-  with Azure cache for redis, fast storage is located in-memory
  instead of being loaded from disk by a database.

- it can also be used as an in-memory data structure
  store, distributed non-relational database, andÂºmessage brokerÂº.

- performance improved by taking advantage of the
  low-latency, high-throughput performance of the redis engine.

- redis supports a variety of data types all oriented
  around binary safe strings.
  - binary-safe strings (most common)
  - lists of strings
  - unordered sets of strings
  - hashes
  - sorted sets of strings
  - maps of strings
- redis works best with smaller values (100k or less)
  BÂº:consider chopping up bigger data into multiple keysÂº

-Âºeach data value is associated to a keyÂºwhich can be used
  to lookup the value from the cache.
- up to 500 mb values are possible, but increases
  network latency and RÂºcan cause caching and out-of-memory issuesÂº
RÂºif cache isn't configured to expire old valuesÂº

- redis keys: binary safe strings.
  - guidelines for choosing keys:
    - avoid long keys. they take up more memory and require longer
      lookup times because they have to be compared byte-by-byte.
    - prefer hash of big keys to big keys themself.
    - maximum size: 512 mb, but much smaller must be used.
    - prefer keys like "sport:football;date:2008-02-02" to
      "fb:8-2-2". extra size and performance difference is
      negligible.

- data in redisÂºis stored in nodes and clustersÂº
-Âºclusters are sets of three or more nodesÂºyour dataset is split
  across.

- redis caching architectures?
  -Âºsingle node  Âº:
  -Âºmultiple nodeÂº:
  -Âºclustered    Âº:

- redis caching architectures areÂºsplit across Azure by (pricing)tiers:Âº
  -Âºbasic cacheÂº: single node redis cache.
    complete dataset stored in a single node.
    (development, testing, non-critical workloads)
    - up toÂº53 gbÂºof memory andÂº20_000 simul. connectionsÂº
    - no sla for this service tier.
  -Âºstandard cacheÂº: multiple node
    redis replicates a cache in a two-node primary/secondary
    configuration. Azure manages the replication
    between the two nodes. Âºproduction-readyÂºcache with
   Âºmaster/slaveÂºreplication.
    - up toÂº53 gbÂºof memory andÂº20_000 simul. connectionsÂº
    - sla: 99.99%.
  -Âºpremium tierÂº:  standard tier + ability to persist data,
    take snapshots, and back up data.
    - it also supports an a.virtual network to give
      complete control over your connections, subnets, ip addressing, and
      network isolation.
    - it alsoÂºincludes geo-replication,Âºensuring that
      data is close to the consumming app.
    - up toÂº530 gbÂºof memory andÂº40_000 simul. connectionsÂº
    - sla: 99.99%.
    - disaster recovery persist data type:
      -Âºrdb persistenceÂº: periodic snapshots, can rebuild cache
                          using the snapshot in case of failure.
      -Âºaof persistenceÂº: saves every write operation to a log
                          at least once per second. it creates
                          bigger files than rdb but has less data loss.
    - clustering support with up to 10 different shards.
      cost: cost of the original node x number of shards.

options: Azure portal, the Azure cli, or Azure powershell.

parameters:
-ÂºnameÂº: globally unique and used to generate a public-facing
         URL to connect and communicate with the service.
        (1 and 63 chars).
-Âºresource groupÂº:
      BÂºmanaged resource and needs a resource group ownerÂº
-ÂºlocationÂº: as close to the data consumer as possible)

-Âºamount of cache memoryÂºavailable on each (pricing) tier -
  is selected by choosing a cache level:
  - Âºc0 to c6Âºfor basic/standard
     c0 really meant for simple dev
     (shared cpu core and very little memory)
  - Âºp0 to p4Âºfor premium.


Âºaccessing the redis instanceÂº

a command is typically issued as:
command parameter1 parameter2 parameter3

common commands include:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚command          â”‚ description                                                          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ping             â”‚ping the server. returns "pong".                                      â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚set [key] [value]â”‚sets a key/value in the cache. returns "ok" on success.               â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚get [key]        â”‚gets a value from the cache.                                          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚exists [key]     â”‚returns '1' if the key exists in the cache, '0' if it doesn't.        â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚type [key]       â”‚returns the type associated to the value for the given key.           â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚incr [key]       â”‚increment the given value associated with key by '1'. the             â”‚
â”‚                 â”‚value must be an integer or double value. this returns the new value. â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚incrby           â”‚ increment the given value associated with key by the specified amountâ”‚
â”‚   [key] [amount]â”‚ the value must be an integer or double value.                        â”‚
â”‚                 â”‚ this returns the new value.                                          â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚del [key]        â”‚ deletes the value associated with the key.                           â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚flushdb          â”‚ delete all keys and values in the database.                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

-Âºredis has a command-line tool (redis-cli)Âº: ex:
  Ëƒ set somekey somevalue     Ëƒ set counter 100
  ok                          ok
  Ëƒ get somekey               Ëƒ incr counter
  "somevalue"                 (integer) 101
  Ëƒ exists somekey            Ëƒ incrby counter 50
  (string) 1                  (integer) 151
  Ëƒ del somekey               Ëƒ type counter
  (string) 1                  (integer)
  Ëƒ exists somekey
  (string) 0

-Âºadding an expiration time to valuesÂº
 key time to live (ttl)

when the ttl elapses, key is automatically deleted.
some notes on ttl expirations.
  - expirations can be set using seconds or milliseconds precision.
  - the expire time resolution is always 1 millisecond.
  - information about expires are replicated and persisted on disk,
    the time virtually passes when your redis server remains stopped
(this means that redis saves the date at which a key will expire).

example of an expiration:
Ëƒ set counter 100
ok
ËƒÂºexpire counter 5Âº
(integer) 1
Ëƒ get counter
100
... wait ...
Ëƒ get counter
(nil)

Âºaccessing a redis cache from a clientÂº
- clients need:
  - host name, port, and an access key for the cache
    ( Azure portal â†’ settings â†’ access keys page)
    note: Azure also offers a connection string for some redis
    clients which bundles this data together into a single
    string.
  - there are two keys created: primary and secondary.
    you can use both. secondary used in  case you need
    to change primary one:
    - switch all clients to the secondary key.
    - regenerate the primary key blocking any app still
      using original primary one.
    - microsoft recommends periodically regenerating the keys

- typically a client app will use a client library.
  a popular high-performance redis client for the .NET language is
  stackexchange.redis nuget package.

 Âºconnection stringÂºwill look  something like:
  (it should be protected. consider using an Azure key vault)
  [cache-name].redis.cache.windows.NET:6380,password=[RÂºpass..Âº],ssl=true,abortconnect=false
 ÂºsslÂº: ensures that communication is encrypted. â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚
 ÂºabortconnectionÂº:  allows a connection to be created even if the â”€â”€â”€â”€â”€â”˜
                    server is unavailable at that moment.

- c# stackexchange.redis:

   using stackexchange.redis;
   ...
   var connectionstring = "[cache-name].redis.cache.windows.NET:6380,"+
                          "password=[password-here],ssl=true,abortconnect=false";
intended to be kept around while you need access to the cache.
   varÂºredisconnectionÂº= connectionmultiplexer.connect(connectionstring);
   //  ^^^^^^^^^^^^^^^ intended to be reused   ^^^^^^ async also supporte
   // ÂºredisconnectionÂº can now be used for:
   //  - accessing redis database:
   //  - use the publisher/subscript features of redis.
   //  - access an individual server for maintenance or monitoring
   //    purposes.
   idatabase DB = redisconnection.getdatabase();
   //        ^^ lightweight object. no need to store.
   bool wasset = DB.stringset("favorite:flavor", "i-love-rocky-road");
   //  bool indicates whether the value was set (true) or not (false).
   string value = DB.stringget("favorite:flavor");
   console.writeline(value);
   byte[] key = ...;
   byte[] value = ...;
   DB.stringset(key, value);
   byte[] value1 = DB.stringget(key);

- idatabase interface includes several other methods to work with
  hashes, lists, sets, and ordered sets.
  more common ones work with single keys.
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚method           â”‚description                                                    â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚createbatch      â”‚ creates a group of operations to be sent to the server        â”‚
  â”‚                 â”‚ as a single unit, but not necessarily processed as a unit.    â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚createtransactionâ”‚ creates a group of operations to be sent to the server        â”‚
  â”‚                 â”‚ as a single unit and processed on the server as a single unit.â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚keydelete        â”‚ delete the key/value.                                         â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚keyexists        â”‚ returns whether the given key exists in cache.                â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚keyexpire        â”‚ sets a timeâ”€toâ”€live (ttl) expiration on a key.                â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚keyrename        â”‚ renames a key.                                                â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚keytimetolive    â”‚ returns the ttl for a key.                                    â”‚
  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚keytype          â”‚ returns the string representation of the type of the value    â”‚
  â”‚                 â”‚ stored at key. the different types that can be returned are:  â”‚
  â”‚                 â”‚ string, list, set, zset and hash.                             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


Âºexecuting other commandsÂº
- idatabase instances can use execute and executeasync
  to pass textual commands to the redis server.
  var result = DB.execute("ping");
  //  ^^^^^^
  // properties:
  // -ÂºtypeÂº    : "string", "integer", ...
  // -ÂºisnullÂº  : true/false
  // -ÂºtostringÂº: actual return value.
  console.writeline(result.tostring()); // displays: "pong"

  ex 2:get all the clients connected to the cache ("client list"):
  var result = await DB.executeasync("client", "list");
  console.writeline($"type = {result.type}\r\nresult = {result}");
  â†’ type = bulkstring
  â†’ result = id=9469 addr=16.183.122.154:54961 fd=18 name=desktop-aaaaaa
  â†’ age=0 idle=0 flags=n DB=0 sub=1 psub=0 multi=-1 qbuf=0 qbuf-free=0
  â†’ obl=0 oll=0 omem=0 ow=0 owmem=0 events=r cmd=subscribe numops=5
  â†’ id=9470 addr=16.183.122.155:54967 fd=13 name=desktop-bbbbbb age=0
  â†’ idle=0 flags=n DB=0 sub=0 psub=0 multi=-1 qbuf=0 qbuf-free=32768
  â†’ obl=0 oll=0 omem=0 ow=0 owmem=0 events=r cmd=client numops=17

Âºstoring more complex valuesÂº
- you can cache off object graphs by serializing them to a textual
  format - typically xml or JSON.
  ex:
  public class gamestat {
      public string id { get; set; }
      public string sport { get; set; }
      public datetimeoffset dateplayed { get; set; }
      public string game { get; set; }
      public ireadonlylistË‚stringËƒ teams { get; set; }
      public ireadonlylistË‚(string team, int score)Ëƒ results { get; set; }
      ...
  }

  we could use the newtonsoft.JSON library to turn an instance of this
  object into a string:
  var stat = new gamestat(...);
  string serializedvalue = Âºnewtonsoft.JSON.JSONconvert.serializeobject(stat);Âº
  bool added = DB.stringset("key1", serializedvalue);
  var result = DB.stringget("key1");
  var stat = newtonsoft.JSON.JSONconvert.
              deserializeobjectË‚gamestaËƒ(result.tostring());
  console.writeline(stat.sport); // displays "soccer"

Âºcleaning up the connectionÂº
 redisconnection.dispose();
 redisconnection = null;

(video)
</pre>
<pre zoom labels="azure,storage,performance,cache">
<span xsmall>develop for storage on cdns</span>
cdns store cached content on edge servers that are close to users
to minimize latency.  these edge servers are located in point of
presence (pop) locations that are distributed throughout the globe.

 using Azure cdn, you can cache publicly available objects loaded
Âºfrom Azure BLOB storage, a web application, a virtual machine,Âº
Âºor any publicly accessible web server.Âº
 Azure cdnÂºcan also accelerate dynamic content, which cannot Âº
Âºbe cached, by taking advantage of various network optimizations byÂº
Âºusing cdn pops.Âº an example is using route optimization to bypass
 border gateway protocol (bgp).

domain name system (dns) routes the request to
the best-performing pop location, usually the one
geographically closest to the user.

 $ az cdn profile list \ â† list all of your existing cdn profiles
                          associated with your subscription.
   --resource-group ..   â† optional. filter by resource group

 $ az cdn profile create \  â† step 1) create a new profile
   --name demoprofile \
   --resource-group examplegroup
   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   by default, standard tier is used
   and theÂºakamai providerÂº. this
   can be customized wit "--sku"
   followed by next options:
   -custom_verizon
   -premium_verizon
   -standard_akamai
   -standard_chinacdn
   -standard_verizon

  $ az cdn endpoint create \    â† step 2) create an endpoint.
    --name contosoendpoint \
    --origin www.contoso.com \
    --profile-name demoprofile \â† profile needed (step 1)
    --resource-group group01    â† resource-group needed

  $ az cdn custom-domain create \    â† assign custom domain to cdn endpoint to
    --name filesdomain                 ensure that users see only choosen domains
   --hostname files.contoso.com \      (instead of the Azure cdn domains)
   --endpoint-name contosoendpoint \
   --profile-name demoprofile \
   --resource-group group01

- to save time and bandwidth consumption, a cached resource
  is not compared to the version on the origin server every time it is
  accessed. instead, as long as a cached resource is considered to be
  fresh, it is assumed to be the most current version and is sent
  directly to the client.
- a cached resource is considered to be fresh
  when its age is less than the age or period defined by a cache
  setting. for example, when a browser reloads a webpage, it verifies
  that each cached resource on your hard drive is fresh and loads it.
  if the resource is not fresh (stale), an up-to-date copy is loaded
  from the server.

Âºcaching rulesÂº
- cache expiration duration in days, hours, minutes, and seconds
  can be set.
- Azure cdn caching rules type:
  -Âºglobal caching rulesÂº: you can set
    1 global caching rule â†â†’ for each endpoint in a profile
    itaffects all requests to the endpoint.
    global caching rule overrides any http cache-directive
    headers, if set.
  -Âºcustom caching rulesÂº:
    1+ custom caching rule â†â†’ for each endpoint in a profile
    they match specific paths and file extensions;
    processed in order
    override the global caching rule, if set.

Âºpurging and preloading assets by using the Azure cliÂº
ÂºpurgeÂº unpublish cached assets from an endpoint.
        very useful if you have an application scenario
        where a large amount of data is invalidated and
        should be updated in the cache.

  $ az cdn endpoint purge \
    --content-paths '/css/*' '/JS/app.JS' \ â† file path/wildcard dir/both
    --name contosoendpoint \
    --profile-name demoprofile \
    --resource-group group01

  $ az cdn endpoint load \                    â† preload assets
    --content-paths '/img/*' '/JS/module.JS'  \ (improve user experience
    --name contosoendpoint \                    by prepopulating the cache)
    --profile-name demoprofile
    --resource-group group01
<hr/>
</pre>
</div>

</div> <!-- Monitor,tune,troubleshoot -->

<div group> <!-- cloud Architecture -->
<span title>Decoupled system integration. Events and Message queues</span>
<hr/>
<div groupv>
<span title>(App Service) Logic App</span>
<hr/>
<pre zoom labels="azure,esb,">
<span xsmall>overview</span>
- (Mule|Cammel) alike Azure "ESB"
  for processing/routing messages
  Ex:
    - FTP â†’ A.Storage
    - events to email
    - (Monitored)tweets subject â†’ analyze sentiment â†’ alert/task
- (e)nterprise (a)pplication (i)ntegration and
  business-to-business (B2B) communication
- app/data/system integration.
- on premises support.
- Design and build with A.Portal ÂºLogic App DesignerÂº
 (and Enterprise Integration Pack for B2B)
- manage with PowerShell.

- Logic App Data Journey:
 â†’ Âºspecific event triggerÂº  â† - data match criteria with (Opt) basic scheduling
   â†’ Logic App Engine
     â†’ Âºnew logic app instanceÂº
        (workflow start)
       â†’ data conversions
         flow controls (conditional|switch|loops|branching)

ÂºConnectorÂº
- 200+ connectors:  A.Service Bus, Functions,  Storage, SQL,
                    Office 365, Dynamics, BizTalk, Salesforce, SAP,
                    Oracle DB, file shares, ...

- Connector  : -ÂºTriggersÂº:
  Components     - Allow to notify of specific events to
                   Logic Apps or ÂºFlowsÂº
                   Ex: FTP_connectorÂºOnUpdatedFileÂº
                 - TYPES: ÂºPollingÂº (at intervals) for new data.
                          ÂºPushÂº  , listening for data on an endpoint.

               -ÂºActionsÂº :
                 - changes directed by a user (Ex: CRUD SQL action)
                 - BÂºAll actions map to Swagger operationsÂº

- Allow to create data-flows in "real time"

- available as:
  -ÂºBuilt-insÂº:  - create data-flows on custom schedules
   ÂºConnectorsÂº  - communicate with endpoints
                 - receive+respond to requests
                 - call:
                   - A.functions
                   - A.API Apps (Web Apps)
                   - managed APIs
                   - nested logic apps

  -ÂºManagedÂº :   - Allow to access other services and systems.
   ÂºConnectorsÂº  - Two managed connector groups:
                   â”œ ÂºManaged API connectorsÂº:
                   â”‚ - use BLOB Storage, Office 365, Dynamics, Power BI,
                   â”‚   OneDrive, Salesforce, SharePoint Online, ...
                   â”” ÂºOnâ”€premises connectorsÂº:
                     - Require PRE-SETUP by installing
                       Âºon-premises data gatewayÂº
                     - In/Out data form SQL Server, SharePoint,
                       Oracle DB, file shares, ...


                 -ÂºEnterprise connectorsÂº:
                   - SAP, IBM MQ, ... forRÂºan additional costÂº
                   - Encryption/e-signatures supported

                 -ÂºEnterprise Integration Pack and ConnectorsÂº
                   - Makes use of B2B scenarios,
                     similar to BizTalk Server.
                   - validate/transform XML, encode/decode flat files,
                     processÂºB2B messages with AS2, EDIFACT, and X12 protocolsÂº.
                   -ÂºArchitecturallyÂº based on Âºintegration accountsÂº:
                     Âºcloud-based containers that store all artifactsÂº
                     Âº(schemas, partners, certificates, maps, and agreements)Âº
                     to design/deploy/maintain logic-apps Non-logic app B2B apps
                   - Logic app integration PRE-SETUP:
                     Enable permissions in integration-account to allow
                     logic app instance/s access to its artifacts.
                   - RÂºintegration account is billed appartÂº
                     to simplify the storage and
                     management of artifacts used in B2B communications.
                   - High-level steps:
                     Create an  â†’ Add partners, â†’ Create a â”€â”€â”
                     integration  schemas,        Logic App  â”‚
                     account in   certificates,              â”‚
                     A.Portal     maps,                      â”‚
                                  agreements                 â”‚
                                  to int.acct.               â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â””â”€â†’ Link Logic  â†’ In the Log.App
                     App to the    use the partners,
                     Integration   schemas,  certificates
                     account       and agreements
                                   stored in the
                                   Int.Account

<hr/>
<span xsmall>Logic Apps in Visual Studio</span>
- create workflows integrating apps, data, systems, services across
  enterprises and organizations.

- Over A.Portal tool, Visual Studio also allows to add logic-apps to
  git, publish different versions, and create A.Resource Manager templates
  for different deployment environments.

ÂºEx App:Âº
  Website's RSS feed  .... â†’ sends email for each new item.

PREREQUISITES
  - Azure subscription
  - Visual Studio 2015+ - Community edition
  - Microsoft Azure SDK for .NET 2.9.1+
  - Azure PowerShell.
  - An email account that's supported by Logic Apps,
    (Office 365 Outlook, Outlook.com, Gmail, ...?).
  - Access to the web while using the embedded Logic App Designer
  - Internet connection to create resources in Azure and
    to read the properties and data from connectors in logic app.

ÂºCreate an Azure Resource Group projectÂº
- Start Visual Studio â†’ sign-in with A.account.
  â†’ File menu â†’ New Ëƒ Project
    â†’ Under Installed, select Visual C#.
      â†’ Select Cloud Ëƒ Azure Resource Group.
          Fill in project name:
        â†’ Select the Logic App template.
          (After project is created, Solution Explorer opens
           and shows your solution)
        â†’ In your solution, the LogicApp.JSON file not only stores the
          definition for your logic app but is also an Azure Resource Manager
          template that you can set up for deployment.

- Create a blank Logic App:
  Solution Explorer â†’ open shortcut menu for "LogicApp.JSON" file.
  â†’ Select "Open With Logic App Designer"
    â†’ Open logic app .JSON file with Logic App Designer
      Select your Subcription.
      For Resource Group: select "Create New..."
      Select resource location

<hr/>
<span xsmall>(Visual)L.Apps Designer</span>
- available in A.Portal and Visual Studio.
- For customized logic apps, logic app definitions can be
  created in JSON. (code view mode).
- A.PowerShell commands and Azure Resource Manager templates
  can be used to select tasks.

ÂºCreating the triggerÂº)
 NOTE: Every logic app must start with a trigger

  Logic App Designer â†’ enter "rss" in search box.
  â†’ Select trigger "When a feed item is published
                    Build your logic app by adding
                    a trigger and actions"
    â†’ Provide next for the trigger:
      (RSS trigger appears in Logic App Designer)
    Property        Value
    RSS feed URL    http://feeds.reuters.com/reuters/topNews
    Interval        1
    Frequency       Minute

ÂºAdding an actionÂº)
 Under "When a feed item is published trigger..."
 select " + New step â†’ Add an action "
 â†’ Select "send an email" from provider ctions list
   â†’ Select action: "Office 365 Outlook - Send an email"
     - Fill in/sign-in credentials if requested.
     - Fill in data to include in email.
       To: address
       Subject:  ....
                 Add dynamic content list
                 â†’ select "Feed title"

                 NOTE: A "For each" loop automatically appears on
                       the designer, if a token for an array was selected.

        Body: ....
<hr/>
<span xsmall>Deploy</span>

Solution Explorer â†’ project's shortcut menu
 â†’ select Deploy Ëƒ New
   â†’ Create logic app deployment
     â†’ When deployment starts, status appears in
       Visual Studio Output window.
       (open "Show output from list", and select
        an Azure resource group if not visible)
</pre>
<pre zoom labels="azure,esb,">
<span xsmall>custom connectors</span>

- Custom connectors are function-based:
  specific functions called in underlying service
  and data returned as response.

- To create a custon connector in Logic Apps :
1) create custom connector
   â†’ Portal â†’ "New" â†’ Search "logic apps connector"
     â†’ Create. Fill in details:
       ( Name, Subscription, Resource Group, Location)
2) define behavior of the connector using an
   OpenAPI definition or a Postman collection
  ÂºPREREQUISITESÂº
   - OpenAPI definition size must be than 1 MB.
   - One of the following subscriptions:
     - Azure, if using Logic Apps
     - Microsoft Flow
     - PowerApps
  ÂºUploading OpenAPIÂº
   portal â†’ open "Logic Apps connector" of step 1)
    â†’ connector's menu â†’ choose "Logic Apps Connector"
      â†’ Click "Edit"
        â†’ Under "General", choose "Upload an OpenAPI file",
          (From this point, we'll show the Microsoft Flow UI,
           but steps are largely the same across all three technologies)
          â†’  review information imported from OpenAPI definition.
             (API host, base URL,...)
             "info": {
               "version": "1.0.0",
               "title": "SentimentDemo",
               "description": "Uses Cognitive Services Text ... "
             },
             "host": "westus.API.cognitive.microsoft.com",
             "basePath": "/",
             "schemes": [
               "https"
             ]
             ...
             "securityDefinitions": {
               "api_key": {         â† ÂºAPI_key Review authentication typeÂº
                 "type": "apiKey",     (Other Auth. methods could apply)
                 "in": "header",
                 "name": "Ocp-Apim-Subscription-Key"
               }
             }
</pre>
<pre zoom labels="azure,esb,">
<span xsmall>Custom templates</span>
- Logic App deployment template overview
  - three basic components compose the Logic App:
    -ÂºLogic app resourceÂº : Info. about pricing plan, location, ...

    -ÂºWorkflow definitionÂº: workflow steps and how the Logic Apps
                            engine should execute them.

    -ÂºConnectionsÂº        : Refers to separate resources that securely store
                            metadata about any connector connections, such as
                            a connection-string and an access-token.

- Create a Logic App deployment template:
  easiest way: use the Visual Studio Tools for Logic Apps.
  other tools:
   - author by hand.
   - logic-app template creatorÂºPowerShell moduleÂº.
     It first evaluates the logic app and any connections
     that it is using, and then generates template resources
     with the necessary parameters for deployment.
     Installation via theÂºPowerShell GalleryÂº:
     $ Install-Module -Name LogicAppTemplate
     - To make module work with any tenant/subscription
       access token, it is recommended to use it with the
      ÂºARMClient command-line toolÂº (ARM: Azure Resource Manager)

   - To generate a template with PowerShell armclient:

     $ armclient token $SubscriptionId | \     â† get access token and pipe to PowerShell script
       Get-LogicAppTemplate -LogicApp MyApp \
       -ResourceGroup MyRG \
       -SubscriptionId $SubscriptionId \
       -Verbose |Out-File  ÂºG/home/myuser/template.JSONÂº

   - Note: You can use BÂºlogic app trigger|actions parametersÂº in:
     - Child workflow
     - Function app
     - APIM call
     - API connection runtime URL
     - API connection path

   - Ex: Parameterized A.Function BÂºresource IDÂº:

     "parameters":{
     QÂº"functionName":Âº{                  // Defining the parameter
         "type":"string",
         "minLength":1,
         "defaultValue":"Ë‚FunctionNameËƒ"
       }
     },

     "MyFunction": {                     // Using te parameter
       "type": "Function",
       "inputs": {
         "body":{},
         "function":{
           "id":"[
           BÂºresourceidÂº(
               'Microsoft.Web/sites/functions',
               'functionApp',
               parameters(QÂº'functionName'Âº)
             )
           ]"
         }
       },
       "runAfter":{}
     }

     Ex2: parameterize Service Bus Send message operation:
     "Send_message": {
     Â· "type": "ApiConnection",
     Â· "inputs": {
     Â· Â· "host"    : {
     Â· Â·               "connection": {
     Â· Â·                 "name":
     Â· Â·               "@BÂºparameters('$connections')Âº['servicebus']['connectionId']"
     Â· Â·               }
     Â· Â·             },
     Â· Â· "method"  : "post",
     Â· Â· "path"    :
     Â· Â·             "[concat(
     Â· Â·                '/@{encodeURIComponent
     Â· Â·                    (
     Â· Â·                      ''',
     Â· Â·                    BÂºparameters('queueuname')Âº,
     Â· Â·                      '''
     Â· Â·                    )}/messages')
     Â· Â·             ]",
     Â· Â· "body"   : { "ContentData": "@{base64(triggerBody())}" },
     Â· Â· "queries": { "systemProperties": "None" }
     Â· },
     Â· "runAfter": {}
     }
     Note: host.runtimeUrl is optional, can be removed from template if present.

     Logic App Designer will need default parameters values to work properly.
     Ex:
     "parameters": {
         "IntegrationAccount": {
           "type":"string",
           "minLength":1,
           "defaultValue":"/subscriptions/$subscriptionID/resourceGroups/$resGroup/"
                          "providers/Microsoft.Logic/integrationAccounts/$intAccount"
         }
     },
<hr/>
<span xsmall>Adding to Resource Group</span>
- Edit "template.JSON", then select "View" â†’ Other Windows â†’ JSON Outline

  To add a resource to the template file:
  â†’ Add resource like:
  Â· - alt1: click "Add Resource" (top of JSON Outline window)
  Â· - alt2: JSON Outline window â†’ right-click "resources" â†’ select "Add New Resource".
  â”” â†’ find and select Logic App in "dialog box",
    Â· Fill in Logic App Name  and click "Add"
    Â·
    â”” â†’ Deploy logic-app-template:
      Â· STEP 1) create parameter file with params values.
      Â· STEP 2) Deploy alternatives:
      Â·         - PowerShell
      Â·         - REST API
      Â·         - Visual Studio Team Services Release Management
      Â·         - A.Portal â†’ template deployment.
      Â·         NOTE: logic-app will works OK with valid
      Â·               parameters after deployment
      Â·
      â”” â†’ Authorize OAuth connections:
          Logic Apps Designer â†’ Open Logic-app â†’ authorize connections.

          For automated deployment, you can use a script to
          consent to each OAuth connection.
          (See example script on GitHub under LogicAppConnectionAuth project)
</pre>
</div>

<div groupv>
<span title>Event Grid</span>
<pre zoom labels="azure,message_stream">
<span xsmall>Event Grid Overview</span>
- Specalized Queue-like for Azure Objects Events
  (File Shares, BLOG, storage ...)
  Custom events also allowed.

1st) select the resource you would like to subscribe to.
2nd) provide event handler | WebHook endpoint to send the event to.

You can use filters to route specific events to different endpoints,
multicast to multiple endpoints, and make sure your events are
reliably delivered.

ÂºConceptsÂº
 â”” Events   : What happened.
                common info: source, time, unique id.
              + specific event type info.
                max size:: 64 KB of data.
   event schema:
   [   â† Grid sends the events to subscribers in an array that has a
   {     single event. (This may change in a future 2020-01).
     "topic"    : string,      â† full resource path the event source.
                                 read-value provided by source
     "subject"  : string,      â† publisher-defined path to the event subject
     "id"       : string,
     "eventType": string,
     "eventTime": string,      â† provider's utc time.
     "dataVersion": string,    â† schema version of data object defined by publisher
     "metadataVersion": string â† Provided by event grid
     "data"     : { ... },     â† resource provider specific.
   }                             top-level data should have same fields as standard
   ]                             Azure resource-defined events.

   Ex:
   [
   Â· {
   Â·   "topic"     : "/subscriptions/"id"/resourcegroups/storage"
                     "/providers/microsoft.storage/storageaccounts/xstoretestaccount",
   Â·   "subject"   : "/BLOBservices/default/containers/"containerId"/"
                     "BLOBs/"blobId"",
   Â·   "eventtype" : "microsoft.storage.BLOBcreated",
   Â·   "eventtime" : "2017-06-26t18:41:00.9584103z",
   Â·   "id"        : "831e1650-001e-001b-66ab-eeb76e069631",
   Â·   "data"      :
        {
   Â·      "API"            : "putblocklist",
   Â·      "clientrequestid": "6d79dbfb-0e37-4fc4-981f-442c9ca65760",
   Â·      "requestid"      : "831e1650-001e-001b-66ab-eeb76e000000",
   Â·      "etag"           : "0x8d4bcc2e4835cd0",
   Â·      "contenttype"    : "application/octet-stream",
   Â·      "contentlength"  : 524288,
   Â·      "BLOBtype"       : "blockBLOB",
   Â·      "URL"            : "https://oc2d2817345i60006.BLOB.core.windows.NET"
                             "/$containerId/$BLOB",
   Â·      "sequencer"      : "00000000000004420000000000028963",
   Â·      "storagediagnostics": {
   Â·        "batchid": "b68529f3-68cd-4744-baa4-3c0498ec19f0"
   Â·      }
   Â·    },
   Â·   "dataversion": "",
   Â·   "metadataversion": "1"
   Â· }
   ]

 â”” Event    : Where the event took place.
   sources

 â”” Topics   : - collection-of-related-events.
                |Publisher| â†’(writes to)â†’ |Topic|â†’(push to) â†’|Subscribers|

              - System topics: A.services built-in topics .
                App Developers can not see them but can subscribe to them.

              - Custom topics: applicationâ…‹third-party topics.

 â”” Event    : - endpoint or built-in mechanism to route events to 1+ handlers.
   subscrip-    Also used by handlers to intelligently filter incoming events.
   tion       - It tells the Event Grid which events on a topic
                you're interested in receiving.
                Filters (on event type, subject pattern) can be set to
                different endpoins.

 â”” Event    : - Azure functions
   handlers   - Logic Apps
              - Azure Automation
              - WebHook
              - Queue Storage:  (retried until queue-push consumes the message)
              - Hybrid Connections
              - Event Hubs
              - Custom webhook: (retried until handler returns HTTP 200 - OK)

 â”” Incomming Events to Event Grid are stored in an array,
   whose total size is up to 1 MB. containing 1+ event objects,
   with each event limited to 64 KB.
<hr/>
<span xsmall>Securityâ…‹Authentication</span>
BÂºauthentication typesÂº
 Âºâ””Âºwebhook event delivery:
    - event grid requires "you" to prove ownership of
      target (subscriber) webhook
      - Automatically handled for:
        - A.logic apps with event grid connector
        - A.automation via webhook
        - A.functions with event grid trigger
      - For other uses casesÂºvalidation handshakeÂºis needed.
        -Âºvalidationcode handshakeÂº(programmatic):
        Â· Recomended when in control of endpoint source code.
        Â· At event-subscription-creation time, event grid POSTs
        Â· a subscription validation event to your endpoint.
        Â·  The data portion of this event includes a
        Â· validationcode property. Handler verifies that
        Â· validation request is for an expected event subscription,
        Â· and echoes the validation code to event grid.
        Â· (supported in all event grid versions).
        -Âºvalidationurl handshakeÂº(manual, 2018-05-01-preview+):
          (non-controlled) third-party service.
          - event grid POST a validationurl property in
            the data portion of the subscription validation event.
            - to complete the handshake, find that URL in the
              event data and manually send a get request to it.
              (URL expires in 10 minutes)

 Âºâ””Âºevent subscriptions:

 Âºâ””Âºcustom topic publishing:
    "aeg-event-type: subscriptionValidation"
    eventtype      : microsoft.eventgrid.subscriptionValidationEvent
    data           :
    {
       ...
       validationcode : random string
       validationurl  : URL-for-manual-validation
    }                   ^ API version 2018-05-01-preview+

BÂºEvent delivery securityÂº:
  â”” webhook subscriber endpoint can add query parameters to
    the webhook URL at event subscription:
    - Set one of them to be a secret for extra security.
      Then, programatically reject non-matching secrets on the Webhook.
      Note: Use --include-full-endpoint-URL  in Azure cli to show
            query params.

  â”” For other event handlers appart of webhook write access is needed.
   Âº"microsoft.eventgrid/eventsubscriptions/write"Âº must be true.

Required resource scope differs based on:
-Âºsystem topicÂº: scope of resource publishing the event.
                 The format of the resource is:
                 /subscriptions/{subscrip.-id}/resourcegroups
                 /{resource-group-name}/providers
                 /{resource-provider}/{resource-type}/{resource-name}

                 Ex: subscribe to events on "myacct" storage account:
                 permission:Âº"microsoft.eventgrid/eventsubscriptions/write"Âº
                 scope     : /subscriptions/####/resourcegroups
                             /testrg/providers
                             /microsoft.storage/storageaccounts/myacct

-Âºcustom topicÂº: scope of the event grid topic.
                 Format of the resource is:
                 /subscriptions/{subscription-id}/resourcegroups
                 /{resource-group-name}/providers
                 /microsoft.eventgrid/topics/{topic-name}

                 Ex: subscribe to "mytopic" custom topic
                 permission:Âº"microsoft.eventgrid/eventsubscriptions/write"Âº
                 scope     : /subscriptions/####/resourcegroups
                             /testrg/providers
                             /microsoft.eventgrid/topics/mytopic

Âºcustom topic publishingÂº
 - authentication value included in http header.
 - custom topics use either:
   -Âºshared access signatureÂº(SAS), recommended.
     HTTP header:
     - "aeg-sas-token: r={resource}â…‹e={expiration}â…‹s={signature}"
                          â””â”€â”€â”€â”¬â”€â”€â”€â”˜
        path for event-grid topic. Ex:
        https://$topic.$region.eventgrid.Azure.NET/eventgrid/API/events

   -Âºkey authenticationÂº: simple programming and is compatible
                          with many existing webhook publishers.
     HTTP header:
     - "aeg-sas-key  : $key"

 - Ex: creates SAS (C#):
   static string buildSharedAccessSignature(
             string resource, datetime expirationutc, string key) {
     const char resource = 'r' , expiration = 'e' , signature = 's';

     string encodedresource = httpUtility.URLencode(resource);
     var culture = cultureinfo.createSpecificCulture("en-us");
     var encodedexpirationutc =
         httputility.URLencode(expirationutc.tostring(culture));

     string unsignedsas =
        $"{resource}={encodedresource}â…‹{expiration}={encodedexpirationutc}";

     using (var hmac = new hmacsha256(convert.frombase64string(key))) {
       string signature = convert.tobase64string(
            hmac.computehash(encoding.utf8.getbytes(unsignedsas)) );
       string encodedsignature = httputility.URLencode(signature);
       string signedsas =
           $"{unsignedsas}â…‹{signature}={encodedsignature}";

       return signedsas;
     }
   }

- event GridÂºRBAC support by user supported actionsÂº:
  - microsoft.eventgrid/*/read
  - microsoft.eventgrid/*/write
  - microsoft.eventgrid/*/delete
  - microsoft.eventgrid/eventsubscriptions          â”  potentially
        /getFullURL/action                          â”œâ”€ secret
  â”€ microsoft.eventgrid/topics/listkeys/action      â”‚  info
  - microsoft.eventgrid/topics/regeneratekey/action â”˜

<hr/>
<span xsmall>Event filtering for subscriptions</span>

- filtering Options:
  - event types
  - subject begins with or ends with
  - advanced fields and operators

  "filter": {     â† ÂºEvent type filterÂº:
    "includedeventtypes": [
      "microsoft.resources.resourcewritefailure",
      "microsoft.resources.resourcewritesuccess"
    ]
  }

  "filter": {     â† Âºsubject filteringÂº
    "subjectbeginswith": "/BLOBservices/default/containers/"
                         "mycontainer/log",
    "subjectendswith": ".jpg"
  }

  "filter": {     â† Âºadvanced filteringÂº JSON Example:
    "advancedfilters": [
      {
        "operatortype": "numberGreaterThanOrEquals", *1
        "key"         : "data.key1",                 *2
        "value"       : 5
      }, {
        "operatortype": "stringContains",            *1
        "key"         : "subject",                   *2
        "values"      : ["container1", "container2"]
      }
    ]
  }
  *1: OPERATOR TYPE
      NUMBER OPERATORS            STRING OPERATORS    boolean OPERATOR
      -------------------------   ----------------    ----------------
      numbergreaterthan           stringcontains      boolequals
      numbergreaterthanorequals   stringbeginswith
      numberlessthan              stringendswith
      numberlessthanorequals      stringin
      numberin                    stringnotin
      numbernotin                 ^
                                  case-INsensitive

  *2: AVAILABLE KEYS
      EVENT GRID SCHEMA        CLOUD EVENTS SCHEMA
      ------------------       -------------------
      id                       eventid
      topic                    source
      subject                  eventtype
      eventtype                eventtypeversion
      dataversion
      event data               event data
      ^                        ^
      For custom input schema, use the event data fields
      (ex: data.key1).

      values can be: number | string | boolean | array

  RÂºLimits:Âº
    - five advanced filters per event grid subscription
      same key can be used in 1+ filters.
    - 512 characters per string value
    - five values for in and not in operators
    - the key can only have one level of nesting (like data.key1)
    - custom event schemas can be filtered only on top-level fields
<hr/>
<span xsmall>Create custom events</span>

$Âº$ az group create \                 Âºâ† create resource group
$Âº  --name   group01 \                Âº
$Âº  --location westus2                Âº

$Âº$ az provider register \            Âºâ† enable (if not yet done)
$Âº  --namespace microsoft.eventgrid   Âº  event grid resource provider.
$Âº                                    Âº  (Wait a moment)

$Âº$ az provider show \                Âºâ† Check status
$Âº  --namespace microsoft.eventgrid \ Âº
$Âº  --query                           Âº
$Âº(Expected output)                   Âº
$Âºâ†’ "registrationstate"               Âº

$Âº$ az eventgrid topic create \       Âºâ† create custom topic
$Âº  --name $topicname \               Âº  to posts events to
$Âº  -l westus2 -g group01             Âº

$Âº$ TURI="https://raw.githubusercontent.com" Âº
$Âº$ TURI="${TURI}/Azure-samples"             Âº
$Âº$ TURI="${TURI}/Azure-event-grid-viewer"   Âº
$Âº$ TURI="${TURI}/master/Azuredeploy.JSON"   Âº

$Âº$ az group deployment create \      Âºâ† deploy (test) web-app
$Âº  --resource-group group01 \        Âº  message endpoint
$Âº  --template-uri TURI      \        Âº
$Âº   --parameters sitename=$SITENAME \Âº â† Destination endpoint
$Âº   hostingplanname=viewerhost       Âº
$Âº (wait some minutes)                Âº
( Test deployed web-app navigating to:
  https://$SITENAME.Azurewebsites.net )

$Âº$ az eventgrid \                    Âºâ† subscribe to topic
$Âº   event-subscription create \      Âº
$Âº   -g group01 \                     Âº
$Âº   --topic-name $topicname \        Âº
$Âº   --name demoviewersub \           Âº
$Âº   --endpoint $endpoint             Âºâ† endpoint must include
                                         the suffix /API/updates/
                                                    â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
      https://$SITENAME.Azurewebsites.NET/API/updates/â† Exâ”˜
                                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
(test that initial subscription-validation event has been sent
 to end-point.)

ÂºUssageÂº:
- let's trigger an event to see how event grid distributes the
  message to your endpoint.

$Âº$ endpoint=$(az eventgrid \      Âºâ† get URL for custom topic.
$Âº             topic show \        Âº
$Âº             --name $topicname \ Âº
$Âº             -g group01 \        Âº
$Âº             --query "endpoint" \Âº
$Âº             --output tsv)       Âº

$Âº$ key=$(az eventgrid \           Âºâ† get key for custom topic.
$Âº        topic key list \         Âº
$Âº        --name $topicname \      Âº
$Âº        -g group01 \             Âº
$Âº        --query "key1" \         Âº
$Âº        --output tsv)            Âº

   sample event data:

$Âº$ d01=$(date +%y-%m-%dt%h:%m:%s%z)Âº
$Âº$ event=$(cat <<EOF               Âº
$Âº[                                 Âº
$Âº {                                Âº
$Âº   "id": "'"$random"'",           Âº
$Âº "eventtype": "recordinserted",   Âº
$Âº "subject": "myapp/vehicles",     Âº
$Âº "eventtime": "$d01",             Âº
$Âº "data": {                        Âº
$Âº     "make" : "ducati",           Âº
$Âº     "model": "monster"           Âº
$Âº   },                             Âº
$Âº "dataversion": "1.0"             Âº
$Âº }                                Âº
$Âº]                                 Âº
$ÂºEOF                               Âº
$Âº)                                 Âº

$Âº$ curl -x post \                  Âº
$Âº  -h "aeg-sas-key: $key" \        Âº
$Âº  -d "$event" $endpoint           Âº

(Check in subscriber (test web app endpoint) the event just sent)

$Âº$ az group delete --name group01  Âºâ† Clean up
</pre>
</div>
<div groupv>
<span title>"Kafka" Event hub</span>
<pre zoom labels="event_stream,kakfa,azure,">
<span xsmall>Event hub Overview</span>
- Sort of "managed kafka cluster".
  with capture, auto-inflate, and geo-disaster recovery.

- Use-case: ingests and process of Big-Data events,
  with low latency and high reliability.

- namespace: unique scoping container, referenced by its
            fully qualified domain name.
            - 1+ event hubs or kafka topics.

BÂºevent hubs for apache kafkaÂº:
  - event hubs using HTTPS, AMQP 1.0, kafka 1.0+
  - It allow applications like "mirror maker" or
    framework like "kafka connect" to work clusterless
    with just configuration changes.

Âºevent publisher(producer)Âº
- shared access signature (SAS) token used to identify to the event
  hub.
- Publisher can have a unique identity, or use a common sas token.
- typically, There is a publisher per client/client-group.

Âºpublishing an eventÂº
 -event data contains:
  - offset
  - sequence number
  - body
  - user properties
  - system properties

- .NET client libraries and classes exists.
- For other platforms you can use any AMQP 1.0 client
  (apache qpid,...).
- events can be published individually or batched.
- a single publication (event data instance) has aÂºlimit of 1 MBÂº.
  regardless ofÂºwhether it is a single event or a batchÂº
-BÂºbest practiceÂº: publishers must beÂºunaware of partitionsÂºwithin
   the event hub and toÂºonly specify a partition keyÂº, or their
   identity via their SAS token.
   - while partitions are identifiable and can be sent to directly,
     sending directly to it is not recommended with
     higher level constructs recommended.

- AMQP requires the establishment of a persistent bidirectional socket
  in addition to transport level security (tls) or ssl/tls.
  It has higher network costs when initializing the session.
  It has higher performance (than HTTPS) for frequent publishers.
- https requires additional ssl overhead for every request.
  image showing the interaction of partition keys and event hub.

-BÂºall events sharing a partition key value are delivered in orderÂº,
 BÂºand to the same partitionÂº.
-  If partition keys are used with publisher policies, then the
   identity of the publisher and the value of the partition key must
   match.

- event hubs enables granular control over event publishers through
  publisher policies:
  ^
  run-time feature designed to facilitate large numbers of
  independent event publishers:
  - each publisher uses its own unique identifier
    when publishing events to an event hub like:

    //[my namespace].servicebus.windows.NET/[event hubname]
    /publishers/[my publisher name]
                 ^
         no need to create publisher names ahead of time
         but they must match the sas token used when publishing an event
         in order to ensure independent publisher identities.

BÂºEvent hubs captureÂº:
  â”” automatically capture the streaming data in event hubs and
    save it to:
    - BLOB storage account
    - Azure data lake.
  â”” Tunnable Setting (in A.portal):
    - minimum size
    - time window

BÂºLOG partitionÂº
  â”” ordered sequence of events held in an event hub.
    (similar to a "commit log")
  â”” event hubs provides message streaming through a
   Âºpartitioned consumer patternÂº in which each consumer only reads
   a specific subset, or partition, of the message stream.
  â”” It provides for horizontal scale and other stream-focused features
  BÂºunavailable in queues and topicsÂº
  â”” event hubs RÂºretains data for a configured retention timeÂº applying
  RÂºto all partitionsÂº:
    -BÂºevents expire on a time basis; you cannot explicitly delete themÂº

  â”” partitions are independent and grow at different rates.

  â”” the number of partitions is set at (hub?) creation:
    must be in the range [2, 32]
    (contact event hubs team to pass the 32 limit).

BÂºnumber of partitions in an event hub directly relates to the numberÂº
BÂºof concurrent readers expected.Âº

-ÂºPartition keyÂº
  - sender-supplied value.
  - Can be used to map incoming event data into specific partitions
    through a static BÂºhashing function returning the final partitionÂº
    assignment.
    -BÂºif partition key is not provided, round-robin is usedÂº

  - Examples of partition keys could be a per-device or user unique identity

ÂºSAS TOKENSÂº
- SAS: shared access signatures available namespace|event-hub level.
  - a sas token is generated from a sas key as an SHA(URL).
- using (key-name ("policy"), token) event hubs can regenerate
  the hash to authenticate sender.
- sas tokens for event publishers are created with only send
  privileges on a specific event hub.

ÂºEVENT CONSUMERSÂº
- entity reading event data from an event hub
- all consumers connect via the AMQP 1.0 session:
  - events delivered as they become available.
    (No need to poll on client side)

ÂºCONSUMER GROUPSÂº
- used to enable the publish/subscribe mechanism.
- a consumer group is a view (state, position, or offset) of
  an entire event hub.
- consumer groups enable different applications to have a
  different view of the event stream, and to read the stream
  independently at their own pace and with their own offsets:
  -BÂºin a stream processing architecture, each downstream applicationÂº
   BÂºequates to a consumer groupÂº
  - To write event data to long-term storage, then that storage
    writer application is a consumer group.

- there is always a default consumer group in an event hub,
  and up to 20 consumer groups (standard tier) can be created.

- at most 5 concurrent readers on a partition per consumer group.
  BÂº(only one active receiver on a partition per consumer groupÂº
  BÂº is recommended, otherwise duplicated messages will rise).Âº

- consumer group URI convention examples:
  //[my namespace].servicebus.windows.NET/[event hub name]/[consumer_group #1]
  //[my namespace].servicebus.windows.NET/[event hub name]/[consumer_group #2]

Âºstream offsetsÂº
- position of an event within a partition. ("consumer-side cursor").
  Can be specified as:
  - timestamp
  - offset value.

Âº(Consumer status) checkpointingÂº
- process by which readers mark or commit their position within
  a partition event sequence.
- consumer reader must keep track of its current position in the
  event stream, and inform the checkpointing service when it
  considers the data stream complete.
  - At partition disconnectâ†’reconnect, it will continue reading at
    checkpointed offset.
- checkpointing can be used to:
  - mark events as "complete"
  - provide failover resiliency

ÂºCOMMON CONSUMER TASKSÂº
- connect to a partition:
  - common practice: use a "leasing mechanism" to coordinate reader
    connections to specific partitions.
- checkpointing, leasing, and managing readers are simplified by
  using the eventProcessorHost class for .NET clients.

- throughput capacity of event hubs is controlled byÂºthroughput unitsÂº
  Âº(pre-purchased units of capacity)Âº
  a throughput unit includes the following capacity:
  - ingress: up to 1 mb per second or 1000 events per second
            (ingress is throttled and a serverbusyexception is returned is
             excedded)
  -  egress: up to 2 mb per second or 4096 events per second.

  -BÂºthroughput units are pre-purchased and are billed per hourÂº
  - up to 20 throughput units can be purchased for a namespace
    More throughput units in blocks of 20, up to 100 throughput units,
    can be purchased by contacting Azure support.

<hr/>
<span xsmall>capture events</span>
- Azure event hubs enables you to automatically capture the
  streaming data in event hubs in an Azure BLOB storage | Azure data lake
  in the same/different region as event hub,
  optionally including a time or size interval.

- apache avro format used for capture:
  - compact, fast, binary format with inline schema.

- capture windowing
  -  window has (minimum size, time) configuration with a
     "first wins policy"

- storage naming convention:
  {namespace}/{eventhub}/{partitionid}
  /{year}/{month}/{day}/{hour}/{minute}/{second}
   ^
   date values are padded with zeroes
<span xsmall>Authâ…‹Security model</span>
- Azure event hubs security model:
  - only clients presenting valid credentials can send data to the hub.
  - a client cannot impersonate another client.
  - a rogue client can be blocked from sending data to an event hub.
  - an event publisher defines a virtual endpoint for an event hub and
    Shared Secret Signature (SAS) is used to Auth.
  - each client is assigned a unique token (stored locally)
    - a client can only send to one publisher with a given token
  - it is possible(not recommended)to equip devices with tokens
    granting direct access to an event hub. device will be able
    to send messages directly and will RÂºNOT be subject to throttling,Âº
  RÂºneither can it be  blacklisted.Âº

- to create a sas key:
  - service automatically generates a 256-bit sas key named
   ÂºrootmanagesharedaccesskeyÂº when creating an event hubs namespace.
    This rule has an associated pair of primary and secondary keys
    that grant send, listen, and manage rights to the namespace.
  -  additional keys can be created.
    (a new key per event-hub recomended). Ex C# code:

    // create namespace manager.
    string servicenamespace = "your_namespace";
    string namespacemanagekeyname = "rootmanagesharedaccesskey";
    string namespacemanagekey = "your_root_manage_shared_access_key";
    URI uri = servicebusenvironment.createserviceuri("sb",
    servicenamespace, string.empty);
    tokenprovider td =
    tokenprovider.createsharedaccesssignaturetokenprovider(namespacemanage
    keyname, namespacemanagekey);
    namespacemanager nm = new namespacemanager(namespaceuri,
    namespacemanagetokenprovider);

    // create event hub with a sas rule that enables sending to that
    event hub
    eventhubdescription ed = new eventhubdescription("my_event_hub") {
    partitioncount = 32 };
    string eventhubsendkeyname = "eventhubsendkey";
    string eventhubsendkey =
    sharedaccessauthorizationrule.generaterandomkey();
    sharedaccessauthorizationrule eventhubsendrule = new
    sharedaccessauthorizationrule(eventhubsendkeyname, eventhubsendkey,
    new[] { accessrights.send });
    ed.authorization.add(eventhubsendrule);
    nm.createeventhub(ed);

Âºgenerate tokensÂº (one per client)
-BÂºtokens lifespan: resembles/exceeds that of clientÂº
  public static string
  sharedaccesssignaturetokenprovider.
    getsharedaccesssignature(
      string   keyname,
      string   sharedaccesskey,
      string   resource,
      timespan tokentimetolive)

    URI should be specified as :
    //$namespace.servicebus.windows.NET/$event_hub_name
    /publishers/$publisher_name
                ^ different for each token.

    this method generates a token with the following structure:

    sharedaccesssignature:
    sr={URI}â…‹
    sig={hmac_sha256_signature}â…‹
    se={expiration_time}â…‹    â† seconds since 1970-01-01
    skn={key_name}
    Ex:
    sharedaccesssignature
    sr=contosoâ…‹sig=npzdnn%2gli0ifrfjwak4mkk0rqab%2byjult%2bgfmbhg77a%3dâ…‹
    se =1403130337â…‹
    skn=rootmanagesharedaccesskey

- sending data must occur over an encrypted channel.

- blacklisting clients by token is possible in case of leak.

Âºback-end applications (consumer) authenticationÂº
An event hubs consumer group is equivalent to a subscription
to a service bus topic.
- a client can create a consumer group if the request
  token grants "manage privileges" for target event-bus|namespace
- a client is allowed to consume data from a consumer group if
  the receive request-token grants receive rights on target
  consumer-group|event-hub|namespace.

- no support yet for sas rules yet for individual subscriptions.
  or event hubs consumer groups.
  - common sas-keys can be used to secure all consumer groups.
<hr/>
<span xsmall>Create event-hub (cli) </span>

  $ az login
  $ az account set --subscription myAzuresub
  $ az group create --name $group --location eastus
  $ az eventhubs namespace create \
    --name $event hubs namespace \
    --resource-group $group \
    -l eastus
  $ az eventhubs eventhub create \
    --name $event hub name \
    --resource-group $group \
    --namespace-name $event hubs namespace

<span xsmall>event hubs .NET API</span>

- primary classes: ( microsoft.Azure.eventhubs nuget package )
                     ^install-package microsoft.Azure.eventhubs
  -ÂºeventhubclientÂº: AMQP communication channel
  -Âºeventdata     Âº: Represents an event
                     used to publish messages to an event hub.
    ^namespace microsoft.Azure.eventhubs.

Âº.NET event hubs clientÂº
Ex:
private const string eventhubconnectionstring =
   "event hubs namespace connection string";

private const string eventhubname = "event hub name";

var connectionstringbuilder = new
  eventhubsconnectionstringbuilder(eventhubconnectionstring) {
      entitypath = eventhubname
  };
eventhubclient = eventhubclient.
  ÂºcreatefromconnectionstringÂº(              â† Instantiate client
     connectionstringbuilder.tostring());


for (var i = 0; i Ë‚ nummessagestosend; i++) {
    var message = $"message {i}";
    await eventhubclient
     Âº.sendasyncÂº(                           â† send event
        new
          eventdata(encoding.utf8.getbytes(message)));
}

Âºevent serializationÂº
- eventdata class has two overloaded constructors taking
  bytes or byte-array, representin the payload.
  To convert JSON to payload use:
  encoding.utf8.getbytes()

Âºpartition keyÂº
- set in .NET partitionsender.partitionid
- Applies when: sending event data.
- Optional: round-robin used if not set.
   - Round-robin is prefered for high availability since
     data will become unavailable if target partition is down,
     at the cost of loosing consistency (pinning ordered events to
     a partition id).

- Par.Key: hashed value to produce a partition assignment.
-  used to set ther partition

Âºbatch event send operationsÂº
- sending events in batches can help increase throughput.
  eventhubclient.createbatch (.NET) sets data objects to be sent
  in next sendasync call. sendasync returns a task object.
  retrypolicy class can be used to control client retry options.
  It also can be used to ensure that the batch does not
  exceed 1 mb with the "tryadd".

Âºevent consumersÂº
  eventhubclient.eventprocessorhost class processes data
  from event hubs needed for event readers.
  It is thread-safe, multi-process and also provides
  checkpointing and partition lease management.
  - It also implements an Azure storage-based checkpointing
    mechanism.
  To use it, implement the interface:
  Ë‚Ë‚ieventprocessorËƒËƒ
  -------------------
  openasync
  closeasync
  processeventsasync
  processerrorasync

  Ex:
  var eventprocessorhost = new eventprocessorhost(
                             eventhubname,
                             partitionreceiver.defaultconsumergroupname,
                             eventhubconnectionstring,
                             storageconnectionstring,
                             storagecontainername);
  await
    eventprocessorhost.
      registereventprocessorasync  â† register instance in runtime
        Ë‚simpleeventprocessorËƒ()
  ;

  at this point, client attempts to acquire a lease on every
  partition in the event hub using a "greedy" algorithm.
  - leases have a timeframe and must be renewed.

BÂºbecause event hubs does not have a direct concept of Âº
BÂºmessage counts, average cpu utilization is often the best Âº
BÂºmechanism to measure back end or consumer scale.Âº
  - if publishers publish faster than consumers can process,
    the cpu increase on consumers can be used to cause an
    auto-scale on worker instance count.


Âºpublisher revocationÂº
 - event hubs publisher revocation block specific publishers
   from sending events.
   (token compromised, bug detected,...)
   It will be the  publisher's identity, part of the sas
   token, that will be blocked.
</pre>
</div>

<div groupv>
<span title>A.Service bus</span>
<pre zoom labels="06,esb,service_bus,azure">
<span xsmall>Service bus summary</span>
- Message bus decoupling and communicating applications.
- It's NOT an ESB. Logic App is used for that mission, allowing to
  transform/filter messages. Logic App can be used to process messages
  converting formats, and finally placing them in a Service bus.
- Decouple applications from each other.
- message is in binary format, which can contain JSON, XML, text,....

- namespace: scoping container for all messaging components.
  (Multiple queues, topics)

- Messages are sent to/received from queues.

- Messages inâ€¯queuesâ€¯areÂºordered and timestampedÂºon arrival.
  Once accepted, message is held safely in redundant storage.
  They are delivered in pull mode (on request).

-ÂºtopicsÂº: useful in publish/subscribe scenarios.
  (vs queue, used for point-to-point communication)

  - Topic 1 â†â†’ N Subscriptions
                 -------------
                 - Have entity
                 - durably created
                 - optionally expire/auto-delete.

  - rules and filters allow to trigger optional actions,
    filter specified messages, and set or modify message
    properties.


 ADVANCED FEATURES
 ---------------+--------------------------------------------------------
 2 Message      | Sessions enable joint and ordered handling of unbounded
   sessions     | sequences of related messages used for reliable FIFO
 ---------------+--------------------------------------------------------
 3 Auto         | chain queue|subscription to another queue|topic
   forwarding   | in namespace.
                | Service Bus automatically will remove messages
                | placed in first queue|subscription (source)
                | and puts in second one (destination)
 ---------------+--------------------------------------------------------
 4 dead-letter  | hold messages un-deliverable to any receiver,
   queue(DLQ)   | or that cannot be processed.
                | They can be removed from the DLQ or inspected.
 ---------------+--------------------------------------------------------
 5 Scheduled    |
   delivery     |
 ---------------+--------------------------------------------------------
 6 Message      | Useful when subcriptor can not process the message at this
   deferral     | moment.  message will remain in the queue|subscription, but
                | it is set aside.
 ---------------+--------------------------------------------------------
 7 Batching     | enables queue|topic client accumulate
                | pending to send messages for a period,
                | then send in a single batch.
 ---------------+--------------------------------------------------------
 8 Transactions | group 2+ operations into an execution scope,
                | against a single messaging entity
                | (queue, topic, subscription)
 ---------------+--------------------------------------------------------
 9 Filtering    | Subscribers can define filtered-in messages
   and actions  | using 1+ named-subscription-rules.
                | Each matching rule produces a message copy
                | that may be differently annotated.
 ---------------+--------------------------------------------------------
10 Auto-delete  | specify idle interval after which
   on idle      | the queue is automatically deleted.
                | 5 minutes or greater.
 ---------------+--------------------------------------------------------
11 Duplicate    | allow sender to re-send same
   detection    | message, and the queue or topic
                | discards any duplicate copies.
                | (If there were doubts about first outcome)
 ---------------+--------------------------------------------------------
12 SAS, RBAC,   | security protocols supported out of the box
   and Managed  |
   identities   |
 ---------------+--------------------------------------------------------
13 Geo-disaster | Continue operation over different
   recovery     | region or datacenter.
 ---------------+--------------------------------------------------------
14 Security     | support for standard AMQP 1.0 and
                | HTTP/REST protocols.
 ---------------+--------------------------------------------------------
<hr/>
<span xsmall>event vs. message services</span>
important distinction: event vs message.

-ÂºEventÂº:
  - lightweight notification of state change.
    publisherRÂºhas no expectation about how the event is handledÂº.
    Consumer decides what to do with them.
    Can be classified into:
    - Discrete: data has info about what happened but
                doesn't have the full data info.
                ("file changed",...)
                Suitable for serverless solutions that scale.

    - Series  : report a condition and are analyzable.
              - time-ordered and interrelated.

-ÂºMessageÂº
  - raw data to be consumed or stored.
  - The message contains the full data that triggered the message pipeline.
  - publisher has an expectation about how the consumer handles the message.
  BÂºA contract exists between end-pointsÂº

ÂºComparison of servicesÂº
Service       Purpose        Type                  When to use
--------------------------------------------------------------------------
Event Grid    Reactive       Event distribution    React to status changes
              programming    (discrete)
--------------------------------------------------------------------------
Event Hubs    Big data       Event streaming       Telemetry and distributed
              pipeline       (series)              data streaming
--------------------------------------------------------------------------
Service Bus   High-value      Message              Order processing,
              enterprise                           financial transactions
              messaging
--------------------------------------------------------------------------

BÂºQUEUESÂº
  - FIFO message delivery to 1+ competing consumers.
  - achieve "temporal decoupling" of act-react. (producers - consumers)
  - Create queues through portal, PowerShell, CLI, or
    ARM templates.
  - send and receive messages with QueueClient object.

  - Receive modes:
    -ÂºReceiveAndDeleteÂº:
      Service Bus marks the message as consumed
      at first request.
      Simplest scenario, when application can
      tolerate not processing a message on exception.
    -ÂºPeekLockÂº:  two-stage receive operation:
      -â˜resilient to consumer chrases.
        1) client "read" request timeout-locks message in queue to
           current client
        2) client calls CompleteAsync and message marked in queue
           as consumed. (Happy path)
        2) client calls  AbandonAsync and message is unlocked.

  - Corner scenario:
  application crashes after processing the message, but before
  CompleteAsync request is issued.
  if duplicate processing is not tolerated, additional logic is
  required in the application to detect such duplicates,
  based upon the MessageId property of the message.
  "Exactly Once" (vs "At least once") processing.

BÂºTopics and subscriptionsÂº
  - one-to-many in publish(to topic)/subscribe (to topic) pattern.
   A topic subscription resembles a virtual queue receiving (copies of)
   the messages that are sent to the topic.

  - Create topics and subscriptions:
    TopicClient class used to send messages.
    SubscriptionClient (similar to queues) used to receive messages.
    Create the subscription client instance, passing the name of the topic,
    the name of the subscription, and (optionally) the receive mode as
    parameters.

  - Rules and actions
    subscriptions can be configured to find messages with desired properties
    and then perform certain modifications to those properties.

    SQL filter expression is optional; without a SQL filter expression,
    any filter action defined on a subscription will be performed on all
    the messages for that subscription.

BÂºMESSAGESÂº
- payload :  (can be empty if metadata is rich enough)
             Blind to Service Bus
- metadata:  key-value pair properties
             - User   properties.
             - broker properties:
               - predefined
               - control broker functionality or
                 map to standardized items.

BÂºPREDEFINED PROPERTIES TABLEÂº
  - used with all official client APIs and in the BrokerProperties
    JSON object of the HTTP protocol mapping.

  - (equivalent AMQP protocol level listed in parentheses)
    + CorrelationId (correlation-id)        â”
    + MessageId (message-id)                â”‚   used to help applications
    + ReplyTo (reply-to)                    â”œâ”€ â†route messages to particular
    + ReplyToSessionId (reply-to-group-id)  â”‚   destinations.
    + To (to)                               â”‚
    + SessionId (groupâ”€id)                  â”˜

    - ContentType (content-type)
    - DeadLetterSource
    - DeliveryCount
    - EnqueuedSequenceNumber
    - EnqueuedTimeUtc
    - ExpiresAtUtc (absolute-expiry-time)
    - ForcePersistence
    - Label (subject)
    - LockedUntilUtc
    - LockToken
    - PartitionKey
    - ScheduledEnqueueTimeUtc
    - SequenceNumber
    - Size
    - State
    - TimeToLive
    - ViaPartitionKey

  - message model doesn't depend on underlying protocol (HTTPS/AMQP/...)

BÂºMESSAGE ROUTING AND CORRELATIONÂº

- PATTERNS:
  -ÂºSimple request/replyÂº:
    publisher â†’  request  â†’ queue1
                 -------
                 MessageId Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â”
                 ReplyTo Â·Â·Â·Â·â”                             Consumer C&P
                             v                             into response
    publisher â†           â† publisher   â† response         Â·
                            owned queue   --------         Â·
                                          CorrelationId â†Â·Â·â”˜
                                          ^
                                          One message can yield
                                          multiple replies identified
                                          by the CorrelationId

  -ÂºMulticast request/replyÂº:
    variation of request/reply
    publisher â†’ message â†’ topic â†’ multiple subscribers
                                  become eligible .

  -ÂºMultiplexingÂº of streams in Sessions to single queue|subscription

    SessionId used to identify receiver-"queue|subscription"-session.

    "queue|subs." "holding" SessionId lock receive the message.

  -ÂºMultiplexed request/replyÂº: session feature

    multiplexed replies, allowing several publishers to
    share a single reply queue.

    publisher instruct consumer(s) to copy:
    request.SessionId  â†’ response.ReplyToSessionId ,

    publisher will wait for session response.ReplyToSessionId

Routing inside Service Bus namespace:
  - auto-forward chaining and topic subscription rules.
Routing across Service Bus namespace:
  - Azure LogicApps.

RÂºWARNÂº: "To" property is reserved for future use
         Applications must implement routing based on user
         properties.

BÂºPAYLOAD SERIALIZATIONÂº
- Payload transit: opaque, binary block.
- ContentType property used to describe payload
  (MIME content-type recomended)
  Ex: application/JSON;charset=utf-8.

- When using AMQP protocol, the object is serialized
  into an AMQP object. The receiver can retrieve those
  objects with the GetBody() method, supplying the
  expected type.

- With AMQP, objects are serialized into an
  AMQP graph of ArrayList and IDictionaryË‚string,objectËƒ
  objects. Any AMQP client can decode them.

-BÂºapplications should take explicit control of object serializationÂº
 BÂºand turn their object graphs into streams before including       Âº
 BÂºthem into a message and do the reverse on the receiver side.     Âº

<hr/>
PRE-SETUP: ÂºMicrosoft.Azure.ServiceBus NuGetÂº package required

STEP 1) Prepare Azure Queue Resource
  $ az group create \                   â† Create resource group
    --name myResourceGroup \
    --location eastus

  $ namespaceName=myNameSpace$RANDOM
  $ az servicebus namespace create \    â† Create Service Bus
     --resource-group myResourceGroup \   messaging namespace
     --name $namespaceName \
     --location eastus

  $ az servicebus queue create \        â† Create Service Bus queue
     --resource-group myResourceGroup \
     --namespace-name $namespaceName \
     --name OÂºmyQueueÂº

  $ connectionString=$(                 â† Get connection string
      az servicebus namespace             for namespace
      authorization-rule keys list \
     --resource-group myResourceGroup \
     --namespace-name $namespaceName \
     --name RootManageSharedAccessKey \
     --query primaryConnectionString --output tsv)

  Write down connection-string and queue-name.

STEP 2) .NET publisher code:

    using System.Text;
    using System.Threading;
    using System.Threading.Tasks;
    using Microsoft.Azure.ServiceBus;

    ...
    const string ServiceBusConnectionString = "connectionString";
    const string QueueName = OÂº"myQueue"Âº;
    static IQueueClient BÂºqueueClientÂº;
    ...
    Main() {
      ...
      MainAsync().GetAwaiter().GetResult();    â† Add line
      ...
    }

    static async Task MainAsync() {
        const int numberOfMessages = 10;
      BÂºqueueClientÂº = new QueueClient(ServiceBusConnectionString, QueueName);
        Console.WriteLine("Press ENTER ...");

        await OÂºSendMessagesAsyncÂº(numberOfMessages);   //  â† Send messages.

        Console.ReadKey();

        await BÂºqueueClientÂº.CloseAsync();
    }

    static async Task SendMessagesAsync(int numberOfMessagesToSend) {
      /*--*/try {
      for (var i = 0; i Ë‚ numberOfMessagesToSend; i++) {
        string messageBody = $"Message {i}";        // â† Create new message to send to the queue.
        var message = new Message(
              Encoding.UTF8.GetBytes(messageBody)
        );
        await BÂºqueueClientÂº.SendAsync(message);
      }
      /*--*/} catch (Exception exception) {
      Console.WriteLine($"{exception.Message}");
      /*--*/}
    }


    After run, check queue in Azure portal:
    ... â†’ namespace Overview window â†’ queue Name
    â†’ queue Essentials screen.
    Check that "Active Message Count" is 10.

STEP 3) Write code to receive messages to the queue
  Main() {
    ...
    MainAsync().GetAwaiter().GetResult();
    ...
  }

  static async Task MainAsync() {
    queueClient = new QueueClient(
                    ServiceBusConnectionString, QueueName);
    Console.WriteLine("Press ENTER....");
    RegisterOnMessageHandlerAndReceiveMessages();
    Console.ReadKey();
    await BÂºqueueClientÂº.CloseAsync();
  }

  Directly after the MainAsync() method, add the following method
  that registers the message handler and receives the messages sent by
  the sender application:

  static void RegisterOnMessageHandlerAndReceiveMessages() {
    // Configure the message handler options in terms of exception
    // handling, number of concurrent messages to deliver, etc.
    var messageHandlerOptions = new MessageHandlerOptions(ExceptionReceivedHandler) {
      // Maximum number of concurrent calls to the callback
      // ProcessMessagesAsync(), set to 1 for simplicity.
      // Set it according to how many messages the application
      // wants to process in parallel.
      MaxConcurrentCalls = 1,

      // Indicates whether the message pump should automatically
      // complete the messages after returning from user callback.
      // False below indicates the complete operation is handled
      // by the user callback as in ProcessMessagesAsync().
      AutoComplete = false
    };

    // Register the function that processes messages.
  BÂºqueueClientÂº.RegisterMessageHandler(
      ProcessMessagesAsync, messageHandlerOptions);
  }

  Directly after the previous method, add the following
  ProcessMessagesAsync() method to process the received messages:

  static async Task
    ProcessmsgsAsync(
       msg msg,
       CancellationToken token // â† Use it to fetch queueClient has closed.
    )                          // In that case CompleteAsync()|AbandonAsync()|...
    {                          // can be skipped avoiding unnecessary exceptions.
      Console.
        WriteLine($"Received "
         $"SequenceNumber:{msg.SystemProperties.SequenceNumber} "
         $"Body:{Encoding.UTF8.GetString(msg.Body)}");

      await queueClient.       // â† Complete to avoid receiving again. queue client
        CompleteAsync(         //   in ReceiveMode.PeekLock mode (default)
          msg.SystemProperties.LockToken);
  }

  // BÂº:handling exceptionsÂº
  static Task ExceptionReceivedHandler(
    ExceptionReceivedEventArgs exceptionReceivedEventArgs)
  {
      Console.WriteLine($"exception: {exceptionReceivedEventArgs.Exception}.");
      var context = exceptionReceivedEventArgs.ExceptionReceivedContext;
      Console.WriteLine("Exception context for troubleshooting:");
      Console.WriteLine($"- Endpoint: {context.Endpoint}");
      Console.WriteLine($"- Entity Path: {context.EntityPath}");
      Console.WriteLine($"- Executing Action: {context.Action}");
      return Task.CompletedTask;
  }
</pre>

<pre zoom labels="06,queue,">
<span xsmall>Storage queues</span>
A.Queue storage: service for storing large numbers of messages that
                 can be accessed from anywhere in the world via
                 authenticated calls using HTTP or HTTPS.
ÂºLimitsÂº
- queue message: Ë‚= 64 KB
  - Use service bus queue for larger sizes
    - up to 256 KB in standard tier
    - up to   1 MB in premium tier
  - service bus topics (vs queue) also allows
    for message filtering before processing.

- queue length : "millions of messages"
                 up to storage account limit

- Use-cases:
  - Creating a backlog of work to process asynchronously
  - Passing messages from an Azure web role to an Azure worker role

- Queue service components

- URL format:
    https://${storage_account}.queue.core.windows.NET/${queue}
                                                      â””â”€â”€â”¬â”€â”€â”€â”˜
                                                    RÂºWARNÂº:
                                 name must be all-lowercase
<hr/>
PRE-SETUP:
 - NuGet packages:
   - Azure SDK for .NET
   - Microsoft Azure Storage Library for .NET
     (^ already included in Azure SDK, but
     BÂºit is recommend to install againÂº
     BÂºfrom NuGet to use latest versionÂº)
       ODataLib dependencies will be resolved to NuGet packages
       (vs WCF Data Services).

- Azure Storage resource set up
- Azure Storage resource connection string
  used to configure endpoints and credentials
  for accessing storage services.
  - Best Pattern: Keep it in a configuration-file:
    Visual Studio â†’ Solution Explorer â†’ app.config file:
    Ë‚configurationËƒ
      Ë‚startupËƒ
        Ë‚supportedRuntime
          version="v4.0"
          sku=".NETFramework,Version=v4.5.2" /Ëƒ
      Ë‚/startupËƒ
      Ë‚appSettingsËƒ                        â†  Add here
       ÂºË‚add key="StorageConnectionString" Âº
       Âº value="DefaultEndpointsProtocol=https;AccountName=account-name;AccountKey=account-key" /ËƒÂº
      Ë‚/appSettingsËƒ
    Ë‚/configurationËƒ

    Ex:
    Ë‚add
      key="StorageConnectionString"
      value="DefaultEndpointsProtocol=https;AccountName=storagesample;AccountKey=GMuzNHjlB3S9itqZJHHCnRkrokLkcSyW7yK9BRbGp0ENePunLPwBgpxV1Z/pVo9zpem/2xSHXkMqTHHLcx8XRA==" /Ëƒ


ÂºCreate the Queue service clientÂº
Here's one way to create the service client:

  //ÂºCREATE QUEUE (IF NOT EXISTANT)Âº
  CloudQueueClient queueClient =            â† used to retrieve queues stored
    storageAccount.CreateCloudQueueClient();  in Queue storage.

  CloudStorageAccount storageAccount =      â† get from input string from config
    CloudStorageAccount.Parse(
      CloudConfigurationManager.
        GetSetting("StorageConnectionString"));

  CloudQueueClient queueClient =           // â† Create the queue client.
    storageAccount.
      CreateCloudQueueClient();

  CloudQueue queue = queueClient.         // â† Retrieve reference to container.
      GetQueueReference("myqueue");

  queue.CreateIfNotExists();              // â† create queue (if needed )

  CloudQueueMessage message =
    new CloudQueueMessage("Hello, World");
  queue.AddMessage(message);              // â† Insert message into queue


  CloudQueueMessage peekedMessage =       // â† Peek next message Âºwithout removingÂº
    queue.PeekMessage();                       from queue
  Console.WriteLine(peekedMessage.AsString);

ÂºChange in place the contents of a queued messageÂº
- Ex: update the status of task/....

  CloudQueueMessage message = queue.      // â† Get from queue
                                GetMessage();
  message.SetMessageContent("...");       // â† Update locally
  queue.UpdateMessage(message,
      TimeSpan.FromSeconds(60.0),         // â† Make invisible for 60 seconds
      MessageUpdateFields.Content              (extra time for client to continue
    | MessageUpdateFields.Visibility);          working on the message locally).
                                               Typically, a retry count is used as well,
                                               if msg is retried more than n times,
                                               it will be deleted protecting against
                                               messages triggers application error
                                               each time it is processed.

 ÂºDE-QUEUE "next message"Âº
  CloudQueueMessage msg02 = queue.        // â† 1) Get the next message
                             GetMessage();        It will becomes invisible to any
                                                  other code and/or client for 30 secs
  ...
  queue.DeleteMessage(msg02);             // â† 2) delete message
                                                  (in less than 30secs)

ÂºAlternative Async-Await codeÂº
  CloudQueueMessage msgOut03 = new CloudQueueMessage("My message");
  await queue.AddMessageAsync(msgOut03); // enqueue the message

  CloudQueueMessage msgIn03 = await queue.GetMessageAsync();
  await queue.DeleteMessageAsync(retrievedMessage);


additional options for de-queuing messages:

The following code
minutes have passed since the call to GetMessages, any messages which
have not been deleted will become visible again.

  foreach (CloudQueueMessage message in
            queue.GetMessages(
              Âº20,                    Âº â† get 20 message in one call
              ÂºTimeSpan.FromMinutes(5)Âº â† Set invisibility to 5 minutes
            )
     ) {
      ... // Process in less than 5 minutes,
      queue.DeleteMessage(message);
  }

ÂºGet the queue length (estimate)Âº

  queue.FetchAttributes(); // Fetch the queue attributes.
  int? cachedMessageCount = queue.
                     ApproximateMessageCount;

ÂºDelete a queueÂº
  queue.Delete();

</pre>
</div>
</div group> <!-- cloud Architecture -->


<div groupv>
<span title>Search Solution</span><br/>
<pre zoom labels="azure,search_engine,">
<span xsmall>Search Overview</span>
- search-as-a-service cloud solution
- APIs and tools for adding aÂºrich search experienceÂº
  over private, heterogenous content in web, mobile,
  and enterprise applications.
BÂºQuery execution is over a user-defined indexÂº

- Build a search corpus containing only your data, sourced from
  multiple content types and platforms.
- AI-powered indexing to extract text and features from
  image files, or entities and key phrases from raw text.
- facet navigation and filters, synonyms, auto-complete,
  and text analysis for "did you mean" auto-corrected search terms.
- Add geo-search for "find near me", language analyzers for
  non-English full text search, and scoring logic for search rank.

-Âºexposed through a simple REST API or .NET SDK that Âº
 Âºmasks the inherent complexity of information retrieval.Âº
- Azure portal provides administration and content
  management support, with tools for prototyping and querying your
  indexes.

ÂºAzure Search How toÂº
Step 1) Provision service
  - Alternatives:
    - A.Portal:
    - A.Resource Management API:
  - Price tiers:
    - free service shared with other subscribers:
    - paid tier: dedicated resources.
      - Scale types:
        - Add Replicas  : handle heavy query loads
        - Add Partitions: grow storage for more documents

Step 2) Create index
(Before uploading searchable content)
- index: database-like table holding your data and
         accepting search queries.
- Developer defines:
  -Âºindex schema to mapÂº to reflect the structure of
                         documents you wish to search for
    (fields like in a database).
    - Created in A.Portal or programmatically using
      .NET SDK or REST API.

Step 3) Load data
- push(SDK/REST API) or pull(external data)  model.
- Indexers automate aspects of data ingestion (connecting to,
  reading, serializing data,..)
  - Indexers are available for Cosmos DB, cloud/VM hosted SQL Database,
  - Indexers can be configured on-demand/scheduled-data-refresh.

Step 4) Search
- search queries can be done through HTTP request to service endpoint

Âºfeature summaryÂº
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Category       â”‚   Features                                                            â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Full           â”‚                                                                       â”‚
â”‚ text search    â”‚ Queries using a supported syntax.                                     â”‚
â”‚ and text       â”‚ Simple query syntax provides logical|phrase search|suffix             â”‚
â”‚ analysis       â”‚   |precedence operators                                               â”‚
â”‚                â”‚ÂºLucene query syntaxÂº:                                                 â”‚
â”‚                â”‚ extensions for fuzzy search, proximity search,                        â”‚
â”‚                â”‚ term boosting, and regular expressions.                               â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Data           â”‚ Azure Search indexes accept data from any source, provided it is      â”‚
â”‚ integration    â”‚ submitted as a JSON data structure.                                   â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ Optionally, for supported data sources in Azure, you can use indexers â”‚
â”‚                â”‚ to automatically crawl Azure SQL Database, Azure Cosmos DB, or Azure  â”‚
â”‚                â”‚ BLOB storage for searchable content in primary data stores. Azure     â”‚
â”‚                â”‚ BLOB indexers can perform document cracking to extract text from      â”‚
â”‚                â”‚ major file formats, including Microsoft Office, PDF, and HTML         â”‚
â”‚                â”‚ documents.                                                            â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Linguistic     â”‚ÂºAnalyzersÂºare components used forÂºtext processing during indexingÂºand â”‚
â”‚ analysis       â”‚Âºsearch operationsÂº There are two types.                               â”‚
â”‚                â”‚ÂºCustom lexical analyzersÂº used for complex search queries             â”‚
â”‚                â”‚ (phonetic matching, regular expressions).                             â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ÂºLanguage analyzersÂº(Lucene or Microsoft) used to intelligently        â”‚
â”‚                â”‚ handle language-specific linguistics including verb tenses, gender,   â”‚
â”‚                â”‚ irregular plural nouns (for example, 'mouse' vs. 'mice'), word        â”‚
â”‚                â”‚ deâ”€compounding, wordâ”€breaking (for languages with no spaces), and     â”‚
â”‚                â”‚ more.                                                                 â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Geoâ”€search     â”‚ Azure Search processes, filters, and displays geographic locations.   â”‚
â”‚                â”‚ It enables users to explore data based on the proximity of a search   â”‚
â”‚                â”‚ result to a physical location.                                        â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ User           â”‚ Search suggestions also works off of partial text inputs in a search  â”‚
â”‚ experience     â”‚ bar, but the results are actual documents in your index rather than   â”‚
â”‚ features       â”‚ query terms.                                                          â”‚
â”‚                â”‚ Synonyms  associates equivalent terms that implicitly expand the      â”‚
â”‚                â”‚ scope of a query, without the user having to provide the alternate    â”‚
â”‚                â”‚ terms.                                                                â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ Faceted navigation is enabled through a single query parameter. Azure â”‚
â”‚                â”‚ Search returns a faceted navigation structure you can use as the code â”‚
â”‚                â”‚ behind a categories list, for selfâ”€directed filtering (for example,   â”‚
â”‚                â”‚ to filter catalog items by priceâ”€range or brand).                     â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ Filters can be used to incorporate faceted navigation into your       â”‚
â”‚                â”‚ application's UI, enhance query formulation, and filter based on      â”‚
â”‚                â”‚ user- or developer-specified criteria. Create filters using the OData â”‚
â”‚                â”‚ syntax.                                                               â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ Hit highlighting applies text formatting to a matching keyword in     â”‚
â”‚                â”‚ search results. You can choose which fields return highlighted        â”‚
â”‚                â”‚ snippets.                                                             â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ Sorting is offered for multiple fields via the index schema and then  â”‚
â”‚                â”‚ toggled at queryâ”€time with a single search parameter.                 â”‚
â”‚                â”‚                                                                       â”‚
â”‚                â”‚ Paging  and throttling your search results is straightforward with    â”‚
â”‚                â”‚ the finely tuned control that Azure Search offers over your search    â”‚
â”‚                â”‚ results.                                                              â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Relevance      â”‚ Simple scoring is a key benefit of Azure Search. Scoring profiles are â”‚
â”‚                â”‚ used to model relevance as a function of values in the documents      â”‚
â”‚                â”‚ themselves. For example, you might want newer products or discounted  â”‚
â”‚                â”‚ products to appear higher in the search results. You can also build   â”‚
â”‚                â”‚ scoring profiles using tags for personalized scoring based on         â”‚
â”‚                â”‚ customer search preferences you've tracked and stored separately.     â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Monitoring     â”‚ â”€ Search traffic analytics are collected and analyzed to unlock       â”‚
â”‚ and reporting  â”‚ insights from what users are typing into the search box.              â”‚
â”‚                â”‚ â”€ Metrics  on queries per second, latency, and throttling are         â”‚
â”‚                â”‚ captured and reported in portal pages with no additional              â”‚
â”‚                â”‚ configuration required. You can also easily monitor index and         â”‚
â”‚                â”‚ document counts so that you can adjust capacity as needed.            â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Tools  for     â”‚ In the portal, you can use the Import data wizard to configure        â”‚
â”‚ prototyping    â”‚ indexers, index designer to stand up an index, and Search explorer to â”‚
â”‚ â…‹ inspection   â”‚ test queries and refine scoring profiles. You can also open any index â”‚
â”‚                â”‚ to view its schema.                                                   â”‚
â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â”‚ Infrastructure â”‚ The highly available platform ensures an extremely reliable search    â”‚
â”‚                â”‚ service experience. When scaled properly, Azure Search offers a 99.9% â”‚
â”‚                â”‚ SLA.                                                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

A.portal â†’ Create Resource â†’ search for "Azure Search"
  â†’ Fill details
   ÂºService name URL endpointÂº: Used for URL endpoint. Ex:
    https://$SERVICE_NAME.search.windows.NET.

   ÂºsubscriptionÂº:
   Âºresource groupÂº:
   Âºhosting locationÂº:
   Âºpricing tier(SKU)Âº: Free|Basic|Standard.
    RÂºWARNÂº: pricing tier cannot be changed once the service is created.
             You need to re-create the service.
    â†’ click "Create"
      â†’ Get anÂºauthorization API-keyÂºandÂºURL endpointÂº
        In the service overview page,
        locate and copy the URL endpoint on
        the right side of the page.

        In the left navigation pane, select
        Keys and then copy either one
        of the admin keys (they are equivalent).



PRE-SETUP: A valid API-key (created in STEP 1) is sent on every request.
           It establishes trust, on a per request basis, between the
           application and service.

-Âºprimary/secondary admin keysÂºgrant full rights
  create/delete indexes, indexers, and data sources.
-Âºquery keysÂº grant read-only access to indexes and documents,
              and are  typically distributed to client apps
              issuing search requests.

Ex: use appsetttings.JSON to retrieve API-key and service name

private static SearchServiceClient
  CreateSearchServiceClient(IConfigurationRoot configuration) {
    string searchServiceName = configuration["SearchServiceName"];
    string adminApiKey       = configuration["SearchServiceAdminApiKey"];
    SearchServiceClientÂºserviceClientÂº= new    // â† Indexes prop. provides all
         SearchServiceClient(                  //   methods needed to "CRUD"
             searchServiceName,                //   Search indexes.
             new SearchCredentials(adminApiKey)//   It also manage connection/s
         );                                    //   (share instance to avoid
    return serviceClient;                      //   many open connections)
}


ÂºDefine Search indexÂº)
- A single call to the Indexes.Create method will create
  the index taking and Index instance as input:
                       â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
  Initialize it as follows:â†â”€â”˜
  - SetÂºNameÂº  prop.
  - SetÂºFieldsÂºprop.: Field array
                      FieldBuilder.BuildForType  can be used
                      passing a model class for the type param.
â”ŒÂ·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·  model class properties map/bind to the fields
Â·                     of the index.
Â·                     Filed instances can also set other properties
v                     like IsSearchable, IsFilterable,...
Ex. Model class:

using System;
using Microsoft.Azure.Search;
using Microsoft.Azure.Search.Models;
using Microsoft.Spatial;
using Newtonsoft.JSON;

// The SerializePropertyNamesAsCamelCase attribute is defined in the
// Azure Search .NET SDK. It ensures that Pascal-case property names in
// the model class are mapped to camel-case field names in the index.
[SerializePropertyNamesAsCamelCase]             â† map to camel-case field
public partial class GÂºHotelÂº                     index names
{
    [System.ComponentModel.DataAnnotations.Key] â† a field of type string
    [IsFilterable, IsSortable, IsFacetable]       must be marked with
    public string HotelId { get; set; }           as key field
    ...
    [IsSearchable]
    [Analyzer(AnalyzerName.AsString.FrLucene)]
    [JSONProperty("description_fr")]           â† map to another index field name
    public string DescriptionFr { get; set; }
    ...
    [IsSearchable, IsFilterable, IsFacetable]
    public stringÂº[]ÂºTags { get; set; }
    ...
    [IsFilterable, IsSortable]
    public GeographyPoint Location { get; set; }
}
BÂºIsSearchableÂº: enable full-text search

   var BÂºdefinitionÂº = new Index() {             // â† create index definition:
       Name = "hotels",
       Fields = FieldBuilder.BuildForTypeGÂºË‚HotelËƒÂº()
   };

   serviceClient.ÂºIndexes.CreateÂº(BÂºdefinitionÂº); // â† Finally create the index
                                                  //   or CloudException thrown
   serviceClient.Indexes.Delete("hotels");        /

   NOTE: Example uses synch methods for clarity.
         Async ones are prefered. (CreateAsync, DeleteAsync)


ÂºSTEP 3.1)Âº
 ISearchIndexClient indexClient =    // â† Create SearchIndexClient instance
       serviceClient.Indexes         //   to connect to index.
          .GetClient("hotels");
 // ^ ISearchIndexClient.Documents property provides
 // all the methods CRUD documents in index.
   NOTE: In typical search apps, index management and population
         is handled by a separate component from search queries.
         Indexes.GetClient is convenient for populating an index because it
         saves you the trouble of providing another SearchCredentials. It does
         this by passing the admin key that you used to create the
         SearchServiceClient to the new SearchIndexClient. However, in the
         part of your application that executes queries, it is better to
         create the SearchIndexClient directly so that you can pass in a query
         key instead of an admin key. This is consistent with the principle of
         least privilege and will help to make your application more secure.

ÂºSTEP 3.2)Âº
   package up the data to index into an IndexBatch instance:
   - 1...N "IndexAction" objects:
            ^^^^^^^^^^^
            IndexAction: ( document , action )
                                      ^^^^^^
                    upload, merge, delete, etc

Depending on  actions, only certain fields must be included each document:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚ Description             â”‚ Necessary fields â”‚ Notes                                    â”‚
  â”‚                         â”‚ for each documentâ”‚                                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ÂºUploadÂº== "upsert"      â”‚ key, plus any    â”‚ When updating/replacing an existing      â”‚
  â”‚                         â”‚ other fields you â”‚   document, any field that is not        â”‚
  â”‚                         â”‚ wish to define   â”‚   specified in the request will have its â”‚
  â”‚                         â”‚                  â”‚   field set to null. This occurs even    â”‚
  â”‚                         â”‚                  â”‚ when the field was previously set to a   â”‚
  â”‚                         â”‚                  â”‚ nonâ”€null value.                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ÂºMergeÂº existing documentâ”‚ key, plus any    â”‚ Any field you specify in a merge will    â”‚
  â”‚ with specified fields.  â”‚ other fields you â”‚ replace the existing field in the        â”‚
  â”‚ document must exists    â”‚ wish to define   â”‚ document. This includes fields of type   â”‚
  â”‚                         â”‚                  â”‚ DataType.Collection(DataType.String).    â”‚
  â”‚                         â”‚                  â”‚ For example, if the document contains a  â”‚
  â”‚                         â”‚                  â”‚ field tags with value ["budget"] and     â”‚
  â”‚                         â”‚                  â”‚ you execute a merge with value           â”‚
  â”‚                         â”‚                  â”‚ ["economy", "pool"] for tags, the final  â”‚
  â”‚                         â”‚                  â”‚ value of the tags field will be          â”‚
  â”‚                         â”‚                  â”‚ ["economy", "pool"]. It will not be      â”‚
  â”‚                         â”‚                  â”‚ ["budget", "economy", "pool"].           â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ MergeOrUpload           â”‚ key, plus any    â”‚                                          â”‚
  â”‚ Merge if document existsâ”‚ other fields you â”‚                                          â”‚
  â”‚ Upload otherwise.       â”‚ wish to define   â”‚                                          â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚ Delete (from index)     â”‚ key only         â”‚ Any fields you specify other than the    â”‚
  â”‚                         â”‚                  â”‚ key field will be ignored. If you want   â”‚
  â”‚                         â”‚                  â”‚ to remove an individual field from a     â”‚
  â”‚                         â”‚                  â”‚ document, use Merge instead and simply   â”‚
  â”‚                         â”‚                  â”‚ set the field explicitly to null.        â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


   var actions = new IndexActionË‚GÂºHotelÂºËƒ[] {
     IndexAction.Upload( new Hotel() {
       HotelId = "1",
       ...
       Category = "Luxury",
       Tags = new[] { "pool", "view", "wifi", "concierge" },
       ...
       LastRenovationDate = new DateTimeOffset(2010, 6, 27, 0, 0, 0, TimeSpan.Zero),
       ...
       Location = GeographyPoint.Create(47.678581, -122.131577)
     }),
     IndexAction.Upload( new Hotel() { HotelId = "2", ...   }),
     IndexAction.MergeOrUpload( new Hotel() { HotelId = "3", BaseRate = 129.99, }),
     IndexAction.Delete(new Hotel() { HotelId = "6" })
   };

   var batch = IndexBatch.New(actions);  // â† 3.2) Create index batch

NOTE:Âº1000 documents maxÂºperÂºindexing requestÂº

ÂºSTEP 3.3)Âº
  Call the Documents.Index method of your SearchIndexClient to send
  data to index the IndexBatch to your search index.
  /*-------*/try {
    indexClient.Documents.Index(batch);
  /*-------*/ } catch (IndexBatchException e) {
    // Sometimes when your Search service is under load, indexing
    // will fail for some of the documents in
    // the batch. Depending on your application, you can take
    // compensating actions like delaying and
    // retrying. For this simple demo, we just log the failed
    // document keys and continue.
    Console.WriteLine(
        "Failed to index some of the documents: {0}",
        String.Join(", ", e.IndexingResults.
          Where(r =Ëƒ !r.Succeeded).Select(r =Ëƒ r.Key)));
  /*-------*/ }
  Console.WriteLine("Waiting for documents to be indexed...\n");
  Thread.Sleep(2000);

ÂºHow the .NET SDK handles documentsÂº
Node index properties are camel-case, (starts with lower case)
while each public property starts with an upper-case letter ("Pascal case").
BÂºThis is a common scenario in .NET applications that perform   Âº
BÂºdata-binding where the target schema is outside the control ofÂº
BÂºthe application developer.                                    Âº

RÂºWhy you should use nullable data typesÂº
If you use a non-nullable property, you have to guarantee
that no documents in your index contain a null value for
the corresponding field.
Neither the SDK nor the Azure Search service will help you
to enforce this.

RÂºIf you add a new field to an existing index, after update,Âº
RÂºall documents will have a null value for that new field   Âº
RÂº(since all types are nullable in Azure Search).Âº
ÂºPRE-SETUPÂº:
 Fetch query API-keys (user key) (vs primary/secondary admin keys)
 A.Portal â†’ Search "Keys"

private static SearchIndexClient
  CreateSearchIndexClient(IConfigurationRoot configuration) {
    string searchServiceName = configuration["SearchServiceName"];
    string queryApiKey = configuration["SearchServiceQueryApiKey"];
    SearchIndexClient indexClient =ÂºnewÂº // â†ÂºSTEP 4.1)ÂºCreate
      ÂºSearchIndexClient(Âº               //   SearchIndexClient instance
         searchServiceName,
         "hotels",
         new SearchCredentials(queryApiKey));
    return indexClient;
}

ÂºQueries Main Types:Âº
-ÂºsearchÂº searches for one or more terms in all searchable
          fields in your index.
-ÂºfilterÂº evaluates a boolean expression over all
          filterable fields in an index.
 ^They can be used together or separately.

Example:
SearchParameters parameters;
DocumentSearchResultË‚HotelËƒ results;
// Search index for term 'budget' and
// return "hotelName" field
parameters = new SearchParameters() {
   Select = new[] { "hotelName" }         // â† Select
};
results = indexClient.Documents.
          SearchË‚HotelËƒ(                  // â†ÂºSearchÂº
              "budget",
              parameters);
// Apply filter to index to find hotels
// cheaper than $150
// return hotelId
parameters = new SearchParameters() {
  Filter = "baseRate lt 150",             // â† Filter
  Select = new[] {
    "hotelId",
    "description"
  }
};
results = indexClient.Documents.
          SearchË‚HotelËƒ(
            "*",
            parameters);





parameters = new SearchParameters() { â† // Search entire index,
  OrderBy = new[] {
              "lastRenovationDate desc" // â† order by lastRenovationDate desc,
            },
  Select = new[] {                      // â† show
              "hotelName",              //      hotelName and
              "lastRenovationDate" },   //      lastRenovationDate

  Top = 2                               // â† take top two results
};

results = indexClient.Documents.
          SearchË‚HotelËƒ(
            "*", parameters);

parameters = new SearchParameters();
results = indexClient.Documents.
          SearchË‚HotelËƒ(                // â† Search entire index  for
                   "motel",             // â† term 'motel'
                   parameters);
WriteDocuments(results);

ÂºHandle search resultsÂº
private static void
  WriteDocuments(
    DocumentSearchResultË‚HotelËƒ searchResults) {
    foreach (
       SearchResultË‚HotelËƒ result in
       searchResults.Results) {
        Console.WriteLine(result.Document);
    }
    Console.WriteLine();
}
<hr/>
<span xsmall>Full text search</span>
Lucene full text search  four stages execution:

 1) query   : extract search terms
    parsing

 2) lexical : Individual query terms are sometimes
    analysis  broken down and reconstituted into
              new forms to cast a broader net over
              what could be considered as a
              potential match.

 3) document: search engine uses an index to
    matching  retrieve documents with matching
              terms.

 4) scoring : A result set is then sorted by a
              relevance score assigned to each
              individual matching document.
              Those at the top of the ranked list
              are returned to the calling app.





Separate query terms from query
operators and create the query   Retrieves and scores matching
structure (a query tree) to be   documents based on the contents of the
sent to the search engine.       inverted index.
         |                        v
Query    v              Query    â”Œ4)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” Top
text    â”Œ1)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  tree     â”‚Search Indexâ”‚ 50
â”€â”€â”€â”€â”€â”€â†’ â”‚Query Parserâ”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”€â”€â”€â”€â”€â†’
        â”‚Simpleâ”‚Full â”‚           â”‚Index (DDBB)â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â””3)â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     Query â”‚     ^ Analyzed       ^
     terms v     â”‚ terms         efficient data structure used to
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”             store and organize searchable terms
          â”‚Analyzerâ”‚             extracted from indexed documents.
          â””2)â”€â”€â”€â”€â”€â”€â”˜
           ^
          Perform lexical analysis on query
          terms. This process can involve
          transforming, removing, or expanding
          of query terms.

ÂºAnatomy of a search requestÂº
- A search request is a complete specification of what should be
  returned in a result set. In simplest form, it is an empty query with
  no criteria of any kind. A more realistic example includes
  parameters, several query terms, perhaps scoped to certain fields,
  with possibly a filter expression and ordering rules.

The following example is a search request you might send to Azure
Search using the REST API.
  POST /indexes/hotels/docs/search?API-version=2017-11-11
  {
      "search": "Spacious, air-condition* +\"Ocean view\"",
      "searchFields": "description, title",
      "searchMode": "any",
      "filter": "price ge 60 and price lt 300",
      "orderby": "geo.distance(location, geography'POINT(-159.47623522.227659)')",
      "queryType": "full"
   }
   ^
   For this request, the search engine does the following:

   - Filters out documents where the price is at least $60 and less
     than $300.
   - Executes the query.
     For this query, the search engine scans the description and
     title fields specified in searchFields for documents that
     contain â€œOcean viewâ€, and additionally on the term "spacious",
     or on terms that start with the prefix â€œair-conditionâ€.
   - The searchMode parameter is used to match on any term
     (default) or all of them, for cases where a term is not
     explicitly required (+).
   - Orders the resulting set of hotels by proximity to a given
     geography location, and then returned to the calling application.

Most that follows is about processing of the search query:
"Spacious, air-condition* +\"Ocean view\"".
Filtering and ordering are out of scope.

Stage 1: Query parsing
   "search": "Spacious, air-condition* +\"Ocean view\"",
   The query parser separates operators (such as * and + in the example)
   from search terms, and deconstructs the search query into subqueries
   of a supported type:
   - term query for standalone terms (like spacious)
   - phrase query for quoted terms (like ocean view)
   - prefix query for terms followed by a prefix operator * (like
     air-condition)

   Operators associated with a subquery determine whether the query
   â€œmust beâ€ or "should be" satisfied in order for a document to be
   considered a match. For example, +"Ocean view" is â€œmustâ€ due to the +
   operator.

   The query parser restructures the subqueries into a query tree (an
   internal structure representing the query) it passes on to the search
   engine. In the first stage of query parsing, the query tree looks
   like this.

   Boolean query searchmode any

ÂºSupported query parsers: Simple (default) and Full LuceneÂº
 Use queryType parameter to choose one of them.
-ÂºSimple query languageÂº: intuitive and robust, often suitable to
  interpret user input as-is without client-side processing.
  It supports query operators familiar from web search engines.
-ÂºFull Lucene query languageÂº: extends default Simple query language
  by adding support for more operators and query types like
  wildcard, fuzzy, regex, and field-scoped queries.

ÂºsearchMode parameterÂº:
- default operator for Boolean queries:
  - any (default): space delimiter OR
  - all          : space delimiter AND

Suppose that we now set searchMode=all. In this case, the space is
interpreted as an â€œandâ€ operation. Each of the remaining terms must
both be present in the document to qualify as a match. The resulting
sample query would be interpreted as follows:

+Spacious,+air-condition*+"Ocean view"

A modified query tree for this query would be as follows, where a
matching document is the intersection of all three subqueries:
Boolean query searchmode all

Choosing searchMode=any over searchMode=all is a decision best
arrived at by running representative queries. Users who are likely to
include operators (common when searching document stores) might find
results more intuitive if searchMode=all informs boolean query
constructs.
Next

We'll cover lexical analysis and document retrieval in Azure Search.
Lexical analysis and document retrieval in Azure Search
Lexical analysis

Lexical analyzers process term queries and phrase queries after the
query tree is structured. An analyzer accepts the text inputs given
to it by the parser, processes the text, and then sends back
tokenized terms to be incorporated into the query tree.

The most common form of lexical analysis is linguistic analysis which
transforms query terms based on rules specific to a given language:

    Reducing a query term to the root form of a word
    Removing non-essential words (stopwords, such as â€œtheâ€ or "and"
in English)
    Breaking a composite word into component parts
    Lower casing an upper case word

All of these operations tend to erase differences between the text
input provided by the user and the terms stored in the index. Such
operations go beyond text processing and require in-depth knowledge
of the language itself. To add this layer of linguistic awareness,
Azure Search supports a long list of language analyzers from both
Lucene and Microsoft.

    Note: Analysis requirements can range from minimal to elaborate
depending on your scenario. You can control complexity of lexical
analysis by the selecting one of the predefined analyzers or by
creating your own custom analyzer. Analyzers are scoped to searchable
fields and are specified as part of a field definition. This allows
you to vary lexical analysis on a per-field basis. Unspecified, the
standard Lucene analyzer is used.

In our example, prior to analysis, the initial query tree has the
term â€œSpacious,â€ with an uppercase "S" and a comma that the query
parser interprets as a part of the query term (a comma is not
considered a query language operator).

When the default analyzer processes the term, it will lowercase
â€œocean viewâ€ and "spacious", and remove the comma character. The
modified query tree will look as follows:
Boolean query with analyzed terms
Testing analyzer behaviors

The behavior of an analyzer can be tested using the Analyze API.
Provide the text you want to analyze to see what terms given analyzer
will generate. For example, to see how the standard analyzer would
process the text â€œair-conditionâ€, you can issue the following request:

{
    "text": "air-condition",
    "analyzer": "standard"
}

The standard analyzer breaks the input text into the following two
tokens, annotating them with attributes like start and end offsets
(used for hit highlighting) as well as their position (used for
phrase matching):

{
  "tokens": [
    {
      "token": "air",
      "startOffset": 0,
      "endOffset": 3,
      "position": 0
    },
    {
      "token": "condition",
      "startOffset": 4,
      "endOffset": 13,
      "position": 1
    }
  ]
}

Exceptions to lexical analysis

Lexical analysis applies only to query types that require complete
terms â€“ either a term query or a phrase query. It doesnâ€™t apply to
query types with incomplete terms â€“ prefix query, wildcard query,
regex query â€“ or to a fuzzy query. Those query types, including the
prefix query with term air-condition* in our example, are added
directly to the query tree, bypassing the analysis stage. The only
transformation performed on query terms of those types is lowercasing.
Document retrieval

Document retrieval refers to finding documents with matching terms in
the index. This stage is understood best through an example. Let's
start with a hotels index having the following simple schema:

{
    "name": "hotels",
    "fields": [
        { "name": "id", "type": "Edm.String", "key": true,
"searchable": false },
        { "name": "title", "type": "Edm.String", "searchable": true
},
        { "name": "description", "type": "Edm.String", "searchable":
true }
    ]
}

Further assume that this index contains the following four documents:

{
    "value": [
        {
            "id": "1",
            "title": "Hotel Atman",
            "description": "Spacious rooms, ocean view, walking
distance to the beach."
        },
        {
            "id": "2",
            "title": "Beach Resort",
            "description": "Located on the north shore of the island
of KauaÊ»i. Ocean view."
        },
        {
            "id": "3",
            "title": "Playa Hotel",
            "description": "Comfortable, air-conditioned rooms with
ocean view."
        },
        {
            "id": "4",
            "title": "Ocean Retreat",
            "description": "Quiet and secluded"
        }
    ]
}

How terms are indexed

To understand retrieval, it helps to know a few basics about
indexing. The unit of storage is an inverted index, one for each
searchable field. Within an inverted index is a sorted list of all
terms from all documents. Each term maps to the list of documents in
which it occurs, as evident in the example below.

To produce the terms in an inverted index, the search engine performs
lexical analysis over the content of documents, similar to what
happens during query processing:

    Text inputs are passed to an analyzer, lower-cased, stripped of
punctuation, and so forth, depending on the analyzer configuration.
    Tokens are the output of text analysis.
    Terms are added to the index.

It's common, but not required, to use the same analyzers for search
and indexing operations so that query terms look more like terms
inside the index.
Inverted index for example documents

Returning to our example, for the title field, the inverted index
looks like this:
  Term    Document list
  atman   1
  beach   2
  hotel   1, 3
  ocean   4
  playa   3
  resort  3
  retreat 4

In the title field, only hotel shows up in two documents: 1, 3.

For the description field, the index is as follows:

Term        Document list
air         3
and         4
beach       1
conditioned 3
comfortable 3
distance    1
island      2
kauaÊ»i      2
located     2
north       2
ocean       1, 2, 3
of          2
on          2
quiet       4
rooms       1, 3
secluded    4
shore       2
spacious    1
the         1, 2
to          1
view        1, 2, 3
walking     1
with        3

Matching query terms against indexed terms

Given the inverted indices above, letâ€™s return to the sample query
and see how matching documents are found for our example query.
Recall that the final query tree looks like this:
Boolean query with analyzed terms

During query execution, individual queries are executed against the
searchable fields independently.

    The TermQuery, â€œspaciousâ€, matches document 1 (Hotel Atman).

    The PrefixQuery, "air-condition*", doesn't match any documents.

    This is a behavior that sometimes confuses developers. Although
the term air-conditioned exists in the document, it is split into two
terms by the default analyzer. Recall that prefix queries, which
contain partial terms, are not analyzed. Therefore terms with prefix
â€œair-conditionâ€ are looked up in the inverted index and not found.

    The PhraseQuery, â€œocean viewâ€, looks up the terms "ocean" and
â€œviewâ€ and checks the proximity of terms in the original document.
Documents 1, 2 and 3 match this query in the description field.
Notice document 4 has the term ocean in the title but isnâ€™t
considered a match, as we're looking for the "ocean view" phrase
rather than individual words.

On the whole, for the query in question, the documents that match are
1, 2, 3.
Next

We'll cover document scoring and wrap up the topic.

Every document in a search result set is assigned a relevance score.
The function of the relevance score is to rank higher those documents
that best answer a user question as expressed by the search query.
The score is computed based on statistical properties of terms that
matched. At the core of the scoring formula is TF/IDF (term
frequency-inverse document frequency). In queries containing rare and
common terms, TF/IDF promotes results containing the rare term. For
example, in a hypothetical index with all Wikipedia articles, from
documents that matched the query the president, documents matching on
president are considered more relevant than documents matching on the.
Scoring example

Recall the three documents that matched our example query:

search=Spacious, air-condition* +"Ocean view"

{
  "value": [
    {
      "@search.score": 0.25610128,
      "id": "1",
      "title": "Hotel Atman",
      "description": "Spacious rooms, ocean view, walking distance to
the beach."
    },
    {
      "@search.score": 0.08951007,
      "id": "3",
      "title": "Playa Hotel",
      "description": "Comfortable, air-conditioned rooms with ocean
view."
    },
    {
      "@search.score": 0.05967338,
      "id": "2",
      "title": "Ocean Resort",
      "description": "Located on a cliff on the north shore of the
island of Kauai. Ocean view."
    }
  ]
}

Document 1 matched the query best because both the term spacious and
the required phrase ocean view occur in the description field. The
next two documents match only the phrase ocean view. It might be
surprising that the relevance score for document 2 and 3 is different
even though they matched the query in the same way. It's because the
scoring formula has more components than just TF/IDF. In this case,
document 3 was assigned a slightly higher score because its
description is shorter. Learn about Lucene's Practical Scoring
Formula to understand how field length and other factors can
influence the relevance score.

Some query types (wildcard, prefix, regex) always contribute a
constant score to the overall document score. This allows matches
found through query expansion to be included in the results, but
without affecting the ranking.

An example illustrates why this matters. Wildcard searches, including
prefix searches, are ambiguous by definition because the input is a
partial string with potential matches on a very large number of
disparate terms (consider an input of "tour*", with matches found on
â€œtoursâ€, â€œtourettesâ€, and â€œtourmalineâ€). Given the nature of these
results, there is no way to reasonably infer which terms are more
valuable than others. For this reason, we ignore term frequencies
when scoring results in queries of types wildcard, prefix and regex.
In a multi-part search request that includes partial and complete
terms, results from the partial input are incorporated with a
constant score to avoid bias towards potentially unexpected matches.
Score tuning

There are two ways to tune relevance scores in Azure Search:

    Scoring profiles promote documents in the ranked list of results
based on a set of rules. In our example, we could consider documents
that matched in the title field more relevant than documents that
matched in the description field. Additionally, if our index had a
price field for each hotel, we could promote documents with lower
price.

    Term boosting (available only in the Full Lucene query syntax)
provides a boosting operator ^ that can be applied to any part of the
query tree. In our example, instead of searching on the prefix
air-condition*, one could search for either the exact term
air-condition or the prefix, but documents that match on the exact
term are ranked higher by applying boost to the term query:
air-condition^2||air-condition*.

Scoring in a distributed index

All indexes in Azure Search are automatically split into multiple
shards, allowing us to quickly distribute the index among multiple
nodes during service scale up or scale down. When a search request is
issued, itâ€™s issued against each shard independently. The results
from each shard are then merged and ordered by score (if no other
ordering is defined). It is important to know that the scoring
function weights query term frequency against its inverse document
frequency in all documents within the shard, not across all shards!

This means a relevance score could be different for identical
documents if they reside on different shards. Fortunately, such
differences tend to disappear as the number of documents in the index
grows due to more even term distribution. Itâ€™s not possible to assume
on which shard any given document will be placed. However, assuming a
document key doesn't change, it will always be assigned to the same
shard.

In general, document score is not the best attribute for ordering
documents if order stability is important. For example, given two
documents with an identical score, there is no guarantee which one
appears first in subsequent runs of the same query. Document score
should only give a general sense of document relevance relative to
other documents in the results set.
Wrap-up

The success of internet search engines has raised expectations for
full text search over private data. For almost any kind of search
experience, we now expect the engine to understand our intent, even
when terms are misspelled or incomplete. We might even expect matches
based on near equivalent terms or synonyms that we never actually
specified.

From a technical standpoint, full text search is highly complex,
requiring sophisticated linguistic analysis and a systematic approach
to processing in ways that distill, expand, and transform query terms
to deliver a relevant result. Given the inherent complexities, there
are a lot of factors that can affect the outcome of a query. For this
reason, investing the time to understand the mechanics of full text
search offers tangible benefits when trying to work through
unexpected results.

This article explored full text search in the context of Azure
Search. We hope it gives you sufficient background to recognize
potential causes and resolutions for addressing common query problems.
</pre>
</div>


</div> <!-- azure -->

<hr/>  <!-- azure end -->

<span xbig>AWS (v0.3)</span><br/>
<div groupv>
<span title>Introduction</span>
<pre zoom labels="aws,TODO,101,">
<span xsmall>External Links</span>
- Tutorials
@[https://aws.amazon.com/getting-started/tutorials/]
<span xsmall>AWS Core Services</span>
Core service according to Calculator:
@[https://calculator.s3.amazonaws.com/index.html]
- EC2
- S3
- Route 53
- CloudFront
- RDS
- Elastic Load Balan.
- DynamoDB
- ElastiCache
- CloudWatch
- SES
- SNS
- ectory Service
- Redshift
- Glacier
- SQS
- SWF
- Elastic MapReduce
- Kinesis Streams
- Snowball
- Direct Connect
- VPC
- Elastic FS:
- Key Mng Srv.
- AWS Support
</pre>

<pre zoom bgorange labels="aws,101,aaa,rbac,governance,devops,secrets,diagram,policy" id="aws_governance">
<span xsmall>Governance Hierarchy</span>
REFs:
  @[https://aws.amazon.com/iam/]
  @[https://docs.aws.amazon.com/general/latest/gr/aws-security-credentials.html]

- IAM: (I)dentity and (A)ccess (M)anagement, primary account-level feature to securely manage fine-grained
       access control to AWS services and resources.
BÂºAWS root vs IAM user credentialsÂº
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚BÂºUSER TYPES                                            â”‚  SECURITY CREDENTIALS TYPESÂº       â”‚
  â”‚  (both are linked to security credential)              â”‚                                    â”‚
  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚  -Âºroot user:Âºaccount owner                            â”‚  Cred. Type     â”‚ Used For         â”‚
  â”‚    - created at account creation.                      â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
  â”‚    - full access to account resources:                 â”‚ Âºuser-name/passÂºâ”‚ - AWS Mng Consoleâ”‚
  â”‚      - no IAM policies can deny access.                â”‚                 â”‚                  â”‚
  â”‚      - AWS Organizations service control policy (SCP)  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
  â”‚        can be used to limit the permissions.           â”‚  ÂºAccess keysÂº  â”‚ - API calls      â”‚
  â”‚    - Admin tasks through AWS-Management-Console        â”‚   ^             â”‚ - AWS cli        â”‚
  â”‚      (sign withÂºroot-user email/passwordÂº)             â”‚   |                                â”‚
  â”‚    â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â”‚ Injected in code like:             â”‚
  â”‚                                                        â”‚ â”œ (alt 1) ENV. VARs:               â”‚
  â”‚  -ÂºIAM  user:ÂºAWS Identity and Access Mng. (IAM) user. â”‚ Â·  - AWS_ACCESS_KEY_ID             â”‚
  â”‚    - created by root user                              â”‚ Â·  - AWS_SECRET_ACCESS_KEY         â”‚
  â”‚    - Permissions to resources can be added/revoked     â”‚ Â·                                  â”‚
  â”‚    Ex: user1 â†’ IAM user 01 â†’ resource  01,05,10,...    â”‚ â”œ (alt 2) Credentials file         â”‚
  â”‚        user2 â†’ IAM user 02 â†’ resource  03,07,23,...    â”‚  ${HOME}/.aws/credentials          â”‚
  â”‚       OÂºBest practiceÂº:                                â”‚  | [default]                       â”‚
  â”‚         - Delegate admin tasks root-user to            â”‚  | aws_access_key_id = ...         â”‚
  â”‚           (new) IAM-admin-user granting full access.   â”‚  | aws_secret_access_key = ...     â”‚
  â”‚         - lock away the access keys for the root user  â”‚                                    â”‚
  â”‚           for anything except for "closing account" ...â”‚                                    â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    - In IAMÂºa Principal is an entity that can access resources.Âº
     ÂºA Principal can be can be the AWS account-root-user, a IAM-user or a IAM-role.Âº
   - Permissions of Principals over resources is defined in two steps:
     - STEP 1) Define permissions within policies.
       â˜BÂºTIP:Âº In practice, aÂºpolicies are a set of attributesÂº, so IAM is an example
                of attribute-based access control. Ex policy:
                {                           â† Allow all dynamodb permissions
                  "Version": "2012-10-17",    constrainged to just a given table
                  "Statement": {              in a given ddbb        â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                   Âº"Effect": "Allow",Âº            â””â”€â”€â”¬â”€â”€â”€â”€â”€â”˜      â”Œâ”€â”€â”€â”€â”€â”€â”˜
                   Âº"Action": "dynamodb:*",Âº          v            v
                    "Resource":                   â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”´â”€â”€â”€â”€â”€â”
                     Âº"arn:aws:dynamodb:us-east-2:123456789012:table/Books"Âº
                  }
                }
                         - The optional (expression) attribute "Condition" allows to conditionally apply
                  a policy based on "runtime" request attributes (IP address, time of day,..)
                  - Conditions can be global ("aws:...") or service-specific ("ec2:InstanceType:...")
                  - Ex: "Condition" : { "StringEquals" : { "aws:username" : "johndoe" }}

                REF: @[https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_condition.html]
     - STEP 2) Attach policies to Principals.

    â”Œâ†’ ÂºIAM role:Âº Similar to IAM-user but
    Â·@[https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_terms-and-concepts.html]
    Â·  - IAM users can incarnate it
    Â·  - NO long-term credentials allowed, just temporal ones for role session
    Â·  -ÂºUse case: Delegate temporal resource-access to (maybe external) Âº
    Â·   Âº          users/apps/ec2 VMs/services (SAML 2.0, IOpenID Connect,...)Âº
    Â·  - Supossing IAM user1 has permissions over RoleA, and RoleA over RoleB then:
    Â·    IAM user1 can incarnate RoleA through "aws ami assume-role" for a max. of 12 hours,
    Â·    then it can assume (chained) RoleB (for a max. of 1 hour)
    Â·  - Session tags will be used to keep trace of assigned roles.
    â””Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·â”
                                                      v             
   ÂºACCOUNTÂº       1â†Â·Â·Â·Â·Â·Â·Â·Â·â†’ 1Âºroot-userÂº1 â†Â·Â·Â·Â·â†’ NÂºIAM userÂº N â†Â·Â·Â·Â·Â·Â·Â·Â·â” Direct attachment of
    - account name                                    N                    Â· IAM users to policies is
    - email-address         Âº*1Âº                    BÂº^Âº                   Â· discouraged. Prefer indirect 
    - AWS-account-ID        Âº*1ÂºOÂº*2Âº               BÂºÂ·Âº                   Â· access through groups to 
    - root-user password    Âº*1Âº                    BÂºÂ·Âº                   Â· simplify management.
    - root-user access-keys Âº*1Âº                    BÂºvÂº                   Â·  
    ----------------------                            M                    v              ÂºPermissionÂº
    - Billing                                  ÂºIAM groupÂºBÂº M â†Â·Â·Â·Â·â†’ NÂº ÂºPolicyÂº 1 â†Â·Â·Â·Â·â†’NÂºAttributesÂº
    * NOTE: Account can be member of       - Can not be assigned a "Principal" in a resource-based policy.
      an AWS Organization where the        - Allows to attach policies to N users in group simultaneously  
      organizational administrator used      â˜ Recommended/Best pattern (vs attaching policies to user)
      a service control policy (SCP)         A IAM (user) group can represent a department/project/...
      can limit permissions.                 a tasks. By adding/removing users from the group management
                                             is greately simplified.

    Âº*1Âº: RÂºWARNÂº: If those credentials are lost you can NOT recover them.
                   For security reasons, AWS doesn't provide the means for you
                   or anyone else to retrieve your credentials.


   OÂº*2Âº: AWS-account-ID: Globally Unique 12-digit number (Ex: 123456789012)
          - Same for root and all IAM users.
            IAM-users also use it to sign to AWS Mng. Console
            (Alternatively using account alias).
          - Embedded also in many Amazon Resource Names (ARNs).
          - Log-in into Console to retrieve it following :

          Canonical-user-ID: obfuscated form of AWS-account-ID.
          (ex: 79a59df900b949e55d96a1e698fbacedfd6e09d98eacf8f8d5218e7cd47ef2be)
          - Useful to "linkt" to real account when granting cross-account access to S3 buckets|objects.
          - Log-in into Console to retrieve it as explained in :

          To retrieve th AWS-account-ID|Cannocical-user-ID:
          - (root user) â†’ Console â†’ "account name" â†’ "My Security Credentials" â†’ "Account identifiers"
          - (IAM  user) â†’ Console â†’                â†’ "My Security Credentials" â†’ "Account details"
                                                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                    Not available when signed as
                                                    federated user (vs IAM user)

          - (aws cli) :$Âº$ aws sts get-caller-identity --query Account --output textÂº

BÂºROLES HOW-TO:Âº
  $Âº$ aws iam create-role        ...      Âº
  $Âº$ aws iam attach-role-policy ...      Âº â† Attach managed permissions policy
  $Âº$ aws iam put-role-policy    ...      Âº â† create inlime  permissions policy
  $Âº$ aws iam tag-role           ...      Âº â† (optional) add custom attr. attaching tags
  $Âº$ aws iam put-role-permission-boundaryÂº â† (optional) Control max. permsissions a role
                                              can have.
  Ex: (most common steps for creating a cross-account role)

  - STEP 1). attachÂºtrust-policyÂºto role in "aws iam create-role"
    $Âº$ ROLE_FILE="Test-UserAccess-Role"                      Âº
    $Âº$ POLICY_FILE="trust_policy_for_acct_123456789012.json" Âº
    $Âº$ cat trust_policy_for_acct_123456789012.json Âº  â† save as customer-managed-policy in AWS
    {                                                    with name "PolicyForRole".
      "Version": "2012-10-17",
      "Statement": [ {
          "Effect"   : "Allow",
          "Principal": {
              "AWS": "arn:aws:iam::123456789012:root" â† allow all users in
          },                                            123456789012 account
         Âº"Action"   : "sts:AssumeRole",Âº             â† to assume the role
          "Condition": { "Bool":
            { "aws:MultiFactorAuthPresent": "true" }  â† when user provide MFA authentication
           }                                            (using SerialNumber and TokenCode params)
        } ]
    }
    $Âº$ aws iam create-role \                      Âº
    $Âº  --role-name ${ROLE_FILE} \                 Âº
    $Âº  --assume-role-policy-document $POLICY_FILE Âº â† Create role attaching trust-policy

  - STEP 2) attach an existingÂºpermission-managed policyÂºto the role.
    {
      "Version": "2012-10-17",
      "Statement": [ {
        "Effect": "Allow",             â† allows anyone who assumes the role
       Âº"Action": "s3:ListBucket",Âº      to perform only ListBucket action on
        "Resource":
          "arn:aws:s3:::example_bucket" â† S3 "example_bucket"
      } ]
    }

    POLICY_ARN="arn:aws:iam::123456789012:role/PolicyForRole"
    $Âº$ aws iam attach-role-policy \               Âº â† Attach permissions policy
    $Âº  --role-name ${ROLE_FILE} \                 Âº   (managed-policy in example)
    $Âº  --policy-arn ${POLICY_ARN}                 Âº   to role to set its alowed permissions


  - STEP 3) Ussage:
    give individual users in trusted-account the permissions to switch to the role.
    At this stage any users in the 123456789012 account can assume the role.


BÂºMANAGE PERMISSIONS AT SCALE WITH TAGSÂºRÂº[TODO]Âº
@[https://www.infoq.com/news/2019/02/iam-tags-attribute-based-access]
  - Starting 2018-10, IAM users and roles can be managed through tags.
  - The release also includes the ability to embrace attribute-based
    access control (ABAC) and match AWS resources with IAM principals
    dynamically to "simplify permissions management at scale"

BÂºIDENTITY FEDERATIONÂºRÂº[TODO]Âº
@[https://docs.aws.amazon.com/IAM/latest/UserGuide/introduction_identity-management.html#%23intro-identity-federation]



</pre>


<pre zoom labels="aws,TODO," TODO>
<span xsmall>Who is who</span>
Forcibely incomplete but still pertinent list of "core" people:

- Danilo Poccia: @[https://twitter.com/danilop]
  - Principal Evangelist, Serverless @AWSCloud.
  - Author of AWS Lambda in Action from Manning.

- Jeff Barr:
  - Chief Evangelist for AWS. Author of blog @[https://aws.amazon.com/blogs/aws/author/jbarr/] (Since 2004)


- Varun Jewalikar, Software Engineer at Prime Video (See AWS Chaos Engineering)
- Adrian Hornsby: Principal Developer Advocate (Architecture) at AWS
</pre>

<pre zoom labels="aws,101,devops,TODO">
<span xsmall>AWS-DevOps Essential</span>
@[https://github.com/PoeBlu/aws-devops-essential]
</pre>


<pre zoom labels="aws,TODO,calculator">
<span xsmall>Price Calculator</span>
- Simple Monthly Calculator:
@[https://calculator.s3.amazonaws.com/index.html]
The actual cost can be observed on AWSâ€˜ billing page.
At the bottom of the page, there is a "Set your first billing alarm"
link that allows to define an email alarm as soon as a certain
threshold is exceeded.

RÂºWARN: for users that are not in the East of the USÂº
 """ I was a little bit confused that the  "Set your first billing alarm"
     link @[https://console.aws.amazon.com/cloudwatch/home?region=Âºus-east-1Âº&#s=Alarms&alarmAction=ListBillingAlarms]
     contains a variable Âºregion=us-east-1Âº, while I am using resources in
    Âºeu-central-1Âºonly.
     The corresponding link https://eu-central-1.console.aws....?Âºregion=eu-central-1Âº...
     does NOT allow to set any billing alarms.
     I assume that billing for all regions is performed centrally in US East
     for all regions (I hope).
  """
</pre>

<pre zoom labels="aws,TODO,101">
<span xsmall bgorange>OnPremise vs AWS </span>
REF:@[https://www.wowslides.com/users/bahadirhalil/projects/AWS-Core-Services-1]
           TRADITIONAL                                    AWS
        INFRASTRUCTURE                         INFRASTRUCTURE
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚ Firewalls,ACLs,Admins          â”‚   Security Groups/Network ACLs/AWS IAM â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ Router,Network Pipeline,Switch â”‚   ELB, VPC                             â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ Onâ”€Premises Servers            â”‚   AMI â”€â”€â†’ EC2 Instances                â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ DAS, SAN, NAS  RDBMS           â”‚   EBS   EFS    S3   RDS                â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
</pre>



<pre zoom labels="aws,calculator">
<span xsmall>AWS Free Tier</span>
- sign into @[https://aws.amazon.com/]
- scroll down and push the "Get Started for Free" button.
  - free tier trial account
    - up to 12 months
    - up to two time 750 hrs of computing time;
    - Linux/Windows 2012 server on a small VM:
      RÂºWARNÂº:
      - t1.micro is free tier
      - t2.nano  isRÂºNOTÂº free tier
</pre>

<pre zoom labels="aws,TODO,101">
<span xsmall>Guided tour</span>
<span xsmall>of core products</span>
@[https://searchaws.techtarget.com/feature/Amazon-Web-Services-product-directory]
</pre>

<pre zoom labels="aws,TODO,">
<span xsmall>AWS CLI</span>
- TODO: Installation:

- POST-INSTALLATION:
  - add aws like:
  $ aws configure
  AWS Access Key ID [****************FJMQ]:
  AWS Secret Access Key [****************DVVn]:
  Default region name [eu-central-1a]: eu-central-1
  Default output format [None]:

- CHECK INSTALL:
  Ex 1:
  $ aws ec2 describe-key-pairs --key-name AWS_SSH_Key

<hr/>
<span xsmall>AWS CLI v2</span>
@[https://www.infoq.com/news/2020/02/AWS-CLI-V2/ ]
- Includes SSO and Interactive Usability Features.
</pre>
</div>

<div groupv>
<span title>Security</span>
<pre zoom labels="aws,TODO," bgorange>
<span xsmall>AWS Security</span>
@[https://cloudonaut.io/aws-security-primer/]

ÂºTODO:Âº
-  User, Group, and Role management with IAM
-  Audit trails with CloudTrail
-  Threat detection and intelligence with GuardDuty
-  Encryption with KMS
</pre>

<pre zoom labels="aws,TODO,secrets">
<span xsmall>(K)ey (M)anagement (S)ervice</span>
@[https://aws.amazon.com/kms/]
</pre>

<pre zoom labels="aws,TODO,secrets">
<span xsmall>Secret Manager</span>
@[https://aws.amazon.com/secrets-manager/]
AWS Secrets Manager helps you protect secrets needed to access your
applications, services, and IT resources. The service enables you to easily
rotate, manage, and retrieve database credentials, API keys, and other
secrets throughout their lifecycle. Users and applications retrieve secrets
with a call to Secrets Manager APIs, eliminating the need to hardcode
sensitive information in plain text. Secrets Manager offers secret rotation
with built-in integration for Amazon RDS, Amazon Redshift, and Amazon
DocumentDB. Also, the service is extensible to other types of secrets,
including API keys and OAuth tokens. In addition, Secrets Manager enables you
to control access to secrets using fine-grained permissions and audit secret
rotation centrally for resources in the AWS Cloud, third-party services, and
on-premises.
ÂºBenefits:Âº
- Rotate secrets safely
- Manage access with fine-grained policies
- Secure and audit secrets centrally
- Pay as you go

@[https://www.infoq.com/news/2018/04/aws-secret-manager-manage]
Amazon announced the launch of the AWS Secrets Manager, which makes it easy
for customers to store and retrieve secrets using an API or the AWS Command
Line Interface (CLI). Furthermore, customers can rotate their credentials
with the built-in schedule feature or custom Lambda functions. The AWS
Secrets Manager enables users to centralize the management of secrets of
distributed services and applications.
</pre>

<pre zoom labels="aws,TODO,aaa">
<span title>Automate Sec.Rule Update</span>

- STEP 1: Verify that AWS user has the needed rights/permissions.
          for AmazonEC2FullAccess policy

- STEP 2: Test that you can see the security policies
          Example output
          $ aws ec2 describe-security-groups
          â†’ {
          â†’   "SecurityGroups": [
          â†’     {
          â†’       "IpPermissionsEgress": [
          â†’         ...(egress rules)...
          â†’       ],
          â†’       "Description": "default VPC security group",
          â†’       "IpPermissions": [
          â†’         ...(ingress rules)...
          â†’       ],
          â†’
          â†’       "GroupName": "default",
          â†’       "VpcId": "vpc-a6e13ecf",
          â†’       "OwnerId": "923026411698",
          â†’       "GroupId": "sg-0433846d"
          â†’     },
          â†’ ...(other security groups)...
          â†’ }

- STEP 3:
  Test adding/removing new ingress rules:
  $ EXTERNAL_IP01=$(wget http://ipinfo.io/ip -qO -)
  $ CidrIp01="${EXTERNAL_IP01}/32"
  $ IP_PERMISSIONS="[{"
  $ IP_PERMISSIONS="${IP_PERMISSIONS} \"IpProtocol\": \"tcp\","
  $ IP_PERMISSIONS="${IP_PERMISSIONS} \"FromPort\"  : 22,"
  $ IP_PERMISSIONS="${IP_PERMISSIONS} \"ToPort\"    : 22, "
  $ IP_PERMISSIONS="${IP_PERMISSIONS} \"IpRanges\"  : [{\"CidrIp\": \"${CidrIp01}\"}]"
  $ IP_PERMISSIONS="${IP_PERMISSIONS} }]"
  $ SG_ID="sg-0123456d"

  $ aws ec2 authorize-security-group-ingress --group-id ${SG_ID} \
        --dry-run \                            â† Check changes. Do not update
        --ip-permissions '${IP_PERMISSIONS}'
    ^^^^^^^
    remove like:
  $ aws ec2    revoke-security-group-ingress --group-id ${SG_ID} \
        --ip-permissions '${IP_PERMISSIONS}'
</pre>

<pre zoom labels="aws,TODO,security,auditing">
<span xsmall>CloudTrail</span>
@[https://aws.amazon.com/cloudtrail/]
AWS CloudTrail is a service that enables governance, compliance, operational
auditing, and risk auditing of your AWS account. With CloudTrail, you can log
, continuously monitor, and retain account activity related to actions across
your AWS infrastructure. CloudTrail provides event history of your AWS
account activity, including actions taken through the AWS Management Console,
AWS SDKs, command line tools, and other AWS services. This event history
simplifies security analysis, resource change tracking, and troubleshooting.
</pre>


<pre zoom labels="aws,TODO,security,monitoring">
<span xsmall>GuardDuty</span>

@[https://aws.amazon.com/guardduty/]
Amazon GuardDuty is a threat detection service that continuously monitors for
malicious activity and unauthorized behavior to protect your AWS accounts and
workloads. With the cloud, the collection and aggregation of account and
network activities is simplified, but it can be time consuming for security
teams to continuously analyze event log data for potential threats. With
GuardDuty, you now have an intelligent and cost-effective option for
continuous threat detection in the AWS Cloud. The service uses machine
learning, anomaly detection, and integrated threat intelligence to identify
and prioritize potential threats. GuardDuty analyzes tens of billions of
events across multiple AWS data sources, such as AWS CloudTrail, Amazon VPC
Flow Logs, and DNS logs. With a few clicks in the AWS Management Console,
GuardDuty can be enabled with no software or hardware to deploy or maintain.
By integrating with AWS CloudWatch Events, GuardDuty alerts are actionable,
easy to aggregate across multiple accounts, and straightforward to push into
existing event management and workflow systems.


@[https://www.infoq.com/news/2019/02/aws-guardduty-threat-detections]
Amazon has added another set of new threat detections to its GuardDuty service
in AWS. The three new threat detections are two new penetration testing
detections and one policy violation detection.

Amazon GuardDuty is a threat detection service available on AWS that
continuously monitors for malicious or unauthorized behaviour to help customers
protect their AWS accounts and workloads. When a threat is detected, the
service will send a detailed security alert to the GuardDuty console and AWS
CloudWatch Events â€“ thus making alerts actionable and easy to integrate into
existing event management and workflow systems.
</pre>

<pre zoom labels="aws,azure,gcp,governance,security,radar,TODO">
<span xsmall>Cloud Custodian</span>
@[https://cloudcustodian.io/]
@[https://manheim-c7n-tools.readthedocs.io/en/latest/]
- Opensource Cloud Security, Governance, and Management
  The Path to a Well Managed Cloud

- Cloud Custodian enables users to be well managed in the cloud. The simple YAML
  DSL allows you to easily define rules to enable a well-managed cloud
  infrastructure, that's both secure and cost optimized. It consolidates many of
  the ad-hoc scripts organizations have into a lightweight and flexible tool,
  with unified metrics and reporting.
</pre>

<pre zoom labels="aws,governance,tool,troubleshoot,TODO">
<span xsmall>PolicyUniverse</span>
@[https://github.com/Netflix-Skunkworks/policyuniverse]
- Package providing classes to parse AWS IAM and Resource Policies.
- Additionally, it can expand wildcards in Policies using permissions
  obtained from the AWS Policy Generator.
</pre>

</div>


<div group>
<span title>IaaS</span><hr/>
<div groupv>
<span title>Storage</span>
<pre zoom labels="aws,storage,cold_storage">
<span xsmall bgorange>Storage Matrix</span>
@[https://aws.amazon.com/products/storage/]
ÂºElastic Block Store(EBS)Âº            ÂºElastic File System(EFS)Âº
 - Persistent local storage for:      - simple, scalable, elastic FS for
   - C2                                 BÂºLinux-based workloadsÂº for use
   - databases                          with Cloud/on-premises resources.
   - data warehousing                   -BÂºScales on demand to petabytesÂº
   - enterprise applications              without disrupting apps,shrinking
   - Big Data processing                  when removing files.
   - Backup/recovery

ÂºFSx for LustreÂº                     GÂºSimple Storage Service ("S3")Âº
- Fully managed FS optimized for      - scalable, durable platform to make data
  compute-intensive workloads(IA,       accessible from Internet for
  media data processing, ...)           user-generated content, active archive,
  seamlessly integrated with S3         serverless computing, Big Data storage
                                        backup and recovery


ÂºS3 Glacier/Glacier Deep-ArchiveÂº     ÂºAWS Storage GatewayÂº
- Highly affordable long-term storage - hybrid storage cloud augmenting
  classes that can replace tape for     BÂºon-premisesÂº environment
  archive and regulatory compliance     BÂºfor bursting, tiering or migrationÂº


ÂºCloud Data Migration ServicesÂº        ÂºAWS BackupÂº
- services portfolio to help simplify  - fully managed backup service that
  moving data of all types and sizes     makes it easy to centralize and automate
  into and out of the AWS cloud          the back up of data across AWS services
                                         in cloud/Storage-gateway</td>
</pre>
<pre zoom labels="aws,storage,iaas,s3,101,price">
<span xsmall>S3</span>
- S3 Standard durability :
  - 99.999999999% object durability over a year
  - 99.99% availability
- Eventually consistent. Read data can be stale due to propagation time
                         of S3 replicas within a AWS region.
  - Updates are atomic. Either new or old data is read. Never mixed.

- Reduced Redundancy Storage (RRS) can be used for lower-cost non-critical  data [price]

- Object storage (vs block and file storage)
  - bucket01             â† independent of any server, no object limits
    â”‚                      automatically replicated.
    â”‚                    BÂºtop level namespaceÂº
    â”‚                      http.//bucket01.s3.amazonaws.com/
    â”‚                      - up to 63 lowercase [a-z][0-9][\/-.]
    â”‚
    â”œâ”€"object1"          â† Objects contain data and metadata Up to 5TB.
    â”‚  â”‚                   http.//bucket01.s3.amazonaws.com/object1 Âº*1Âº
    â”‚  â”‚
    â”‚  â”œâ”€ metadata       â† key:value dictionary/map
    â”‚  â”‚  â”œâ”€ system: (last mod. time, size, md5 digest, Content-type,..)
    â”‚  â”‚  â””â”€ user  :
    â”‚  â””â”€ (opaque to AWS)
    â”‚  â”‚
    â”œâ”€"dir1/dir2/object2"â† dir1/dir2 is just an string
    â”‚                    BÂºREST API support search-by-prefixÂº
    â”œâ”€..
      ^^^^^^^^^^^^^^^^^^
    - Each object is represented by itsÂºkeyÂº(sort of "filename")
      (Up to 1024 bytes UTF-9 chars)
    - Access through HTTP verbs GET/PUT/POST/DELETE to:
      - CREATE/DELETE buckets. LIST KEYS in bucket.
      - WRITE/READ/DELETE an object
        MF Auth. can be activated to protect DELETE operations.
      - Multipart Upload API allows to pause/retry/resume upload of
        objects.  Recomended for 100MB+. Forced for 5GB+.
        - Steps: (initialization, upload of part, completion) .
        - Range GETs allows to download a range of the object.
    - Cross-region replica: aysnch src_bucket_reg1 â†’ dst_bucket_reg2
      (metadata and ACLs are also replicated).
      Useful toÂºdecrease read latencyÂº, backups, ...
      PRE-SETUP: Enable versioning in src/dst buckets and a IAM policy
                 given AWS S3 permission to replicate on owner behalf.
    - PUT/POST/COPY/Multipart-Completion/DELETE/ÂºEventsÂºcan be
      activated atÂºbucket level,ÂºalsoÂºfiltering by keys and key-prefixesÂº
      and sent through AWSÂºSimple Notification Service (SNS)Âºor
     ÂºSimple Queue Service (SQS)ÂºorÂºLambdas (as input)Âº.

    -BÂºVersioning can be enabled for buckets affecting all its objectsÂº
     RÂºWARNÂº: once activated it can be suspended, but not removed.
    - Access logs can be enabled for a bucket, indicating the destination
      bucket for new logs. Best pracite: add a "logs/" prefix.

  Âº*1ÂºOÂºpre-signed URLsÂºcan be created indicating the bucket name,
       object key, HTTP methods and expiration date+time.

BÂºS3 STORAGE CLASSESÂº:
  -ÂºS3 standard:Âºhigh durability+availability, low (fist-byte) latency
                 (general purpose data).
  -ÂºS3 standard-infrequent access:Âº (long lived data)
    As S3, but with lower GB/month storage cost

  -ÂºS3 Reduced Redundancy Storage (RRS):ÂºLower durability (4 nines)/
    lower costs. (easy to reproduce data) [price]

  -ÂºGlacier:Âº"cold data"
    - Extremenly low cost [price]
    - Can be used as S3 storage class or independent service
    - Issue a "RESTORE" command and wait 3-5 hours to put data
      back in S3 RRS.

- Lifecycle policies (Mng Console or APIs)
  - Help migrate to appropiate class with no code changes. Ex:
    - "hot" storage (S3 Standard)
    - "warn" (S3 infrequent access) 30 days later
    - "cold" (Glacier),             90 days later
    - "delete".                      3 years later.

- S3 AAA:
  - By default only owner is allowed access. Access to third parties through:
    - ACLs, legacy, READ/WRITE/FULL CONTROL coarse-grain Access  to object or buckets.
    - bucket policies  fine-grain Access
      Similar to Identity and Access Management IAM policies but:
      - linked to buckets (vs IAMs principals).
      - include an explicit ref. to the IAM principal. This principal
        can be associated to different AWS accounts.
      - Allows to set:
        - who can access
        - from where (IP or Classless Inter-Domain Routing CIDR)
        - time of day.

- Data At rest encryption:
  - Server-side encryption (SSE) is used to encrypt S3 objects
    (256 AES). (Client side encrypt. can also be used).
  - SSE-S3 AWS managed keys: Let AWS manage keys. Every object is encrypted
    with a unique key first, then with a master key later.
    - AWS rotates master keys (at least) montly.
  - SSE-KMS keys: AWS handles key-management. User handles keys.
    It also provides auditing: "who used a given key to access which object".
  - SSE-C (Custome-provided keys): Use encryption keys without implementing
    client-side encryption libraries. AWS does encrypt/decrypt with keys
    out of AWS control.
  - Client-side encryption options:
    - use AWS KMS-managed customer master key.
    - use client-side master key.



- S3 Common Use: Static Website Hosting.
  â†’ Create bucket â†’ Upload static files â†’ mek files public (world readable)
    â†’ ÂºEnable static website hosting for bucketÂº:
       â†’ set index and error page objects.
         Site will then be available as:
         $bucket_name.s3-website-$region.amazonaws.com
       â†’ Create "riendly" DNS name using :
         -     standard CNAME DNS registry
         - non-standard alias DNS registry (Amazon Route 53 only)
<hr/>
<span xsmall>Glacier</span>
 - Vault â† Up to 1000 vaults per AWS account
   â”” lock â† used to deploy/enforce compliance controls through policies.Âº*1Âº
   â”” archive1-UUID â† encrypted, immutable, Up to 40TB,
   â”” archive2-UUID â† Unique (non-friendly) ID created by AWS at creation time
   â”” ...
   Âº*1Âº ex: Write Once, read Many. Policies can be locked (for always).
 - Up to 5% of data can be retrieved each month [price]
   Up to ??? daily. Beyond that, fee is incurred.
 - GlacierÂºcan be used as an S3 storage class to get most of the featuresÂº.
</pre>
<pre zoom labels="aws,storage,hybrid_cloud,TODO">
<span xsmall>EFS</span>
@[https://www.infoq.com/news/2020/01/aws-efs-iam-access-points/]
- EFS: Elastic File System Service
- simple, scalable, fully managed elasticÂºNFS file systemÂº (Cloud and on-premises)
</pre>

<pre zoom labels="aws,storage,data_lake,TODO">
<span xsmall>Lake Formation</span>
@[https://www.infoq.com/news/2019/08/aws-lake-formation-ga/]
- fully managed service that makes it much easier for customers
   to build, secure, and manage data lakes.
</pre>


</div>
<div groupv>
<span title>Compute: EC2 VMs</span>
<pre zoom labels="aws,computing,ec2,iaas,network,101,troubleshooting,price,security,performance,monitoring,automation,TODO" id="ec2_summary">
<span xsmall>EC2</span>
<!-- @ma page 92: http://127.0.0.1:8000/Cloud/AWS-Certified-Solutions-Architect-Official-Study-Guide.pdf.pdf -->

- VM classified in familes with similar ratio of vCPUs/RAM.
  - For each family vCPU/RAM/price scales linearly. 
  - 't2.micro' is free tier.                                        [price]
      ^2 indicates generation. It changes over time.
     RÂºWARNÂº: T2 / T3 unlimited provides *unlimited burst* but extra pays
              if going over credit balance.
  -RÂºWARNÂº: $0.01/GB for transfer among availability zones          [price]

  - compute main families types:
  | Type | Mnemonic   | Category           | Description                                   | Use-cases |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | M    | (M)ain/    | General purpose    | Balance of compute, memory, network resources | General, mid-size databases,
           (M)edium                                                                        | web apps |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | C    | (C)ompute  |  Compute optimized | Advanced CPUs                                 | Modeling, analytics, databases |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | H    | (H)DD      | Storage optimized  | Local HDD storage                             | Map reduce |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | R    | (R)am      | Memory Optimized   | More ram per $                                | In-memory caching |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | X    | (X)treme   | Memory Optimized   | Terabytes of RAM and SSD                      | In-memory databases |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | I    | (I)ops     | IO optimized       | Local SSD storage, High IOPS                  | NoSQL databases |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | G    | (G)PU      | GPU Graphics       | GPUs with video encoders                      | 3D rendering |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | P    | (P)ictures | GPU Compute        | GPUs with tensor cores                        | Machine learning |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | F    | (F)ield    | Accelerated        | Field Programmable Gate Array,                | Genomics, financial analytics |
                      | Computing          |  custom hardware accelerations                |
  | ---- | --------   | --------           | -----------                                   | --------- |
  | T    | (T)iny     | Burstable, Shared  | Web servers                                   |               [price]
           (T)urbo    | CPUs, lowest cost  |                                               |

   RÂºContrainÂº: Only if VM launched in an Amazon VPC.

- BÂºVM images typesÂº: [TODO]
@[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/virtualization_types.html]

BÂºAmazon Machine Image type (AMI)Âº:
- AMI: OS + soft + config. include:
  - Published by AWS (Note: Initial user is "ec2-user" for Linux instances)
  - AWS market place
  - Generated from Existing Instances
  - Uploaded+imported VM image (format: raw, VHD, VMDK, OVA).
[TODO] Amazon Kernel Image (AKI) and ARI Amazon Ramdisk Image (ARI)

- Instance IP and DNS name RÂºchange on rebootÂº. 
  ÂºElastic IPÂº (lifetime independent of VM) must be reserved  [network][troubleshooting]
               and assigned to the instance.
  ÂºElastic IPÂº is free of charge.                             [price][network]
   .

- TerminateRÂºdeletes the instance and its root storageÂº. Use stop to be able to reboot.
- EBS backed instances can be stopped (vs terminated) and restarted without data loss.
- Stopped instances are not charged, but associated EBS volumes are [price]



BÂºSecurity GroupsÂº: Virtual firewalls. Capabilites depends:            [security]
                           --------------------------------
          Sec.Group Type \ Capabilites
  ---------------------------------------------------------
   EC2-Classic Sec.Group   Control    out instance traffic
  ---------------------------------------------------------
   VPC         Sec.Group   Control in/out instance traffic
  ---------------------------------------------------------

-Âºhttp://169.254.169.254/latest/meta-data/Âº allow running VM to collect data
  about itself (associated security groups, instance ID/type, AMI). Ex:
  Get IAM credentials: `http://169.254.169.254/latest/meta-data/iam/security-credentials/$iam_role_name
-ÂºCloudWatchÂº can be used to monitor/alert about EC2 instances.        [monitoring]
-ÂºTermination protectionÂº: enable to avoid accidental VM termination.  [troubleshooting]

OÂºPricing optionsÂº
  - On-Demand  No upfront cost, 
  - Reserved   Up to 75% cheaper if used frequently for long time      [price]
  - Spot       Cheapest ones for non-critical or tolerant to interruption.
               Instance deallocated when another customer pays more for it.

BÂºTenancy OptionsÂº
  - Shared tenancy     : (def). host 1â†â†’ N Customers/M VMs
  - Dedicated instances:        host 1â†â†’ 1 Customer /M VMs
  - Dedicated host     :        host 1â†â†’ 1 Customer /1 host (useful to fix
                                                            licence issues)

<!-- @ma page 106: http://127.0.0.1:8000/Cloud/AWS-Certified-Solutions-Architect-Official-Study-Guide.pdf.pdf -->
BÂºPlacement GroupÂº
  - Place instance on same placement group (same hardware) for High bandwidth I/O [networking][performance]
  - When creating a placement group, we must switch among two strategies:
    - Cluster: place instances into low-latency group in a single Availability Zone
               - Same rack â†’ RÂºIf the rack fails, all instances fails at the same time.Âº
    - Spread : place instances across underlying hardware (max 7 instances per group per AZ)
               - Ensures all instances are located in different hardware.
               - Can span across multi AZ.
               - Minimizes failure risks.
    - Partition: Spreads instances across many different partitions 
                 (which rely on different sets of racks) within an AZ.
                 - Use cases: HDFS, HBase, Cassandra, Kafka
  - Use alsoÂºEnhanced networkingÂºto provide high-performance in a single root     [networking][performance]
             I/O virtualization (SR-IOV) and lower latency/jitter in supported
             instances (C4,C3,D2,I2,I3,H1,C5,M4,X1,M5,R3,...) 
             enhanced networking is free of charge                                [price]
             -RÂºLimitationsÂº:
               - Instances must be launched in a VPC
               - Instances must be launched from a HVM AMI
             - Elastic Network Adapter: High performance
             - Elastic Fabric Adapter: High Performance Computing (Linux only)


Ex: Install Ubuntu from EC2 image repository
  - Enter EC2 Console then "Launch Instance".
  Â· - Choose "Ubuntu HVM version" (looks to have better performance)
  Â· RÂºonly t1.micro is available for "Free tier"Âº
  Â· - Review setup and "Launch"
  â”” â†’ Adapt Security Settings
      Â· - click on "Edit security groups"
      Â·   - From the drop down list of the Source, select "My IP",
      Â·     then press "Review and Launch".
      Â· - review instance data again and "Launch"
      â””â†’ Create and download ÂºSSH Key PairÂº
         Â· - call the key "AWS_SSH_key" and download the generated PEM file
         â”” â†’ Check Instance Status
             - go to instnce and check that "Status Checks" are being performed.
             - Public IP and DNS name (FQDN) will be displayed too.

- virtualization types:
  - Hardware Virtual Machine (HVM), prefered, take advantage of HW extensions.   [performance]
  - Paravirtual (PV)

BÂºMonitoring InstancesÂº                                                          [monitoring]
  - Use CloudWatch: collect logs, monitor logs, set alarms.                      [troubleshooting]
  - basic monitoring by default free of cost, capturing data every 5 minutes     [price]
    Up to 2 weeks of history recorded.
  - detailed monitoring, every minute. Has non free cost
  - alarms: receive notifications or automated actions when crossing a threshold.[automatition]
            alarms can be simulated with AWS cli.

BÂºBackup and RecoveryÂº
  - Regularly: create an AMI from EC2 running instance as template to quickly restory custom config.
  - Regularly: back up EBS volumesÂºusing snapshots in Amazon S3.Âº
  - deploy critical components across multiple Availability Zones (AZs).
  - Attach manually an Elastic IP address or network interface to an alternate EC2 instance for recovery purposes
  - Regularly: test the process of recovering your instances from volume fail.
  - incremental backups is done through "Snapshots".

BÂºElastic Network Interface (ENI)Âº
  - 1 default ENI per instance with:
    - one primary private IPv4 (optionally 2+)
    - one ÂºElastic IPÂº (static IP) per private IPv4 (optional)
    - one public IPv4
    - one or more IPv6
    - one or more security group
    - a mac address
    - source destination check flag
    - description of ENI

- An instance may immediately terminate if:                      [troubleshooting]
  - EBS volume limit reached or snapshot is corrupt
  - Missing permissions to access the KMS key for EBS volume decryption

- TODO: Hibernating
  
BÂºEC2 User DataÂº                                                 [automation][vagrant]
- Simple alternative to Vagrant.
- Enables to bootstrap instances using EC2 data script.
  - ÂºbootstrappingÂº means launching commands when a machine starts
  â†’ Create EC2 â†’ Configure Instance â†’ Advanced Details â†’ User data 
    â†’ Paste commands:
      #!/bin/bash
      ...
</pre>

<pre zoom labels="aws,IaaS,devops,TODO">
<span xsmall>Vagrant</span>
@[https://vocon-it.com/2016/04/01/aws-automation-using-vagrant-a-hello-world-example/]

NOTE: .
   Vagrant potentially allows for more sophisticated provisioning tasks
   when compared to AWS CLI commands like Software Installation and upload
   and execution of arbitrary shell scripts.

NOTE: Vagrant creates a local dummy Vagrant box supporting the AWS provider,
  used only to spin up a remote AWS (AMI) image in the cloud.
  ÂºNo Vagrant box is uploaded during the processÂº

- PRE-SETUP:
  - (Optional) Set HTTP proxy, if needed
    - export http_proxy='http://myproxy.dns.name:8080'
    - export https_proxy='http://myproxy.dns.name:8080'
      ^^^^^^
      replace with set on Win*

  - Install the VagrantÂºAWS pluginÂº
    $ vagrant plugin install vagrant-aws

  -ÂºDownload dummy box:Âº
    $ vagrant box add dummy \
      https://github.com/mitchellh/vagrant-aws/raw/master/dummy.box

  - "init vagrant enviroment"
    $ mkdir MyVagrantBox
    $ cd    MyVagrantBox
    $ vagrant init
              ^^^^
              Will create a template Vagrantfile

  - Add next lines to "Vagrantfile"
    # Vagrantfile
    Vagrant.configure(2) do |config|
     config.vm.provider :aws do |aws, override|
       aws.access_key_id = ENV['AWS_KEY']
       aws.secret_access_key = ENV['AWS_SECRET']
       aws.keypair_name = ENV['AWS_KEYNAME']
       aws.ami = "ami-87564feb"                         â† See ami list in EC2 web console
       aws.region = "us-west-1"                         â† adapt to your (signed in) region
       aws.instance_type = "t2.micro"

       override.vm.box = "dummy"                        â†  Problem:
                                                           - Most boxes do not support AWS.
                                                           Work around:
                                                           - load dummy box with AWS provider
                                                             and override the image that
                                                              spin up in the Cloud

       override.ssh.username = "ubuntu"
       override.ssh.private_key_path = ENV['AWS_KEYPATH']  â† EC2 console/Net.Sec/Key Pairs
     end


   - Add a IAM user and apply the appropriate permissions
    - if not already done, create new user on the AWS IAM Users page, .
    - Assign required access rights to user like:
      - go to @[https://console.aws.amazon.com/iam/home?region=eu-central-1#policies]
                                                               ^^^^^^^^^^^^
                                                               adapt to your setup
      - Click the "Get Started" button, if the list of policies is not visible already:
        you should see the list of policies and a filter field.
      - In the Filter field, search for the term ÂºAmazonEC2FullAccessÂº (Policy)
      - Click on this policy, then choose the tab Attached Identities.
      - Click "Attach" button and attach the main user.


  - create the launch script like:
    cat launch_aws.sh
    #!/bin/sh
    export AWS_KEY='your-key'            â† Create them on the "users" tab
    export AWS_SECRET='your-secret'        of the IAM console:
    export AWS_KEYNAME='your-keyname'      - click on "create new users"
    export AWS_KEYPATH='your-keypath'        You will be displayed the needed key/secret
    vagrant up --provider=aws

  - ./launch_aws.sh
    Bringing machine 'default' up with 'aws' provider...
    ==> default: Warning! The AWS provider doesn't support any of the Vagrant
    ==> default: high-level network configurations (`config.vm.network`). They
    ==> default: will be silently ignored.
    ==> ...
    ==> default: Waiting for SSH to become available...
    ... RÂº(can take up to 20 minutes in free-tier)Âº
    ==> default: Machine is booted and ready for use!

  - Update the security group manually to allow SSH access to the instance.
    (Appendix B shows how to  automate with a shell script)
    Go to EC2 console/ÂºNetwork&SecurityÂº/Sec.Groups,
    - we can find the default security group.
    - Edit the inbound rule to allow the current source IP address.


ÂºDestroy the Instance (save money!!!) Âº

  $ vagrant destroy

<span xsmall>Vagrant+Docker</span>
@[https://vocon-it.com/2016/04/06/aws-automation-using-vagrant-part-3-creating-a-docker-host-on-aws-in-10-minutes-using-vagrant/]
</pre>
<pre zoom labels="aws,TODO,IaaS,IaC,devops">
<span xsmall>Terraform</span>
@[https://vocon-it.com/2016/09/22/aws-automation-part-4-using-terraform-for-aws-automation/]
@[https://www.terraform.io/docs/providers/aws/]
AWS Provider

- interact with many AWS resources.

ÂºPRE-SETUP:Âº
- Terraform provider credentials must be configured (TODO)
  The following methods are supported:
   - Static/hardcoded credentials. ÂºR(discouraged)Âº
     provider "aws" {
       region     = "us-west-2"
     }

ÂºExampleÂº:

provider "aws" {                 # â† STEP 1: Set provider
  version = "~Ëƒ 2.0"
GÂºregion  = "us-east-1"Âº         # â† alt: $ export AWS_DEFAULT_REGION="us-west-1"
# Credentials
# access_key =RÂº"my-access-key"Âº # â† Hardcoded credentials are discouraged
# secret_key =RÂº"my-secret-key"Âº # BÂºAlt 1: Use next ENV.VARsÂº
                                 #     (override use of AWS_SHARED_CREDENTIALS_FILE/AWS_PROFILE)
                                 #     -BÂºAWS_ACCESS_KEY_ID Âº
                                 #     -BÂºAWS_SECRET_ACCESS_KEY Âº
                                 #     -BÂºAWS_SESSION_TOKEN Âº (if applicable)
                                 #   Alt 2: Use Shared credentials file
                                 #     $HOME/.aws/credentials
                                 #     ^^^^^^^^^^^^^^^^^^^^^^
                                 #     Default location can be replaced by
                                 #     with AWS_SHARED_CREDENTIALS_FILE profile
                                 #     (also supporte by matching profile configuration
                                 #      AWS_PROFILE ENV.VAR)
                                 #     provider "aws" {
                                 #       ...
                                 #       shared_credentials_file = "/Users/tf_user/.aws/creds"
                                 #       profile                 = "customprofile"
                                 #     }
                                 #     AWS_SDK_LOAD_CONFIG=1 for advanced AWS client configs,
                                 #                           (profiles using source_profile or
                                 #                            role_arn configs)
                                 #

resource "aws_vpc" "example" {   # â† STEP 2: Create VPC
  cidr_block = "10.0.0.0/16"
}

<span xsmall bgorange>resource list</span>
@[https://www.terraform.io/docs/providers/aws/r/api_gateway_deployment.html]
@[https://www.terraform.io/docs/providers/aws/r/api_gateway_rest_api.html]
@[https://www.terraform.io/docs/providers/aws/r/api_gateway_stage.html]
@[https://www.terraform.io/docs/providers/aws/r/budgets_budget.html]
@[https://www.terraform.io/docs/providers/aws/r/cognito_identity_pool.html]
@[https://www.terraform.io/docs/providers/aws/r/cognito_user_pool.html]
@[https://www.terraform.io/docs/providers/aws/r/cognito_user_poolsdatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/dms_replication_subnet_group.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_connection.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_hosted_private_virtual_interface_accepter.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_hosted_private_virtual_interface.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_hosted_public_virtual_interface_accepter.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_hosted_public_virtual_interface.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_lag.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_private_virtual_interface.html]
@[https://www.terraform.io/docs/providers/aws/r/dx_public_virtual_interface.html]
@[https://www.terraform.io/docs/providers/aws/r/ebs_volumedatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/ec2_transit_gateway.html]
@[https://www.terraform.io/docs/providers/aws/r/ec2_transit_gateway_route_table.html]
@[https://www.terraform.io/docs/providers/aws/r/ec2_transit_gateway_vpc_attachment.html]
@[https://www.terraform.io/docs/providers/aws/r/ecs_cluster.html]
@[https://www.terraform.io/docs/providers/aws/r/ecs_service.html]
@[https://www.terraform.io/docs/providers/aws/r/efs_file_systemdatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/efs_file_system.html]
@[https://www.terraform.io/docs/providers/aws/r/efs_mount_targetdatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/efs_mount_target.html]
@[https://www.terraform.io/docs/providers/aws/r/eip_association.html]
@[https://www.terraform.io/docs/providers/aws/r/eip.html
@[https://www.terraform.io/docs/providers/aws/r/eip.html]
@[https://www.terraform.io/docs/providers/aws/r/elasticache_clusterdatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/elasticache_cluster.html]
@[https://www.terraform.io/docs/providers/aws/r/elb.html]
@[https://www.terraform.io/docs/providers/aws/r/glue_crawler.html]
@[https://www.terraform.io/docs/providers/aws/r/instancedatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/instance.html]
@[https://www.terraform.io/docs/providers/aws/r/internet_gateway.html]
@[https://www.terraform.io/docs/providers/aws/r/launch_template.html]
@[https://www.terraform.io/docs/providers/aws/r/redshift_cluster.html]
@[https://www.terraform.io/docs/providers/aws/r/redshift_subnet_group.html]
@[https://www.terraform.io/docs/providers/aws/r/route.html]
@[https://www.terraform.io/docs/providers/aws/r/route_table_association.html]
@[https://www.terraform.io/docs/providers/aws/r/route_table.html]
@[https://www.terraform.io/docs/providers/aws/r/s3_account_public_access_block.html]
@[https://www.terraform.io/docs/providers/aws/r/security_group.html]
@[https://www.terraform.io/docs/providers/aws/r/ses_domain_identity.html]
@[https://www.terraform.io/docs/providers/aws/r/ses_domain_identity_verification.html]
@[https://www.terraform.io/docs/providers/aws/r/ssm_document.html]
@[https://www.terraform.io/docs/providers/aws/r/ssm_parameter.html]
@[https://www.terraform.io/docs/providers/aws/r/subnet.html]
@[https://www.terraform.io/docs/providers/aws/r/vpcdatasource.html]
@[https://www.terraform.io/docs/providers/aws/r/vpc.html]
@[https://www.terraform.io/docs/providers/aws/r/waf_ipset.html]
@[https://www.terraform.io/docs/providers/aws/r/wafregional_ipset.html]
</pre>
<pre zoom labels="aws,TODO,arm,devops">
<span xsmall>ARM CPU</span>
@[https://www.datacenterknowledge.com/amazon/aws-launches-cloud-instances-powered-custom-arm-chips]
@[https://aws.amazon.com/es/blogs/aws/new-ec2-instances-a1-powered-by-arm-based-aws-graviton-processors/]
</pre>

<pre zoom labels="aws,TODO,scaling">
<span xsmall TODO>Autoscaling</span>
@[https://aws.amazon.com/autoscaling/]

AWS Auto Scaling monitors your applications and automatically adjusts
capacity to maintain steady, predictable performance at the lowest possible
cost.
application scaling for multiple resources across multiple services.
- EC2 instances and Spot Fleets
- ECS tasks
- DynamoDB tables and indexes
- Aurora Replicas.
</pre>

<pre zoom labels="aws,IaaS,VM,security,TODO">
<span xsmall>ssh over SSM</span>
@[https://github.com/PoeBlu/ssh-over-ssm]
SSH over AWS SSM. No bastions or public-facing instances. SSH user management
through IAM. No requirement to store SSH keys locally or on server.
</pre>

</div>
<div groupv>
<span title>Compute: Others</span>
<pre zoom labels="aws,TODO,">
<span xsmall>Firecracker Ligthweight Virtualization</span>
@[http://www.eweek.com/security/aws-boosts-serverless-security-with-firecracker-microvms]

@[https://firecracker-microvm.github.io]
Firecracker implements a virtual machine monitor (VMM) that uses the Linux
Kernel-based Virtual Machine (KVM) to create and manage microVMs. Firecracker has
a minimalist design. It excludes unnecessary devices and guest functionality to
reduce the memory footprint and attack surface area of each microVM.
This improves security, decreases the startup time, and increases hardware utilization.
Firecracker currently supports Intel CPUs, with planned AMD and Arm support.
Firecracker will also be integrated with popular container runtimes such as containerd.
</pre>

<pre zoom labels="aws,ecs,compute,container,TODO" id="aws_ecs_summary">
<span xsmall>ECS summary</span>
@[https://aws.amazon.com/es/ecs/]
- ECS: Elastic Container Service
- """ Highly secure, reliable, and scalable way to run containers """
- "because ECS has been a foundational pillar for key Amazon services,
   it can natively integrate with other services such as Amazon Route 53,
   Secrets Manager, AWS Identity and Access Management (IAM),
   and Amazon CloudWatch providing you a familiar experience to deploy

BÂºCREATE TASKs:Âº

$Âº$ POLICY_FILE="file://task-execution-assume-role.json"            Âº
$Âº$ aws iam --region $REGION create-role \                          Âº
$Âº    --role-name ecsTaskExecutionRole \                            Âº
$Âº    --assume-role-policy-document ${POLICY_FILE}                  Âº

$Âº$ ARN="arn:aws:iam::aws:policy/service-role"                      Âº
$Âº$ ARN="${ARN}/AmazonECSTaskExecutionRolePolicy"                   Âº
$Âº$ aws iam --region $REGION attach-role-policy \                   Âº
$Âº    --role-name ecsTaskExecutionRole \                            Âº
$Âº    --policy-arn ${ARN}                                           Âº

$Âº$ aws logs create-log-group \                                     Âº
$Âº$   --log-group-name /ecs/project01 \                             Âº
$Âº$   --tags system-name=zonpro,environment=POC                     Âº

$Âº$ aws ecs create-cluster --cluster-name cluster01 \               Âº
$Âº$   --tags key=system-name,value=zonpro key=environment,value=POC Âº

$Âº$ AWS_VPC_CONFIG="{"                                              Âº
$Âº$ AWS_VPC_CONFIG="${AWS_VPC_CONFIG}subnets=[subnet-0....]"        Âº
$Âº$ AWS_VPC_CONFIG="${AWS_VPC_CONFIG},securityGroups=[sg-0..]"      Âº
$Âº$ AWS_VPC_CONFIG="${AWS_VPC_CONFIG},assignPublicIp=ENABLED"       Âº
$Âº$ AWS_VPC_CONFIG="}"                                              Âº
$Âº$ aws ecs create-service --cluster cluster01 \                    Âº
$Âº     --service-name zonpro-service01 \                            Âº
$Âº     --task-definition project01:4 --desired-count 1 \            Âº
$Âº     --launch-type "SERVICE01" \                                  Âº
$Âº     --network-configuration ${AWS_VPC_CONFIG}                    Âº

BÂºCHECKSÂº
$Âº$ aws ecs register-task-definition \                              Âº
$Âº     --cli-input-json file://./task.json                          Âº
$Âº$ aws ecs list-task-definitions                                   Âº
$Âº$                                                                 Âº
$Âº$ aws ecs describe-services --cluster cluster01 \                 Âº
$Âº     --services zonpro-service01                                  Âº
$Âº$ aws ecs list-tasks --cluster cluster01                          Âº
$Âº$ aws ecs describe-tasks --cluster cluster01 --tasks $taskId      Âº

$Âº$ aws logs describe-log-streams \                                 Âº
$Âº     --log-group-name /ecs/project01                              Âº

$Âº$ aws logs get-log-events --log-group-name /ecs/project01         Âº
$Âº     --log-stream-name ecs/middleware/034ec756-36d0-8384-...

BÂºCREATE REPOÂº
$ aws sts get-caller-identity
$ aws ecr create-repository \
      --repository-name $REPO_NS/$DOCKER_IMAGE_NAME \
      --image-tag-mutability MUTABLE

BÂºCLEAR ECS:Âº
$Âº$ aws ecs deregister-task-definition \                            Âº
$Âº    --task-definition project01:1Âº                                Âº
$Âº$ aws ecs delete-service --cluster cluster01 \                    Âº
$Âº    --service zonpro-service01 --force                            Âº
$Âº$ aws ecs delete-cluster --cluster cluster01                      Âº
$Âº$ aws logs delete-log-group --log-group-name /ecs/project01       Âº

<hr/>
<span xsmall>ECR</span>
- ECR: Elastic Container Registry (private "Dockerhub")
@[https://aws.amazon.com/ecr/]

<span xsmall>AWS CI flow</span>

AWS CI flow "==" CodePipeline + CodeBuild.

Ex. Dev Pipeline:                                  Flux CD scans every 2
                                                   minutes the Registry
                                                   for new images
                                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                   AWS CodeBuild        v         â”‚
â”‚Devâ”‚ â†’ git push â†’ â”‚ AWS      â”‚ â†’  (run Buildspec) â†’  â”‚AWSâ”‚     â”‚AWSâ”‚
                   â”‚CodeCommitâ”‚    - testÂº*1Âº         â”‚ECRâ”‚     â”‚EKSâ”‚
                   â”‚repositoryâ”‚    - package            â”‚        ^
                                   - build OCI image    â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   (or Github)                       On new image detected
                                                     trigger new deployment

                                                           â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜
                                                         Integration with
                                                       ECR will be similar
Âº*1:Âº Run unit tests, Sonarqube(QA), ...
      Deploying sources to the S3 Repository of artifacts.

Buildspec:
  - build (stages) specification YAML file
  - collection of build commands and related settings
  - it can be placed in source code or s3.
  - note:  if 'install' or 'pre_build' fails, build stops.
           if build fails, post_build is still executed.


Example Buildspec:
   version: 0.2      â† Recomended version
   phases:
    Âºinstall:Âº                 â† Setup build packages and variables
     Â· runtime-versions:
     Â·   java: corretto11
     Â· commands:
     Â·   # Install Maven
     Â·   - wget https://jcenter.bintray.com/org/apache/maven/apache-maven/3.6.3/apache-maven-3.6.3-bin.tar.gz
     Â·   - tar xzvf apache-maven-3.6.3-bin.tar.gz -C /opt/
     Â·   # Add Maven to classpath
     Â·   - export PATH=/opt/apache-maven-3.6.3/bin:$PATH
     Â·   # Extract Artifact ID
     Â·   - ARTIFACT_ID=$(mvn help:evaluate -Dexpression=project.artifactId -q -DforceStdout)
     Â·   # Extract Group ID and get its domain
     Â·   - GROUP_ID=$(mvn help:evaluate -Dexpression=project.groupId -q -DforceStdout)
     Â·   - DOMAIN=${GROUP_ID##*.}
     Â·   # Extract Version
     Â·   - VERSION=$(mvn help:evaluate -Dexpression=project.version -q -DforceStdout)
     Â·   # Login to AWS ECR
     Â·   - $(aws ecr get-login --region $AWS_DEFAULT_REGION --no-include-email)
     Â·   # Assign ECR Repository URI
     Â·   - REPOSITORY_URI=${AMI_ID}.dkr.ecr.eu-west-1.amazonaws.com/${DOMAIN}-${ARTIFACT_ID}
    Âºbuild:Âº
     Â· commands:
     Â·   # Validate, compile and test
     Â·    - mvn test -T 2C
    Âºpost_build:Âº
       commands:
         - |
           if [[ $CODEBUILD_BUILD_SUCCEEDING == 1 ]] ; then
              mvn deploy \
                -Dmaven.test.skip \
                -DaltDeploymentRepository=snapshots::default::s3://s3mavenrepo01/snapshots
             aws s3 cp s3://automation-yaml-files-location/obp-microservice-deploy/Dockerfile Dockerfile
             docker build -t $REPOSITORY_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION --build-arg ARTIFACT_ID=$ARTIFACT_ID --build-arg VERSION=$VERSION .
             docker push $REPOSITORY_URI:$CODEBUILD_RESOLVED_SOURCE_VERSION
           fi
<hr/>
<span xsmall>ECS vs EKS</span>
@[https://www.missioncloud.com/blog/amazon-ecs-vs-eks-which-service-is-right-for-you]
</pre>

<pre zoom labels="aws,CodeCommit,git,TODO">
<span xsmall>Setup CodeCommit Access</span>
REF: @[https://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-gc.html?icmpid=docs_acc_console_connect_np]
(Cli REF @[https://docs.aws.amazon.com/codecommit/latest/userguide/cmd-ref.html])
- Setup for HTTPS users using Git credentials
  configure Git credentials:
  - simplest way: Setup credentials in IAM console
    then use those credentials for HTTPS connections.

NOTE: if local computer already configured to use
      credential helper for CodeCommit, then remove
      such info in '.gitconfig'.

Step 1: Initial configuration for CodeCommit

Follow these steps to set up an AWS account, create an IAM user, and configure access to CodeCommit.

To create and configure an IAM user for accessing CodeCommit

    http://aws.amazon.com â†’ Sign Up
      â†’ Create or reuse IAM user
        - Check  access-key-ID and a secret-access-key associated
          to IAM user are in place.
        - Check  no policies attached to the user expressly deny
          AWS KMS actions required by CodeCommit.
          (CodeCommit requires 'KMS')
          (see AWS KMS and encryption)
        â†’ Sign into https://console.aws.amazon.com/
          â†’ Open IAM console https://console.aws.amazon.com/iam/
            â†’ choose Users (@navigation pane)
              â†’ choose IAM user for CodeCommit:
                â†’ choose Add Permissions (@Permissions tab)
                  â†’ "Attach existing policies directly"
                    (@Grant permissions)
                    â†’ List of Policies:
                      select AWSCodeCommitPowerUser
                      (or another managed policy for CodeCommit access).
                      â†’ Next â†’ Review (review list of policies)
                        â†’  click "Add permissions".

      â†’ Create Git credentials for HTTPS connections to CodeCommit
        â†’ https://console.aws.amazon.com/iam/
        RÂºWARNÂº: Sign in as IAM user who will create and use Git credentials
          â†’ Users â†’ choose IAM user from list.
            â†’ Security Credentials tab@User Details
              â†’ HTTPS Git credentials for AWS CodeCommit
                â†’ Generate (You cannot choose your own user name
                  or password for Git credentials).
                â†’ Copy user name + password  and Close
                  RÂºWARNÂº: Password can not be recovered later on,
                           it must be reseted.

      â†’ Connect to the CodeCommit console and clone the repository
        (Can be skipted if some admin has already sent the name and
         connection details for the CodeCommit repository)
        â†’ Open https://console.aws.amazon.com/codesuite/codecommit/home
          â†’ Choose correct AWS Region.
            â†’ Find and choose appropiate repository
              â†’ click Clone URL  â†’ choose protocol â†’ copy URL
              â†’ Al local PC console: $ git clone $URL
                Ex:
                $ git clone https://git-codecommit.us-east-2.amazonaws.com\
                            /v1/repos/MyDemoRepo my-demo-repo

- See also:
  https://docs.aws.amazon.com/codecommit/latest/userguide/how-to-share-repository.html
  https://docs.aws.amazon.com/codecommit/latest/userguide/auth-and-access-control.html
</pre>
</div>

<div groupv>
<span title>Compute: Lambdas</span>
<pre zoom labels="serverless,lambda,java,TODO">
<span xsmall>Quarkus+Lambdas</span>
@[https://quarkus.io/guides/amazon-lambda-http]
</pre>
</div>

<div groupv>
<span title>Infra. As Code</span>
<pre zoom labels="aws,TODO,IaC" bgorange>
<span xsmall>AWS CDK</span>
Home   @[https://aws.amazon.com/cdk/]
Doc    @[https://docs.aws.amazon.com/cdk/latest/guide/home.html]
GitHub @[https://github.com/aws/aws-cdk]
Gitter @[https://gitter.im/awslabs/aws-cdk]
StackO @[https://stackoverflow.com/questions/tagged/aws-cdk]

-ÂºJAVA API Ref:Âº
@[https://docs.aws.amazon.com/cdk/api/latest/java/index.html]

- TypeScript and Python:
@[https://aws.amazon.com/blogs/aws/aws-cloud-development-kit-cdk-typescript-and-python-are-now-generally-available/]
</pre>

<pre zoom labels="aws,TODO,IaC,comparative">
<span xsmall>CloudFormation</span>
<span xsmall>(AWS IaC) API REF</span>
<span xsmall>"deprecated"</span>
RÂºNote: AWS CDK is prefered:Âº
  - A 10 lines AWS-CDK produces a 500 lines CloudFormation config file.
  @[https://docs.aws.amazon.com/cdk/latest/guide/home.html]


AWS CloudFormation:
- common language to describe and provision all
  the infrastructure resources in a cloud environment,
  using a programming languages or a simple text file
  to model and provision, in an automated and secure manner,
  all the resources needed for your applications across all
  regions and accounts.
ÂºThis gives you a single source of truth for your AWS resources.Âº

@[https://docs.aws.amazon.com/AWSCloudFormation/latest/APIReference/]
- See also @[https://www.pulumi.com/]
           @[https://www.pulumi.com/docs/intro/vs/cloud_templates/]

@[https://www.reddit.com/r/aws/comments/bfh7mj/how_to_be_more_productive_with_cloudformation/]
"""
  Iâ€™ve used a tool called Sceptre (https://github.com/cloudreach/sceptre) with
  a lot of success. Iâ€™ve found that using vanilla CloudFormation via the aws-
cli to be very frustrating in comparison.

    It defines some nice conventions to using Cloud Formation. For example,
stack configuration (e.g. parameters, region, etc.) are stored in YAML
files in a configuration directory, where CloudFormation templates are
stored in the templates/ directory. Cloud Formation stacks are named via
convention of the file name and path.

   The CLI is a lot easier to use. Rather than having to switch between
create-stack, delete-stack, and update-stack â€” you can simple run
sceptre launch <stack>. Sceptre will figure out if the stack needs to
be created or is in a UPDATE_ROLLBACK_FAILED state â€” and either
create the stack, update the stack, or delete it and re-create it.

   Additionally, it shows the output of the CloudFormation events pane right
in the CLI so you donâ€™t have to navigate windows to see logs.

   3. You can extend Sceptre to add functionality. For example, we store some
secrets in SecretsManager and it was trivially easy to configure Sceptre
to pull a secret out of SecretsManager and pass the encrypted string as a
parameter to a CloudFormation
template.

   4. It allows you to use Jinja templates â€” which greatly simplifies
CloudFormation templates with a lot of repetition (e.g. VPC / Subnet stacks
across multiple AZs)

   I was personally drawn to it because I could use native CloudFormation. I
could use all of my other tooling and resources. It just added a very nice
convention over top of it. Weâ€™ve used it for the last 10 months or so and
it has been very, very nice!
""

ÂºRELATEDÂº
Is there a common wrapper around AWS/Azure/GCloud/...?
@[https://stackoverflow.com/questions/52691127/is-there-a-wrapper-for-aws-azure-gcloud-apis]
"""...For serverless infrastructure you could use Serverless Framework
  (https://serverless.com/framework, tag:serverless-framework).
- With this framework you can deploy serverless infra to all these clouds
  with minimal changes to the actual source code."""
</pre>
</div>

<div groupv>
<span title>Hybrid Cloud</span><br/>
<pre zoom labels="aws,IaaS,hybrid_cloud,TODO">
<span xsmall>AWS Outposts</span>
https://www.infoq.com/news/2020/01/AWS-Outposts-Hybrid/
Amazon Releases AWS Outposts, Enabling Hybrid Data Center Architectures

In a recent blog post, Amazon announced the release of AWS Outposts,
which allows AWS customers to take advantage of a single-vendor
compute and storage solution. The Outposts architecture is based upon
Amazon public cloud compute architecture but is hosted in a customer
data center. This solution allows customers to take advantage of AWS
technology, but addresses local processing and low latency
requirements. Customers place infrastructure orders online, Amazon
will then ship the modular compute rack and a trained AWS technician
will connect, set up and validate the installation.
</pre>

<pre zoom labels="aws,IaaS,hybrid_cloud,TODO">
<span xsmall>Systems Manager</span>
@[https://blogs.sequoiainc.com/raspberry-pi-remote-management-with-aws-ec2-systems-manager/]
- Remote Management agent based platform for configuring, controlling,
  and governing on premise servers from within the EC2 console.
BÂºHow-To:Âº
- install Systems Manager agent on on-premises server, then
  execute commands remotely, ensure servers remain in specific state,
  and enforce configuration management requirements.
</pre>
</div>

</div>

<div groupv>
<span title>Network</span><br/>
<pre zoom labels="aws,networking,TODO">
<span xsmall>ENI</span>
@[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-eni.html]
- An elastic network interface is a logical networking component in a
  VPC that represents a virtual network card. It can include the
  following attributes:
  - A primary private IPv4 address from the IPv4 address range of your VPC
  - One or more secondary private IPv4 addresses from the IPv4 address range of your VPC
  - One Elastic IP address (IPv4) per private IPv4 address
  - One public IPv4 address
  - One or more IPv6 addresses
  - One or more security groups
  - A MAC address
  - A source/destination check flag
  - A description
</pre>

<pre zoom labels="aws,network,TODO">
<span title>Network:VPC</span>
Amazon Virtual Private Cloud (Amazon VPC) lets you provision a logically
isolated section of the AWS Cloud where you can launch AWS resources in a
virtual network that you define. You have complete control over your virtual
networking environment, including selection of your own IP address range,
creation of subnets, and configuration of route tables and network gateways.
You can use both IPv4 and IPv6 in your VPC for secure and easy access to
resources and applications.
</pre>
<pre zoom labels="aws,TODO,network">
<span xsmall>Network:ELB</span>
Elastic Load Balancing automatically distributes incoming application traffic
across multiple targets, such as Amazon EC2 instances, containers, IP
addresses, and Lambda functions. It can handle the varying load of your
application traffic in a single Availability Zone or across multiple
Availability Zones. Elastic Load Balancing offers three types of load balancers
that all feature the high availability, automatic scaling, and robust security
necessary to make your applications fault tolerant.
</pre>

<pre zoom labels="aws,TODO,network">
<span title>Network:Direct Connect</span>
AWS Direct Connect is a cloud service solution that makes it easy to establish
a dedicated network connection from your premises to AWS. Using AWS Direct
Connect, you can establish private connectivity between AWS and your
datacenter, office, or colocation environment, which in many cases can reduce
your network costs, increase bandwidth throughput, and provide a more
consistent network experience than Internet-based connections.
</pre>
<pre zoom labels="aws,TODO,network">
<span title>DNS:Route 53</span>
Amazon Route 53 is a highly available and scalable cloud Domain Name System
(DNS) web service. It is designed to give developers and businesses an
extremely reliable and cost effective way to route end users to Internet
applications by translating names like www.example.com into the numeric IP
addresses like 192.0.2.1 that computers use to connect to each other. Amazon
Route 53 is fully compliant with IPv6 as well.
</pre>
</div>

<div groupv>
<span title>Decoupled system integration. Events and Message queues</span>
<pre zoom labels="aws,message_stream,TODO">
<span xsmall>EventBridge</span>
Recently Amazon announced the general availability of the Schema
Registry capability in the Amazon EventBridge service. With Amazon
EventBridge Schema Registry, developers can store the event
structure-or schema-in a shared central location and map those
schemas to code for Java, Python, and Typescript, meaning that they
can use events as objects in their code.

With this new feature, Amazon's EventBridge is now a competitive
service in comparison with other cloud vendors that provide similar
services. Microsoft offers EventGrid, which has been GA since the
beginning of 2018 and received several updates including advanced
filtering, retry policies, and support for CloudEvents. However, the
service lacks a schema registry capability. Moreover, the same
applies to Triggermesh's EveryBridge. This event bus can consume
events from various sources, which developers can use to start
serverless functions that are running on any of the major cloud
providers as well as on-premises.
</pre>

</div>

<div groupv>
<span title>Full Text Search</span>
<pre zoom labels="aws,search_engine,TODO">
<span xsmall>Kendra</span>
@[https://aws.amazon.com/es/kendra/]
- highly accurate and easy to use enterprise search service
- powered by machine learning.
- natural language search capabilities to websites and applications
- content added from file systems, SharePoint, intranet sites,
  file sharing services, ... into a centralized location.
</pre>
</div>



<div groupv>
<span title>monitor, troubleshoot, and optimize</span>
<pre zoom labels="aws,troubleshoot,TODO,">
<span title>Troubleshooting</span>
<span xsmall>basics</span>
- Check aws.region!!!!
</pre>

<pre zoom labels="aws,troubleshoot,monitoring,IaaS,TODO,">
<span xsmall>CloudWatch</span>
@[https://aws.amazon.com/cloudwatch/]
- monitoring and observability service built for:
  - DevOps engineers
  - developers
  - site reliability engineers (SREs)
  - IT managers.
- Cloud and on-premises
- CloudWatch provides data and actionable insights to:
  - monitor applications/resources/services
  - respond to system-wide performance changes
  - optimize resource utilization
  - get a unified view of operational health.
-  collected  data from logs, metrics, and events
- detect anomalous behavior and set alarms.

<hr/>
<span xsmall>CWL</span>
@[https://github.com/PoeBlu/cwl]
- CloudWatch Logs CLI, helping to monitor CloudWatch logs on the command line.
  - The AWS CLI displays logs in JSON format, and while that can be processed
    with another tool like jq, it's a bit of a pain.
  - cwl simplify parameters, choosing sane defaults.
</pre>


<pre zoom labels="aws,TODO,cache,performance">
<span xsmall>Elastic Cache</span>
@[https://aws.amazon.com/elasticache/]
- seamlessly set up, run, and scale popular
open-Source compatible in-memory data stores in the cloud. Build data-intensive
apps or boost the performance of your existing databases by retrieving data
from high throughput and low latency in-memory data stores. Amazon ElastiCache
is a popular choice for real-time use cases like Caching, Session Stores,
Gaming, Geospatial Services, Real-Time Analytics, and Queuing.

Amazon ElastiCache offers fully managed Redis and Memcached for your most
demanding applications that require sub-millisecond response times.
</pre>

<pre zoom labels="aws,storage,sql,serverless,TODO">
<span xsmall>RDS proxy</span>
@[https://www.infoq.com/news/2020/07/aws-rds-proxy/]
https://aws.amazon.com/rds/proxy/
- fully managed, highly available database proxy for MySQL and PostgreSQL
  running on Amazon RDS and Aurora.
- Tailored toÂºarchitectures opening/closing database connections at a high rateÂº
  (Serverless,...)

- RDS Proxy allows apps to pool and share connections established with the database.
- Avoid exhausting database memory and compute resources.
- Corey Quinn, cloud economist and author of the  Last Week in AWS newsletter,
  summarized: "...This solves the '10,000 Lambda functions just hugged your database to deathâ€™
  problem"...'
</pre>
</div>
<div group><!-- AWS App Development -->
<span title>App Development</span><br/>

<pre zoom labels="development,git,ci,devops,low_code,aws,TODO">
<span xsmall>CodeStar</span>
@[https://aws.amazon.com/codestar/]
Easily manage software development activities in one place.

- Applications on AWS.
- Set up your entire continuous delivery toolchain
  (develop, build, deploy/delivery) in minutes.
- Unified user interface.
- Easily manage access with project built-in role-based
  policies that follow IAM best practices:
  - owners
  - contributors
  - viewers
  (No need to manually configure custom policies for each service)

- Integration with AWS CodeDeploy and AWS CloudFormation
  to deploy in EC2 and AWS Lambda.

- project templates for EC2, AWS Lambda, and AWS Elastic Beanstalk
  (Java, JS, Python, Ruby, PHP)
  -  Visual Studio, Eclipse or AWS cli.
- project management dashboard.
  - issue tracking (powered by JIRA)
    from backlog of work items to teams' recent code deployments.

    https://aws.amazon.com/codestar/features/

-OÂºZERO COSTÂº
  - Charged only for AWS resources provisioned for devel/run
    apps.

- Source Control alternatives:
  - AWS CodeCommit
  - GitHub

- Centralize monitoring for commits, builds, tests, deployments.

BÂºAWS CodeCommitÂº:
  - "Github" like fully-managed build service that makes it
    possible to build, test, and integrate code more frequently.
  - High Availability and Durability
    (S3 and DynamoDB as storage backend).
  - Encrypted data redundantly stored across multiple facilities.
  - up to 1,000 repositories by default and no limits upon request.
  - AWS SNS Notifications and Custom Scripts.
    notifications include status message + link to resource
  - Amazon SNS HTTP webhooks or Lambda functions reactive
    programming

BÂºAWS CodeBuildÂº:
  - fully-managed build service (build, test, integrate)

BÂºAWS CodePipelineÂº:
  - Continuous integration and continuous delivery (CI/CD) service.
  - Each project comes pre-configured with an automated pipeline
    that continuously builds, tests, and deploys your code with
    each commit.
</pre>
<pre zoom labels="aws,IaaS,tool,UML,TODO">
<span xsmall>AWS-PlantUML</span>
@[https://github.com/milo-minderbinder/AWS-PlantUML]
- PlantUML sprites, macros, stereotypes, and other goodies for
  creating PlantUML diagrams with AWS components.
</pre>
<pre zoom labels="aws,api_management,dev_framework,TODO">
<span xsmall>API Gateway</span>
@[https://aws.amazon.com/api-gateway/]
- Create, maintain, and secure APIs at any scale
- fully managed service that makes it easy for
  developers to create, publish, maintain, monitor, and secure APIs.
- APIs act as the "front door" for applications .
- Using API Gateway, you can create RESTful APIs and WebSocket APIs that enable
  real-time two-way communication applications.
- supports containerized and serverless workloads, as well as web applications.
- Hides developer from traffic management, CORS support, AAA, throttling,
  monitoring, and API version management.
- No minimum fees or startup costs!!!.
  pay for the API calls you receive and the amount of data transferred out
<hr/>
<span xsmall>Labmda Authorizer</span>
@[https://www.theguild.nl/enriching-requests-with-an-aws-lambda-authorizer/]
- As the name suggests a Lambda Authorizer placed on an Amazon API
  Gateway can provide authorization on requests, but did you know it
  can also enrich the request with additional session information?
</pre>
</div><!-- AWS App Development -->

<div groupv>
<span title>AWS: Non Classified</span>

<pre zoom labels="qa,TODO">
<span xsmall>Chaos Engineering</span>
@[https://www.infoq.com/news/2020/08/aws-chaos-engineering/]

Varun Jewalikar, Software Engineer at Prime Video, and Adrian
Hornsby, Principal Developer Advocate (Architecture) at AWS, write
that typical chaos engineering experiments include simulating
resource exhaustion and a failed or slow network. There are
countermeasures for such scenarios but "they are rarely adequately
tested, as unit or integration tests generally can't validate them
with high confidence".

AWS Systems Manager is a tool that can perform various operational
tasks across AWS resources with an agent component called SSM Agent.
The agent - pre-installed by default on certain Windows and Linux
AMIs - has the concept of "Documents" which are similar to runbooks
that can be executed. It can run simple shell scripts too, a feature
leveraged by the AWSSSMChaosRunner. The SendCommand API in SSM
enables running commands across multiple instances, which can be
filtered by AWS tags. CloudWatch can be used to view logs from all
the instances in a single place.
</pre>

</div>

<hr/> <!-- aws end -->
<span xbig>Scaleway (v0.3)</span><br/>
<div group>
<span title>Scaleway</span><br/>
<pre zoom labels="scaleway,101,storage,TODO">
<span xsmall>Summary</span>
@[https://slack.scaleway.com/]
- Scaleway console :
- API documentation:
</pre>

<pre zoom labels="scaleway,storage,cold_storage,TODO">
<span xsmall>C14 Cold Storage</span>
Block Storage:
- powered by SSDs offering 5,000 IOPS Âºmonth price:Âºâ‚¬0.08/GB
- public beta (2019).
- 99.99% SLA
- full replication of data.
Scaleway 2020-02-28:
@[https://www.scaleway.com/en/c14-cold-storage/]
  74 GB Free, then Eu0.01/GB
the worldâ€™s first alternative S3 Glacier.
fully integrated into our Object Storage!
 C14 Cold Storage is a S3-compatible cold storage. It lets you archive your
data in a fallout shelter located 25 meters underground in Paris. Both
archiving and restoring are totally free. Using C14 Cold Storage, you will
simply pay for the data stored at only â‚¬0.002/GB/month, a five-time lower
price than on Object Storageâ€™s standard class
</pre>

<pre zoom labels="scaleway,postgresql,data_architecture,storage,TODO">
<span xsmall>PostGIS extension on Mng DBs</span>
@[https://www.scaleway.com/en/docs/using-the-postgis-extension-on-managed-databases/]
</pre>

<pre zoom labels="scaleway,terraform,devops,IaC,TODO">
<span xsmall>Deploy Infra with Terraform</span>
Tutorial showing how to:
- Install Terraform
- Connect Terraform to Scaleway cloud by creating an API Token
- Create a first Instance using Terraform
- Modify an Instance using Terraform
- Add resources to an infrastructure managed by Terraform
- Delete infrastructures using Terraform
</pre>

<pre zoom labels="scaleway,moodle,TODO">
<span xsmall>Setup Moodle Learning Platform</span>
@[https://www.scaleway.com/en/docs/setting-up-moodle-with-ubuntu-focal-fossa/]

Moodle is an open source Learning Management System (LMS) that
provides educators and students with a platform and various tools to
create and participate in collaborative online learning environments.
</pre>

<pre zoom labels="scaleway,storage,encryption,troubleshoot,backup,TODO">
<span xsmall>encrypt Object Storage (rclone)</span>
https://www.scaleway.com/en/docs/encrypt-object-storage-using-rclone-crypt/?
</pre>

</div>
<hr/> <!-- scaleway end -->
<span xbig>Other Clouds</span><br/>
<div groupv>
<span title>GCP</span>
<pre zoom labels="gcp,todo">
<span xsmall TODO>TODO</span>
Elastifile
- provider of scalable, enterprise file storage in the cloud
@[https://www.infoq.com/news/2019/07/google-elastifile-acquisition/]
</pre>
</div>

<div groupv>
<span title>OVH</span>
<pre zoom labels="ovh,todo">
<span xsmall TODO>TODO</span>
@[https://www.muycomputerpro.com/2019/03/07/managed-kubernetes-ovh-contenedores]
@[https://www.muycomputerpro.com/2019/03/07/managed-kubernetes-ovh-contenedores]
@[http://www.teinteresa.es/noticias/OVH-vigesimo-aniversario-crecimiento-Espana_0_2176582456.html]
@[https://www.muycomputerpro.com/2019/02/08/ovh-celebra-su-20-aniversario-y-prepara-el-lanzamiento-de-su-plataforma-kubernetes]
@[https://norfipc.com/web/los-mejores-servicios-hosting-alojamiento-web-espanol.php]
@[https://www.muycomputerpro.com/2019/01/17/ingeniera-devops]
</pre>

<pre zoom labels="serverless,security,TODO">
<span xsmall>Serverless OWASP TOP 10</span>
@[https://dzone.com/articles/serverless-and-the-owasp-top-10]
</pre>

</div>
<hr/>
<div group>
<span xbig>Non-Classified</span><br/>
<pre zoom labels="azure,storage,TODO">
<span xsmall>azure-shared-disks</span>
Microsoft Ships Preview of Cluster-Friendly Cloud Disks
https://www.infoq.com/news/2020/03/azure-shared-disks/
</pre>

<pre zoom labels="azure,devops,container,aws,gcp,TODO">
<span xsmall>Azure Pipelines</span>
@[https://azure.microsoft.com/es-es/services/devops/pipelines/]
- Automate builds and deployments.

- Build, test, and deploy Node.js, Python, Java, PHP, Ruby, C/C++,
  .NET, Android, and iOS apps. Run in parallel on Linux, macOS, and
  Windows.

- Deploy to Azure, AWS and GCP..

- Deploy to Azure, AWS and GCP.
</pre>

<pre zoom labels="azure,governance,TODO">
<span xsmall>Azure Government Governance</span>
@[https://www.infoq.com/news/2019/01/Microsoft-Azure-Government-Cloud]

Microsoft recently expanded its Microsoft Learn platform with an
introductory class on Azure Government. Azure Government is
Microsoft's solution for hosting United States government solutions
on its cloud.

Government services have unique requirements in terms of security and
compliance, but public cloud solutions can provide scale, elasticity
and resilience that is not easy to be achieved with on premises
solutions. Available in eight U.S. regions, namely Chicago, Dallas,
New York, Phoenix, San Antonio, Seattle, Silicon Valley, and
Washington DC, it is specifically built for the U.S. government
needs. As such, it follows numerous compliance standards, from both
the U.S. and abroad (E.U., India, Australia, Chine, Singapore and
others). Some of the compliance standards are Level 5 Department of
Defence approval, FedRAMP High and DISA L4 (42 services) and L5 (45
services in scope). Microsoft Azure Government is operated using
completely separate physical infrastructure within Azure.
</pre>

<pre zoom labels="azure,ad,identity,nodejs,TODO">
<span xsmall>AD Node.js Lib</span>
@[https://github.com/AzureAD/azure-activedirectory-library-for-nodejs]

Windows Azure Active Directory Authentication Library (ADAL) for Node.js

The ADAL for node.js library makes it easy for node.js applications
to authenticate to AAD in order to access AAD protected web
resources. It supports 3 authentication modes shown in the quickstart
code below.
</pre>

<pre zoom labels="azure,ad,identity,governance,TODO">
<span xsmall>Azure Samples: AD</span>
@[https://github.com/azure-samples?q=active-directory]
</pre>

<pre zoom labels="OpenStack,storage,fs,TODO">
<span xsmall>SVFS</span>
@[https://github.com/ovh/svfs]
SVFS is a Virtual File System over Openstack Swift built upon fuse.
It is compatible with hubiC, OVH Public Cloud Storage and basically
every endpoint using a standard Openstack Swift setup. It brings a
layer of abstraction over object storage, making it as accessible and
convenient as a filesystem, without being intrusive on the way your
data is stored.
</pre>

<pre zoom labels="101,aws,networking,TODO">
<span xsmall>AWS availability zone</span>
- In AWS, â€œlocal networkâ€ basically means â€œavailability zoneâ€. If two instances
  are in the same AWS availability zone, they can just put the MAC
  address of the target computer on it, and then the packet will get to
  the right place. It doesnâ€™t matter what IP address is on the packet!
</pre>

<pre zoom labels="aws,lambda,devops,TODO">
<span xsmall>Openfit: ChatOps</span>
Openfit: ChatOps with Slack and AWS Lambda
@[https://youtu.be/Kebb0LOVC28]
</pre>

<pre zoom labels="price,comparative,TODO">
<span xsmall>Pricing: S3 vs Az. vs B2</span>
@[https://www.flexmonster.com/demos/pivot-table-js/]
</pre>

<pre zoom labels="azure,governance,ad,identity,TODO">
<span xsmall>ADFS as Identity Provider for A.AD B2C</span>
@[https://blogs.msdn.microsoft.com/azuredev/2017/06/23/using-adfs-as-an-identity-provider-for-azure-ad-b2c/]
</pre>

<pre zoom labels="gcp,service_discovery,TODO">
<span xsmall>Google Service Directory</span>
@[https://www.infoq.com/news/2020/04/google-service-directory-beta/]
Google Introduces Service Directory to Manage All Your Services in One Place at Scale
In a recent blog post, Google introduced a new managed service on its
Cloud Platform (GCP) called Service Directory. With this service,
Google allows customers to publish, discover, and connect services
consistently and reliably, regardless of the environment and platform
where they reside.

Service Directory, currently available as beta, is a designed by
Google for looking up services. For its users, the service provides
real-time information about all their services in a single place,
allowing them to perform service inventory management at scale,
regardless of the number of endpoints.

Google Cloud software engineer Matt DeLoria and product manager
Karthik Balakrishnan, Service Directory, stated in the announcement
blog post:

    Service Directory reduces the complexity of management and
operations by providing unified visibility for all your services
across cloud and on-premises environments. And because Service
Directory is fully managed, you get enhanced service inventory
management at scale with no operational overhead, increasing the
productivity of your DevOps teams.

Source:
https://cloud.google.com/blog/products/networking/introducing-service-directory

With Service Directory users can define services with metadata
allowing to group service together, while quickly making the
endpoints understood by their consumers and applications.
Furthermore, users can use the service to register different types of
services and resolve them securely over HTTP and gRPC. And finally,
for DNS clients they can leverage Service Directory's private DNS
zones, a feature that automatically updates DNS records as services
change.
</pre>

<pre zoom labels="datadog,monitoring,aws,azure,gcp,TODO">
<span xsmall>Datadog</span>
https://en.wikipedia.org/wiki/Datadog

- founded in 2010[6] by Olivier Pomel and Alexis LÃª-QuÃ´c.
- Datadog uses a Go based agent, rewritten from scratch since its major
  to ofer a cloud infrastructure monitoring service,
  with a dashboard, alerting, and visualizations of metrics. As cloud
  adoption increased, Datadog grew rapidly and expanded its product
  offering to cover service providers including Amazon Web Services
  (AWS), Microsoft Azure, Google Cloud Platform, Red Hat OpenShift, and
  OpenStack.[7]
- In 2015 Datadog announced the acquisition of Mortar Data,[8] bringing
  on its team and adding its data and analytics capabilities to
  Datadog's platform. That year Datadog also opened a research and
  development office in Paris,.[9] version 6.0.0 released on
  February 28, 2018.[2]
</pre>

<pre zoom labels="aws,iac,devops,TODO">
<span xsmall>Terraform Cloud Dev Kit</span>
@[https://www.infoq.com/news/2020/07/cdk-terraform/]
AWS, HashiCorp, and Terrastack collaborated to release a preview of
the Cloud Development Kit (CDK) for Terraform, or cdktf. Developers
can use programming languages like Python or Typescript to manage
infrastructure as code. cdktf generates a Terraform configuration in
JSON that can deploy resources with a "terraform apply" command.
Also, cdktf supports any existing modules and providers from the
Terraform registry to deploy resources to AWS, Azure, or Google Cloud.
</pre>

<pre zoom labels="comparative,serverless,TODO">
<span xsmall>TOP serverless Vendors</span>
@[https://www.datamation.com/cloud-computing/top-serverless-vendors.html]
</pre>

<pre zoom labels="aws,azure,101,microservices,serverless,architecture,resource,diagram,TODO">
<span xsmall>What typical 100% Serverless Architecture looks like in AWS!</span>
@[https://medium.com/serverless-transformation/what-a-typical-100-serverless-architecture-looks-like-in-aws-40f252cd0ecb]
</pre>

<pre zoom labels="aws,ec2,vm,arm,compute,TODO">
<span xsmall>Graviton EC2</span>
@[https://www.infoq.com/news/2020/09/aws-ec2-t4g-instances/]

AWS provides various Amazon Elastic Compute Cloud (EC2) instances,
including a broad choice of Graviton2 processor-based, which allow
customers to optimize their workloads on performance and costs. The
latest addition to the Graviton2-based instances is the low cost
burstable general-purpose T4g instances.

In the past, AWS released EC2 T3g instances for customers to run
general-purpose workloads in a very cost-effective manner. Yet,
customers asked for instances that could run at increased peak
performance at a low cost. Hence, AWS announced the release of T4g
instances, a new generation of low-cost burstable instance type
powered by AWS Graviton2 - a processor custom-built by AWS using
64-bit Arm Neoverse cores.
</pre>

<pre zoom labels="aws,pricing,free_tier,TODO">
<span xsmall>Is the AWS Free Tier Really Free?</span>
@[https://www.infoq.com/news/2020/09/aws-free-tier/]

""... The AWS Free Tier is free in the same way that a table saw is
childproof. If you blindly rush in to use an AWS service with the
expectation that you wonâ€™t be charged, youâ€™re likely to lose a
hand in the process...""

- three different offers depending on the product used:
- "always free":
  - 1 million requests per month on AWS Lambda
  - 25GB of storage on DynamoDB.
- "12 months free":
  - Amazon EC2
  - RDS
-  short-term "trials":
  - Amazon Inspector
  - GuardDuty.

- long term risk associated with this complexity:

"""..It seems pretty sensible to spin up your free EC2 instance in a
  private subnetâ€”and then RÂºyou're very reasonably surprised when you Âº
 RÂº get charged almost $80 a month for the Managed NAT Gateway attached Âº
 RÂº to that subnet. This has an unfortunate side effect of teaching Âº
 RÂº beginners to use AWS services in ways that won't serve them well in Âº
 RÂº corporate environments.Âº

 BÂºOracle, Azure, and GCP have all mastered this problem in a far moreÂº
 BÂºcomprehensive, less user-hostile way.Âº

   Azure free account includes 12 months of popular services and $200
   credit, the Google Cloud free program offers 20+ products and $300
   credit. BÂºAn important difference with AWS is the ability to not be Âº
 BÂºcharged until the user manually switches to a paid account.Âº

   Corey Quinn closes with an advice for users who receive unexpected bills:

    Open a ticket with AWS Support. ... BÂº If itâ€™s your first time with anÂº
  BÂºoverage, they will almost universally waive the fee.Âº
</pre>

<pre zoom labels="low_code,aws,TODO">
<span xsmall>honeycode.aws</span>
@[https://www.honeycode.aws/]
- Build form-like apps in "minutes".
</pre>

<pre zoom labels="azure,ias,terraform,TODO">
<span xsmall>AzureRM Terraform Provider 2.0</span>
@[https://www.infoq.com/news/2020/03/azurerm-terraform-2-0/]

 This release includes an overhaul of how two core resources are
described, an introduction of custom timeouts
</pre>

<pre zoom labels="aws,serverless,devops,TODO">
<span xsmall>Openfit</span>
Openfit: ChatOps with Slack and AWS Lambda
https://youtu.be/Kebb0LOVC28
</pre>

<pre zoom labels="terraform,IaC,low_code,aws,azure,scaleway,openstack,gcp,tool,qa,TODO" bgorange>
<span xsmall>BrainBoard.co</span>
@[https://www.brainboard.co/]

- The WYSIWYG of the Multi-Cloud.
- Design, Deploy and Depict all resources across all cloud providers around the world.

- Terraform file generated & versionned
  """ You can define variables to use in your products configuration,
     visualise the resulting code and download it.  Or simply commit your
     architecture using the integrated Git."""

- Worldwide overview of all your resources
  Visualize all cloud resources around the world for all cloud providers in ONE place.
</pre>

<pre zoom labels="gcp,aws,s3,storage,tool,TODO">
<span xsmall>s5cmd</span>
@[https://github.com/peak/s5cmd]
- very fast S3 and local filesystem execution tool.
s5cmd supports wide range of object management tasks both for cloud
storage services and local filesystems.
- List buckets and objects
- Upload, download or delete objects
- Move, copy or rename objects
- Set Server Side Encryption using AWS Key Management Service (KMS)
- Set Access Control List (ACL) for objects/files on the upload, copy, move.
- Print object contents to stdout
- Create buckets
- Summarize objects sizes, grouping by storage class
- Wildcard support for all operations
- Multiple arguments support for delete operation
- Command file support to run commands in batches at very high execution speeds
- Dry run support
- S3 Transfer Acceleration support
- Google Cloud Storage (and any other S3 API compatible service) support
- Structured logging for querying command outputs
- Shell auto-completion

</pre>

<pre zoom labels="aws,tool,cli,TODO">
<span xsmall>AWS Cloud Shell</span>
@[https://aws.amazon.com/cloudshell/]
- Targeting administrators and developers .
- Includes aws  CLI v2 and Amazon Linux 2 OS
   installed and configured (Bash, zsh, PowerShell, editors,
   Git source control, and package management â€“ npm/JS,
   pip/Python,...)
- When accessing the AWS CloudShell, user is
- file uploads up to 1GB (to $HOME)Âºpersisted between sessionsÂº.
  (1 GB of persistent storage per AWS region)
-  root access provided.
- outbound connections allowed, inbound connections rejected.

"""...  The point of CloudShell is to easily use AWS CLI without setting
  it up and setting the credentials; however, to use this from your own
  terminal, it means you have to install software and then configure
  credentials- well then that would be exactly the same as installing
  AWS CLI and configuring it...
BÂº    The main value is if you're on a machine that isn't your normal  Âº
BÂºwork machine and you want quick access to the CLI without installing Âº
BÂºthe CLI itself and adding your credentials.                      Âº

- Note: Microsoft and GCP offer Cloud Shell since 2017 (5GB persistence storage)

OÂºUp to 10 concurrent shells in each region at no chargeÂº:
 - unofficial AWS CloudShell plugin for VS Code:
   https://github.com/iann0036/vscode-aws-cloudshell
</pre>
</div> <!-- Non classified ->

</body>
</html>
<!--
AWS, Queue Services:
    https://www.infoq.com/news/2020/11/aws-amazon-mq-rabbitmq/

- AWS announced Amazon MQ will now support RabbitMQ, a popular
  open-source message broker. With the support for RabbitMQ, customers
  can migrate their existing RabbitMQ message brokers to AWS without
  rewriting code.

- In 2017, AWS released Amazon MQ as a managed message broker
  service for Apache ActiveMQ, an open-sourced, enterprise-grade
  message broker compatible with most industry-standard protocols like
  JMS, NMS, AMQP 1.0 and 0-9-1, STOMP, MQTT, and WebSocket. Since then,
  the public cloud vendor enhanced the service with new features to
  improve high scalability and availability, such as vertical scaling,
  throughput-optimized message brokers with a 99.9% Service Level
  Agreement (SLA), and LDAP Authentication and Authorization support.
  The company now expands Amazon MQ with an additional message broker
  providing customers an option to reduce their operational overhead
  managing RabbitMQ message brokers by moving their existing brokers to
  AWS.

<div groupv>
<span title>"GAIA-X"</span>
<pre zoom labels="gaiax,TODO">
<span xsmall>Introduction</span>
@[https://www.bmwi.de/Redaktion/EN/Publikationen/Digitale-Welt/das-projekt-gaia-x-executive-summary.pdf?__blob=publicationFile&v=6]
</pre>
</div>


___________________
GCP: Google Announces Eventarc in Preview
https://www.infoq.com/news/2020/11/eventarc-google-cloud-run
Adhere to the CloudEvents standard for all events, regardless of source, to ensure a consistent developer experience
######################
comparative: spec/primer.md at v1.0 Â· cloudevents/spec
https://github.com/cloudevents/spec/blob/v1.0/primer.mdÂ 
Al final hay una comparativa de definiciÃ³n de eventos aws, azure, gcp, openstack,...
#######################aws: donnemartin/saws: A supercharged AWS command line interface (CLI).
https://github.com/donnemartin/saws

############################Google: Anthos: plataforma de modernizaciÃ³n de aplicaciones Â | Â Google Cloud
https://cloud.google.com/anthosÂ 

https://www.infoq.com/news/2020/12/google-anthos-baremetal-ga/

According to the blog post, the minimal requirement for customers to run Anthos on bare metal at the edge is two nodes with a minimum of fourÂ cores, a2 GB RAM, and 128GB of disk space with no specialized hardware. Furthermore, customers can choose their operating system too, with support for Red Hat Enterprise Linux 8.1/8.2, CentOS 8.1/8.2, and Ubuntu 18.04/20.04 LTS.

##############################OpenStack Docs: Masakari API
https://docs.openstack.org/api-ref/instance-ha/Â 
#####################ghostinthewires/Azure-Readiness-Checklist: This checklist is your guide to the best practices for deploying secure, scalable, and highly available infrastructure in Azure. Before you go live, go through each item, and make sure you haven't missed anything important!
https://github.com/ghostinthewires/Azure-Readiness-Checklist

##########################Cloud Native Interactive Landscape
https://landscape.cncf.io/Â 
#######################Open Networking Foundation is an operator led consortium leveraging SDN, NFV and Cloud technologies to transform operator networks and business modelshttps://www.opennetworking.org/

Public Preview of Azure Arc Enabled Kubernetes at Build 2020
https://www.infoq.com/news/2020/06/azure-arc-kubernetes-preview/
During this year's digital Build event, Microsoft announced the
public preview of Azure Arc enabled Kubernetes with support for most
of the Cloud Native Computing Foundation (CNCF)-certified Kubernetes
distributions. With this capability, customers can manage and govern
their Kubernetes clusters from Azure across their data centers,
multi-cloud configurations, and Azure Stack Hub.Microsoft released
Azure Arc as a preview last year in November during the Ignite
Conference. The new service makes it easier for customers to deploy
and manage Azure services across multiple clouds and on-premises IT
environments. The public cloud vendor is now expanding that
capability with Azure Arc-enabled Kubernetes, allowing customers to
attach and configure Kubernetes clusters inside or outside of
Azure.Â Furthermore, Microsoft announced the first set of Azure Arc
integration partners, including Red Hat OpenShift, Canonical
Kubernetes, and Rancher Labs to ensure Azure Arc works for all the
key platforms their customers are currently using. In the
announcement blog post, Mike Evans, vice president, technical
business development, Red Hat OpenShift, said
################################3
Cloud, Architecture: AWS Lambda is Not a Magic Reliability Wand | by Karl Pickett | Jul, 2020 | Medium
https://medium.com/@karl.pickett/aws-lambda-is-not-a-magic-reliability-wand-91da728acbaÂ 
#######################
Cloud: Microsoft Introduces the Azure Well-Architected Framework
https://www.infoq.com/news/2020/08/azure-well-architected-framework/
#####################

Cloud: LWS VPS prices (5â‚¬ /mois 4GB RAM)
Serveur VPS - location de serveur virtuel - serveur linux - LWS
https://www.lws.fr/serveur-virtuel.phpÂ 
#################
https://github.com/BBVA/idsfree
Launch hacking tests in cloud providers securely, isolated and without raise security alerts in the provider
###############
aws,storage,s3,price,low_code
https://github.com/aws-samples/s3-select-phonebook-search
- simple serverless Java application illustrating
  the usage of AWS S3 "Select" to execute a SQL query on a comma
  separated value (CSV) file stored on Amazon Simple Storage Service
  (Amazon S3).
- S3 Select does not require any database servers and runs
  directly on S3.

- S3 Select (2018+) and Amazon S3 Glacier Select allow customers to
  run SQL queries directly on data stored in S3 and Amazon S3 Glacier.
  - Customers previously needed to deploy a database to query this data.
  With Amazon S3 Select, you simply store your data on S3 and query away
  using simple (SQL) statements to filter the contents of Amazon S3 objects
  and retrieve only the subset of data that you need.
- By retrieving only a subset of the data, customers reduce the amount of
  data that Amazon S3 transfers, which reduces the cost and latency to
  retrieve this data.

- Compatible with CSV, JSON, or Apache Parquet format.

- Demo app also leverages Amazon API Gateway and AWS Lambda.
##############
https://github.com/tldr-pages/tldr/blob/master/pages/common/gcloud.md

AWS:
https://github.com/tldr-pages/tldr/tree/master/pages/common

AZ:
https://github.com/tldr-pages/tldr/blob/master/pages/common/az.md
####################
GitOps + k8s on Google Antos
https://seroter.com/2021/01/12/how-gitops-and-the-krm-make-multi-cloud-less-scary/
######################
 Azure AD roles
 -------------------------
 - Global Administrator
 - Billing Administrator
 - Service Administrator
 - User Administrator
 - Password Administrator
###################
https://www.xataka.com/pro/microsoft-mima-a-azure-novedades-para-servicio-cloud-anunciadas-ignite-2021
##################
https://github.com/localstack/localstack
- easy-to-use test/mocking framework for developing Cloud applications.
- focus is primarily on supporting the AWS cloud stack.
##################
AWS EC2 nomenclature:
- M5d.xlarge
  â”‚â”‚â”‚ â””â”€â”¬â”€â”€â”˜
  â”‚â”‚â”‚   â””â”€ Instance size
  â”‚â”‚â”” additional capabilites
  â”‚â”” Intance generation
  â”” Intances family ("M": "Memory" optimized)
##################
[price]
AWS EC2:
Expected load and instance type reservation

r: Reserved   : manage normal loads
o: on-demand  : manage load peaks
b: batch tasks: short time recoverable task
       o           b           o
      ooo          b          ooo      b
     oooooo        b         oooooo    b
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr
rrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr

######################
$ aws ec2 run-instances  : run == create and start new
$ aws ec2 start-instances
$ aws ec2 reboot-instances
$ aws ec2 stop-instances
$ aws ec2 terminate-instances

$ aws ec2 attach-volume
$ aws ec2 create-volume
$ aws ec2 create-image
$ aws ec2 copy-images
$ aws ec2 describe-images
$ aws ec2 describe-instances
######################
in aws:
high_availability: Service is respondent maybe in degraded mode
Fault-tolerant == high_availability + compliant SLA
########################
https://aws.amazon.com/elasticbeanstalk/
AWS Elastic Beanstalk:
- easy-to-use service for deploying and scaling web applications
  and services developed with Java, .NET, PHP, Node.js, Python,
  Ruby, Go, and Docker on familiar servers such as Apache, Nginx,
  Passenger, and IIS.
########################
aws, aaa, auditing:
https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html
##################
AWS IAM  and STS (Security Token Service) quotas:
https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_iam-quotas.html
################
AWS JDK JAVA:
https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/setup-project-maven.html#configuring-maven-individual-components
###################
AWS: 99designs AWS Vault:
https://github.com/99designs/aws-vault
#####################
See aws.s3, iam, kinesis, labmda,... @
https://github.com/tldr-pages/tldr/tree/master/pages/common
######################
graph_db,gcp,aaa,iam,
https://pavankumarkattamuri.medium.com/explore-neo4j-by-visualizing-google-cloud-iam-hierarchy-2da562eaa855
Exploring Neo4j by visualizing Google Cloud IAM hierarchy
#################
AWS : Diagrama resumen Red.!!!
https://github.com/undergroundwires 
https://github.com/undergroundwires/AWS-in-bullet-points
https://github.com/undergroundwires/AWS-in-bullet-points/blob/master/saa/img/iam/iam-entities.png AWS IAM entities !!!
####################
https://github.com/undergroundwires/Azure-in-bullet-points
#################
https://github.com/undergroundwires/Azure-in-bullet-points
