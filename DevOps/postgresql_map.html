<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>PostgreSQL(beta)</title>
<head>
<script src="/IT_notes/map_v1.js"></script>
<link rel="stylesheet" type="text/css" href="/IT_notes/map_v1.css" />
</head>

<body>

<div groupv >
<span title>Intro</span>
<pre zoom>
<span xsmall>Ext. Links</span> 
<a href="https://www.postgresql.org/docs/manuals/">Manuals</a>
<a href="http://www.interdb.jp/pg/index.html">PostgreSQL internals</a> (Hironobu SUZUKI)
</pre>
<pre zoom labels="">
<span xsmall>ENV.VARS</span>
PGPORT
PGUSER  (alt. -U option)
</pre>
<pre zoom>
<span xsmall>Show ddbbs, tables,</span>
<span xsmall>schemas, permissions,..</span>
mydb=˃ SELECT version();
mydb=˂˃ SLECT current_date;
psql commands:
\?
\h
\l            list databases
\c db_name    connect to ddbb
\q            quit
\dt           show all tables in ddbb
\dt º.º.      show all tables globally
\d  table     show table schema
\d+ table                      
\du           list current user's permissions
\u
SELECT current_database();

SELECT rolname       list users
   FROM pg_roles;
</pre>

<pre zoom bgorange>
<span xsmall>Instance "layout"</span>
 PostgresSQL instance:  Server 1←→1 DDBB Cluster  1←→N Catalog(Database) 1←→N Schemas
  ├─ Databases
  │  │ 
  │  ├─ postgres                │ SCHEMA COMMANDS
  │  │   │                      │ ºlist schemasº
  │  │   ├─ Casts               │ \dn
  │  │   │                      │ SELECT schema_name FROM information_schema.schemata;
  │  │   ├─ Catalogs *1         │ SELECT nspname     FROM pg_catalog.pg_namespace;
  │  │   │                      │ 
  │  │   ├─ Event Triggers      │ ºcreate schemaº
  │  │   │                      │ @[http://www.postgresql.org/docs/current/static/sql-createschema.html]
  │  │   ├─ Extensions          │ CREATE SCHEMA IF NOT EXISTS <schema_name>;
  │  │   │                      │ 
  │  │   ├─ Foreing Data Wrap   │ ºdrop schemaº
  │  │   │                      │ @[http://www.postgresql.org/docs/current/static/sql-dropschema.html]
  │  │   ├─ Languages           │ DROP SCHEMA IF EXISTS <schema_name> CASCADE;
  │  │   │
  │  │   └─ Schemas
  │  │
  │  ├─ myDDBB01
  │
  ├─ Login/Group @[https://www.postgresql.org/docs/10/static/user-manag.html]
  │
  └─ Tablespaces

Difference between catalog and schema
@[https://stackoverflow.com/questions/7022755/whats-the-difference-between-a-catalog-and-a-schema-in-a-relational-database]
*1 So in both Postgres and the SQL Standard we have this containment hierarchy:
@[https://stackoverflow.com/questions/7022755/whats-the-difference-between-a-catalog-and-a-schema-in-a-relational-database]
 - A computer may have one cluster or multiple.
 - A database server is a cluster.
 - A cluster has catalogs. ( Catalog = Database )
 - Catalogs have schemas. (Schema = namespace of tables, and security boundary)
 - Schemas have tables.
 - Tables have rows.
 - Rows have values, defined by columns.
 - Those values are the business data your apps and users care about such as 
   person's name, invoice due date, product price, gamer’s high score. 
   The column defines the data type of the values (text, date, number, and so on).
</pre>
</div>

<div groupv>
<pre zoom>
<span xsmall>Create DB Cluster (initdb)</span>
<span xsmall>Init storage area </span>
<span xsmall>(data directory in FS terms) </span>
Server 1 → N Databases 
OS admin → shell:º$ sudo su postgres º
                 º$ initdb -D /usr/local/pgsql/dataº
                  alternatively
                  º# pg_ctl -D /usr/local/pgsql/data initdbº
                  ('postgres' and 'template1' ddbbs will be created inside)

                  This is the database/catalog cluster:
                  Collection of DDBB managed by a single instance

  <span xsmall>Starting the Server</span>
OS admin → shell: º# su postgres -c 'postgres -D /usr/local/pgsql/data 1˃/var/log/postgresql/logfile 2˃⅋1 ⅋'º
                   alternatively (using pg_ctl "easy" wrapper)
                   º# su postgres -c 'postgres@~$ pg_ctrl start -l /var/log/postgresql/logfile'º
                   alternatively (PostgreSQL systemd enabled)
                   º$ sudo systemctl enable postgresql.service'º
                   º$ sudo systemctl start postgresql.service'º
                   º$ sudo journalctl --unit postgresql.service'º

<span xsmall>CRUD Users</span>
$ sudo su postgres # change to postgres user
$ psql
#- CREATE USER IF NOT EXISTS my_user_login WITH PASSWORD 'my_user_password';
#- ALTER USER my_user_login WITH PASSWORD 'my_new_password';
#- DROP USER IF EXISTS my_user_login;
# TODO: switch to given ddbb

<span xsmall>Granting privileges to users</span>
GRANT ALL PRIVILEGES ON table TO my_user_login;

-- grant all permissions to ddbb
GRANT ALL PRIVILEGES ON DATABASE my_db_name TO my_user_name;

-- grant connection permissions on database
GRANT CONNECT ON DATABASE my_db_name TO my_user_name;

-- grant permissions on schema
GRANT USAGE ON SCHEMA public TO my_user_name;

-- grant permissions to functions
GRANT EXECUTE ON ALL FUNCTIONS IN SCHEMA public TO my_user_name;

-- grant permissions to select, update, insert, delete, on a all tables
GRANT SELECT, UPDATE, INSERT ON ALL TABLES IN SCHEMA public TO my_user_name

-- grant permissions, on a table
GRANT SELECT, UPDATE, INSERT ON table_name TO my_user_name;

-- grant permissions, to select, on a table
GRANT SELECT ON ALL TABLES IN SCHEMA public TO my_user_name;

<span xsmall>Create new DDBB</span>
"admin" → PostgreSQL: create user account
("admin" ussually match postgres OS user)
...
user → shell: $ createdb mydb

Many tools assume ddbb names == (-U,PGUSER) username by default
Drop existing DDBB:
user → shell: $ dropdb mydb # warn can NOT be undon

Console access:
user → shell: $ psql mydb

<span xsmall>CREATE TABLE IF NOT EXISTS example</span>
-- DO $$
-- BEGIN
--   EXECUTE 'ALTER DATABASE ' || current_database() || ' SET TIMEZONE TO UTC';
-- END; $$;

CREATE TABLE IF NOT EXISTS devices (
  ID         VARCHAR(40)                NOT NULL CONSTRAINT DEVICE_PK PRIMARY KEY,
  NAME       VARCHAR(255)               NOT NULL,
  CREATED_AT TIMESTAMP    DEFAULT NOW() NOT NULL,
  UPDATED_AT TIMESTAMP    DEFAULT NOW() NOT NULL,
  OWNER_ID   VARCHAR(40)                NOT NULL,
- - PUB_KEY NOT YET USED. Can be used to cipher message and decipher using device private key
  PUB_KEY    VARCHAR(88)                NOT NULL UNIQUE
  PARENT_ID  VARCHAR(40)                NULL CONSTRAINT DEVICE_FK3 REFERENCES devices (ID),
);
CREATE IF NOT EXISTS INDEX DEVICE_PUB_KEY_IDX ON devices   (PUB_KEY);
CREATE IF NOT EXISTS INDEX OWNER_IDX          ON devices   (OWNER_ID);

-- (Alt) ALTER TABLE tableName ADD PRIMARY KEY (id);
-- (Alt) CREATE UNIQUE INDEX indexName ON tableName (columnNames);
</pre> 

<pre zoom>
<span xsmall TODO>Client Authentication</span>
@[https://www.postgresql.org/docs/10/static/client-authentication.html]
</pre>

<pre zoom>
<span xsmall>pg_hba.conf</span>
<span xsmall>(AAA+Encryption)</span>
@[https://www.postgresql.org/docs/devel/static/auth-pg-hba-conf.html]
- the default client authentication setup allows any local user
   to connect to the database and even become the database superuser.
   If you do not trust them:
   1 - use one initdb -W, --pwprompt or --pwfile options 
       to assign a password to the database superuser.
   2 - also, specify -A md5 or -A password so that the 
       default trust authentication mode is not used; 
       or modify the generated ºpg_hba.confº file after running initdb
       , but before you start the server for the first time.

- <a  href="https://www.postgresql.org/docs/10/static/ssl-tcp.html">TSL </a>
   allows both the client and server to provide SSL certificates to each other. 

- <a TODO href="https://www.postgresql.org/docs/10/static/encryption-options.html">Encryp. Options </a>
  - The pg_hba.conf file allows administrators to specify which hosts can use
    non-encrypted connections (host) and which require SSL-encrypted 
    connections (hostssl). Also, clients can specify that they connect to 
    servers only via SSL.
  
  - The pgcrypto module allows certain fields to be stored encrypted. This 
    is useful if only some of the data is sensitive. The client supplies the 
    decryption key and the data is decrypted on the server and then sent to the client.
</pre>
</div>
<div groupv>
<pre xxxsmall zoom>
<span xsmall>Backup/Restore</span>
ºBACKUPº                                         ºRESTOREº
# backup ddbb                                    $ pg_restore -d ddbb_name               -a backup.sql
$ pg_dump ${dbName} ˃ dbName.sql                 $ pg_restore -d ddbb_name --data-only   -a backup.sql
# backup ddbb , only data                        $ pg_restore -d ddbb_name --schema-only -a backup.sql
$ pg_dump --data-only ${dbName} ˃ dbName.sql
# backup ddbb , only schema
$ pg_dump --schema-only ${dbName} ˃ dbName.sql

# backup all ddbb
$ pg_dumpall ˃ pgbackup.sql
</pre>

<pre zoom>
  <span xsmall>EXPORT/Import (COPY) file</span>
\copy table_name            TO   '/home/user/weather.csv' CSV
\copy table_name(col1,col2) TO   '/home/user/weather.csv' CSV

\copy table_name            FROM '/home/user/weather.csv' CSV
\copy table_name(col1,col2) FROM   '/home/user/weather.csv' CSV
</pre>

<pre zoom>
<span xsmall>Table Maintenance</span>
-- VACUUM
VACUUM ANALYZE table;

-- Reindex a database, table or index
REINDEX DATABASE dbName;

-- <a href="https://www.postgresql.org/docs/current/static/using-explain.html">Show query plan</a>

EXPLAIN SELECT * FROM table;
</pre>

<pre zoom>
  <span xsmall TODO>Rotate logs</span>
</pre>
</td>  
</div>

<div groupv>
<pre zoom>
<a xsmall href="https://www.postgresql.org/docs/10/static/config-setting.html">postgresql.conf</a>
- Config Settings for logs, buffers, ...
  - Re-read changes with ºSIGHUPºsignal or $ºpg_ctl reloadº</li>
</pre>

<pre zoom>
<a xsmall TODO href="https://www.postgresql.org/docs/10/static/view-pg-file-settings.html">pg_file_settings</a>
@[https://www.postgresql.org/docs/10/static/config-setting.html]
- Server instance configuration
- can be used to debug or pre-test changes in conf.
</pre>

<pre zoom>
<a xsmall TODO href="http://dalibo.github.io/pgbadger/">pgbadger</a>
- @[https://www.dalibo.org/_media/pgconf.eu.2013.conferences-pgbadger_v.4.pdf]
- log analyzer with detailed reports from PSQL log files 
  (syslog, stderr or csvlog) with in-browser zoomable graph
- designed to parse huge log files as well as gzip compressed file
</pre>

<pre zoom>
<span xsmall>Upgrading the server</span>
- @[https://www.postgresql.org/docs/10/static/upgrading.html"]
  (pg_dumpall, pg_upgrade, replication)
</pre>
</div>

<div groupv>
<pre zoom>
<a xsmall href="https://www.postgresql.org/docs/10/static/kernel-resources.html">Tunning OS</a>
(Shared Memory/Semaphores/...):
- Show all runtime parameters: 
 º#- SHOW ALL;º
</pre>


<pre zoom>
<span xsmall>Performance Optimization</span>
- Enable autovacuum. The working memory for autovacuum should be no more than 2% of
    the total available memory.
- Enable database caching with an effective cache size between 6% and 8% of the total
    available memory.
- To increase performance, the working memory should be at least 10% of the total
    available
  Set SERVER_ENCODING , LC_COLLATE and LC_CTYPE as :
    server_encoding = UTF8
    lc_collate = en_US.UTF-8
    lc_ctype = en_US.UTF-8
</pre>

<pre zoom>
<a xsmall href="https://www.pg-versus-ms.com/">PSQL vs MSQL comparative</a>
<span xsmall>(for data analytics)</span>
  - A data analytics platform which cannot handle CSV robustly is a broken, useless liability
  - PostgreSQL's CSV support is top notch. The COPY TO and COPY FROM commands support the spec outlined in
     RFC4180 (which is the closest thing there is to an official CSV standard) as well as a multitude of common
    and not-so-common variants and dialects. These commands are fast and robust. When an error occurs, they give
    helpful error messages. Importantly, they will not silently corrupt, misunderstand or alter data. If PostgreSQL 
    says your import worked, then it worked properly. The slightest whiff of a problem and it abandons the import
    and throws a helpful error message.
  
  - 
PostgreSQL                     | MS SQL Server:
DROP TABLE IF EXISTS my_table; | 
                               | IF OBJECT_ID (N'dbo.my_table', N'U') IS NOT NULL
                               | DROP TABLE dbo.my_table;
  
  - PostgreSQL supports DROP SCHEMA CASCADE, which drops a schema and all the database objects inside it. This is very, very important for a robust analytics delivery methodology, where tear-down-and-rebuild is the underlying principle of repeatable, auditable, collaborative analytics work.
  - 
PostgreSQL                  |  MS SQL Server
CREATE TABLE good_films AS  |  SELECT
SELECT                      |    *
  *                         |  INTO
FROM                        |    good_films
  all_films                 |  FROM
WHERE                       |    all_films
  imdb_rating ˃= 8;         |  WHERE
  
  - In PostgreSQL, you can execute as many SQL statements as you like in one 
  batch; as long as you've ended each statement with a semicolon, you can 
  execute whatever combination of statements you like. For executing 
  automated batch processes or repeatable data builds or output tasks, this 
  is critically important functionality.
  - PostgreSQL supports the RETURNING clause, allowing UPDATE, INSERT and 
  DELETE statements to return values from affected rows. This is elegant and 
  useful. MS SQL Server has the OUTPUT clause, which requires a separate 
  table variable definition to function. This is clunky and inconvenient and 
  forces a programmer to create and maintain unnecessary boilerplate code.
  - PostgreSQL supports $$ string quoting, like so:
SELECT $$Hello, World$$ AS greeting;
    This is extremely useful for generating dynamic SQL because (a) it allows 
  the user to avoid tedious and unreliable manual quoting and escaping when 
  literal strings are nested and (b) since text editors and IDEs tend not to 
  recogniise $$ as a string delimiter, syntax highlighting remains functional 
  even in dynamic SQL code.
  - PostgreSQL lets you use procedural languages simply by submitting code to 
  the database engine; you write procedural code in Python or Perl or R or 
  JavaScript or any of the other supported languages (see below) right next 
  to your SQL, in the same script. This is convenient, quick, maintainable, 
  easy to review, easy to reuse and so on.
  - "Pure" declarative SQL is good at what it was designed for – relational 
  data manipulation and querying. You quickly reach its limits if you try to 
  use it for more involved analytical processes, such as complex interest 
  calculations, time series analysis and general algorithm design. SQL 
  database providers know this, so almost all SQL databases implement some 
  kind of procedural language. This allows a database user to write imperative
  - style code for more complex or fiddly tasks.
  - PostgreSQL's procedural language support is exceptional:
    - PL/PGSQL: this is PostgreSQL's native procedural language. It's like Oracle's PL/SQL, but more modern and feature-complete.
    - PL/V8: the V8 JavaScript engine from Google Chrome is available in PostgreSQL.
      Even better, PL/V8 supports global (i.e. cross-function call) state, 
      allowing the user to selectively cache data in RAM for fast random access.
      Suppose you need to use 100,000 rows of data from table A on each of 1,000,000 
      rows of data from table B. In traditional SQL, you either need to join 
      these tables (resulting in a 100bn row intermediate table, which will 
      kill any but the most immense server) or do something akin to a scalar 
      subquery (or, worse, cursor-based nested loops), resulting in crippling 
      I/O load if the query planner doesn't read your intentions properly. 
     º In PL/V8 you simply cache table A in memory and run a function on each        º
     ºof the rows of table B – in effect giving you RAM-quality access (             º
     ºnegligible latency and random access penalty; no non-volatile I/O load)        º
     ºto the 100k-row table. I did this on a real piece of work recently – my        º
     ºPostgreSQL/PLV8 code was about 80 times faster than the MS T-SQL solution      º
     ºand the code was much smaller and more maintainable. Because it took about     º
     º23 seconds instead of half an hour to run, I was able to run 20 run-test-modifyº
     ºcycles in an hour, resulting in feature-complete, properly tested, bug-free    º
     ºcode.                                                                          º
      (All those run-test-modify cycles were only possible because of DROP SCHEMA CASCADE
       and freedom to execute CREATE FUNCTION statements in the middle of a statement
       batch, as explained above. See how nicely it all fits together?)
    - PL/Python: Fancy running a SVM from scikit-learn or some arbitrary-precision arithmetic provided by gmpy2 in the middle of a SQL query? No problem!
    - PL/R
    - C: doesn't quite belong in this list because you have to compile it 
      separately, but it's worth a mention.  In PostgreSQL it is trivially easy 
      to create functions which execute compiled, optimised C (or C++ or assembler)
      in the database backend.
  - In PostgreSQL, custom aggregates are convenient and simple to use, resulting in fast problem-solving and maintainable code:
CREATE FUNCTION interest_sfunc(state JSON, movement FLOAT, rate FLOAT, dt DATE) RETURNS JSON AS
$$
state.balance += movement;  //payments into/withdrawals from account
if (0 === dt.getUTCDate()) //compound interest on 1st of every month
{
  state.balance += state.accrual;
  state.accrual = 0;
}
state.accrual += state.balance * rate;
return state;
$$ LANGUAGE plv8;

CREATE AGGREGATE interest(FLOAT, FLOAT, DATE)
(
  SFUNC=interest_sfunc,
  STYPE=JSON,
  INITCOND='{"balance": 0, "accrual": 0}'
);

--assume accounts table has customer ID, date, interest rate and account movement for each day
CREATE TABLE cust_balances AS
SELECT
  cust_id,
  (interest(movement, rate, dt ORDER BY dt)-˃˃'balance')::FLOAT AS balance
FROM
  accounts
GROUP BY
  cust_id;
Elegant, eh? A custom aggregate is specified in terms of an internal state 
  and a way to modify that state when we push new values into the aggregate 
  function. In this case we start each customer off with zero balance and no 
  interest accrued, and on each day we accrue interest appropriately and 
  account for payments and withdrawals. We compound the interest on the 1st 
  of every month. Notice that the aggregate accepts an ORDER BY clause (since
  , unlike SUM, MAX and MIN, this aggregate is order-dependent) and 
  PostgreSQL provides operators for extracting values from JSON objects. So, 
  in 28 lines of code we've created the framework for monthly compounding 
  interest on bank accounts and used it to calculate final balances. If 
  features are to be added to the methodology (e.g. interest rate 
  modifications depending on debit/credit balance, detection of exceptional 
  circumstances), it's all right there in the transition function and is 
  written in an appropriate language for implementing complex logic. (Tragic 
  side-note: I have seen large organisations spend tens of thousands of 
  pounds over weeks of work trying to achieve the same thing using poorer tools.)
- Date/Time
    - PostgreSQL: you get DATE, TIME, TIMESTAMP and TIMESTAMP WITH TIME ZONE, 
  all of which do exactly what you would expect. They also have fantastic 
  range and precision, supporting microsecond resolution from the 5th 
  millennium BC to almost 300 millennia in the future. They accept input in a 
  wide variety of formats and the last one has full support for time zones
    - They can be converted to and from Unix time, which is very important 
      for interoperability with other systems.
    - They also support the INTERVAL type, which is so useful it has its own 
      section right after this one.
        SELECT to_char('2001-02-03'::DATE, 'FMDay DD Mon YYYY');  --this produces the string "Saturday 03 Feb 2001"
        and, going in the other direction,
        SELECT to_timestamp('Saturday 03 Feb 2001', 'FMDay DD Mon YYYY');  --this produces the timestamp value 2001-02-03 00:00:00+00
    - PostgreSQL: the INTERVAL type represents a period of time, such as "30 
  microseconds" or "50 years". It can also be negative, which may seem 
  counterintuitive until you remember that the word "ago" exists. PostgreSQL 
  also knows about "ago", in fact, and will accept strings like '1 day ago' 
  as interval values (this will be internally represented as an interval of -
  1 days). Interval values let you do intuitive date arithmetic and store 
  time durations as first-class data values. They work exactly as you expect 
  and can be freely casted and converted to and from anything which makes sense
  - PostgreSQL arrays are supported as a first-class data type
    - eaning fields in tables, variables in PL/PGSQL, parameters to functions 
      and so on can be arrays. Arrays can contain any data type you like, 
      including other arrays. This is very, very useful. Here are some of the 
      things you can do with arrays:
            
    - Store the results of function calls with arbitrarily-many return values, such as regex matches
    - Represent a string as integer word IDs, for use in fast text matching algorithms
    - Aggregation of multiple data values across groups, for efficient cross-tabulation
    - Perform row operations using multiple data values without the expense of a join
    - Accurately and semantically represent array data from other applications in your tool stack
    - Feed array data to other applications in your tool stack
  - PostgreSQL: full support for JSON, including a large set of utility functions for
     transforming between JSON types and tables (in both directions)
  - PostgreSQL: HSTORE is a PostgreSQL extension which implements a fast key-value store as a data type.
     Like arrays, this is very useful because virtually every high-level programming language has such
     a concept (associative arrays, dicts, std::map ...)
     There are also some fun unexpected uses of such a data type. A colleague recently asked me if there was a good way to deduplicate a text array. Here's what I came up with:
        SELECT akeys(hstore(my_array, my_array)) FROM my_table;
     i.e. put the array into both the keys and values of an HSTORE, forcing a 
     dedupe to take place (since key values are unique) then retrieve the keys 
     from the HSTORE. There's that PostgreSQL versatility again.
  
  - PostgreSQL:<a href="https://www.postgresql.org/docs/9.3/static/rangetypes.html">range types</a>.
    Every database programmer has seen fields called start_date and end_date, 
    and most of them have had to implement logic to detect overlaps. Some have even found, the hard way,
    that joins to ranges using BETWEEN can go horribly wrong, for a number of reasons.
    PostgreSQL's approach is to treat time ranges as first-class data types. Not only can you put a
    range of time (or INTs or NUMERICs or whatever) into a single data value, you can use a host of
    built-in operators to manipulate and query ranges safely and quickly. You can even apply specially-developed
    indices to them to massively accelerate queries that use these operators. 
  - PostgreSQL: NUMERIC (and DECIMAL - they're symonyms) is near-as-dammit arbitrary 
    precision: it supports 131,072 digits before the decimal point and 16,383 digits after the decimal point.
  - PostgreSQL: XML/Xpath querying is supported
  - PostgreSQL's logs, by default, are all in one place. By changing a couple of settings in a text file,
  you can get it to log to CSV (and since we're talking about PostgreSQL, it's proper CSV, not broken CSV).
  You can easily set the logging level anywhere from "don't bother logging anything" to "full profiling and debugging output".
  The documentation even contains DDL for a table into which the CSV-format logs can be conveniently imported.
   You can also log to stderr or the system log or to the Windows event log (provided you're running PostgreSQL in Windows, of course).  
  The logs themselves are human-readable and machine-readable and contain data likely to be of great value to a sysadmin.
  Who logged in and out, at what times, and from where? Which queries are being run and by whom?
  How long are they taking? How many queries are submitted in each batch? Because the data is well-formatted CSV,
  it is trivially easy to visualise or analyse it in R or PostgreSQL itself or Python's matplotlib or whatever you like.
  - PostgreSQL comes with a set of extensions called contrib modules. There are libraries of functions,
    types and utilities for doing certain useful things which don't quite fall 
    into the core feature set of the server. There are libraries for fuzzy 
    string matching, fast integer array handling, external database connectivity,
    cryptography, UUID generation, tree data types and loads, loads more. A few
    of the modules don't even do anything except provide templates to allow 
    developers and advanced users to develop their own extensions and custom functionality.
</pre>
</div>

<div groupv>
<span title>Non classified</span>
<pre zoom>
<span xsmall>Liquibase</span>
@[http://www.liquibase.org/]
Source control for the DDBB schema.
- Supports code branching and merging
- Supports multiple developers
- Supports multiple database types
- Supports XML, YAML, JSON and SQL formats
- Supports context-dependent logic
- Cluster-safe database upgrades
- Generate Database change documentation
- Generate Database "diffs"
- Run through your build process, embedded in your application or on demand
- Automatically generate SQL scripts for DBA code review
- Does not require a live database connection
</pre>

<pre zoom labels="">
<span xsmall>AWS Serverless PostgreSQL</span>
@[https://aws.amazon.com/blogs/aws/amazon-aurora-postgresql-serverless-now-generally-available/]

- "serverless" relational database service (RDS) in AWS Aurora.
- Automatically starts, scales, and shuts down database capacity 
- per-second billing for applications with less predictable usage patterns.

- It's a *different implementation of the standard versions of these open-source databases.

From the RDS console:
- select the Amazon Aurora database engine PostgreSQL 
- set new DB cluster identifier, specification of credentials, 
- set capacity: 
  - minimum and maximum capacity units for their database, in terms of Aurora Capacity Units (ACUs)
    – a combination of processing and memory capacity. Besides defining the ACUs, users can also
      determine when compute power should stop after a certain amount of idle time.

ºhow capacity settings will work once the database is availableº
- client apps transparently connect to a proxy fleet
  that routes the workload to a pool of resources that 
  are automatically scaled.
- Scaling is very fast because resources are "warm" and
  ready to be added to serve your requests.

- Minimum storage: 10GB, will automatically grow up to 64 TB
 (based on the database usage) in 10GB increments 
 ºwith no impact to database performanceº

ºpricing modelsº
- On-Demand Instance Pricing: pay by hour, no long-term commitments
- Reserved  Instance Pricing: steady-state database workloads 

ºCostº
@[https://aws.amazon.com/rds/aurora/pricing/]

</pre>

<pre zoom labels="java">
<span xsmall>Reactive java Client</span>
@[https://github.com/vietj/reactive-pg-client]
@[https://github.com/eclipse-vertx/vertx-sql-client/tree/3.8/vertx-pg-client]

High performance reactive PostgreSQL client written in Java 
(By Julien Viet, core developer of VertX and Java Crash shell)


</pre>
</div>
</body>
</html>
<!--


ql -U <username> -d <database> -h <hostname>SELECT rolname       list users FROM pg_roles;
_________________
_________________
Error Reporting and Logging
https://www.postgresql.org/docs/current/static/runtime-config-logging.html
_____________________
1.5. Native regular expression support

Regular expressons (regexen or regexes) are as fundamental to analytics work as arithmetic – they are the first choice (and often only choice) for a huge variety of text processing tasks. A data analytics tool without regex support is like a bicycle without a saddle – you can still use it, but it's painful.

PostgreSQL has smashing out-of-the-box support for regex. Some examples:

Get all lines starting with a repeated digit followed by a vowel:

SELECT * FROM my_table WHERE my_field ~ E'^([0-9])\\1+[aeiou]';

Get the first isolated hex string occurring in a field:

SELECT SUBSTRING(my_field FROM E'\\y[A-Fa-f0-9]+\\y') FROM my_table;

Break a string on whitespace and return each fragment in a separate row:

SELECT REGEXP_SPLIT_TO_TABLE('The quick brown fox', E'\\s+');
- - Returns this:

- - | column |
- - 
- - | The    |
- - | quick  |
- - | brown  |
- - | fox    |

Case-insensitively find all words in a string with at least 10 letters:

SELECT REGEXP_MATCHES(my_string, E'\\y[a-z]{10,}\\y', 'gi') FROM my_table;
____________________________
PostgreSQL's documentation is excellent. Everything is covered comprehensively but the documents are not merely reference manuals – they are full of examples, hints, useful advice and guidance. If you are an advanced programmer and really want to get stuck in, you can also simply read PostgreSQL's source code, all of which is openly and freely available. The docs also have a sense of humour:

    The first century starts at 0001-01-01 00:00:00 AD, although they did not know it at the time. This definition applies to all Gregorian calendar countries. There is no century number 0, you go from -1 century to 1 century. If you disagree with this, please write your complaint to: Pope, Cathedral Saint-Peter of Roma, Vatican.

MS SQL Server's documentation is all on MSDN, which is an unfriendly, sprawling mess. Because Microsoft is a large corporation and its clients tend to be conservative and humourless, the documentation is "business appropriate" – i.e. officious, boring and dry. Not only does it lack amusing references to the historical role of Catholicism in the development of date arithmetic, it is impenetrably stuffy and hidden behind layers of unnecessary categorisation and ostentatiously capitalised official terms. Try this: go to the product documentation page for MS SQL Server 2012 and try to get from there to something useful. Or try reading this gem (not cherry-picked, I promise):

    A report part definition is an XML fragment of a report definition file. You create report parts by creating a report definition, and then selecting report items in the report to publish separately as report parts.

Has the word "report" started to lose its meaning yet?

(And, of course, MS SQL Server is closed source, so you can't look at the source code. Yes, I know source code is not the same as documentation, but it is occasionally surprisingly useful to be able to simply grep the source for a relevant term and cast an eye over the code and the comments of the developers. It's easy to think of our tools as magical black boxes and to forget that even something as huge and complex as an RDBMS engine is, after all, just a list of instructions written by humans in a human-readable language.)
____________________

  <a  TODO href="https://www.infoq.com/vendorcontent/show.action?vcr=4727">PSQL for Analytics Apps</a>
   https://aws.amazon.com/rds/postgresql/ 
__________________
Install with official Docker Postgresql images
__________________
TODO: ROLES
https://www.postgresql.org/docs/9.0/static/sql-alterrole.html
  ALTER ROLE name [ [ WITH ] option [ ... ] ]
  
  where option can be:
        SUPERUSER  | NOSUPERUSER
      | CREATEDB   | NOCREATEDB
      | CREATEROLE | NOCREATEROLE
      | CREATEUSER | NOCREATEUSER
      | INHERIT    | NOINHERIT
      | LOGIN      | NOLOGIN
      | CONNECTION LIMIT connlimit
      | [ ENCRYPTED | UNENCRYPTED ] PASSWORD 'password'
      | VALID UNTIL 'timestamp'
    ALTER ROLE name RENAME TO new_name
    
    ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter { TO | = } { value | DEFAULT }
    ALTER ROLE name [ IN DATABASE database_name ] SET configuration_parameter FROM CURRENT
    ALTER ROLE name [ IN DATABASE database_name ] RESET configuration_parameter
    ALTER ROLE name [ IN DATABASE database_name ] RESET ALL
__________________________
New features in PSQL 10
http://m.linuxjournal.com/content/postgresql-10-great-new-version-great-database
_________________
https://dzone.com/articles/rant-there-is-no-nosql-data-storage-engine

https://db-engines.com/en/ranking
_________________
Cloud options:
   Google: https://cloud.google.com/sql/
   AWS: https://???
___________________
PL pgSQL:
  <li><a href="https://www.postgresql.org/docs/10/static/plpgsql.html">Doc</a></li>
___________________
The new release of PostgreSQL 10 certainly helped to further stimulate interest in that product. With the introduction of Declarative Partitioning, improved Query Parallelism, Logical Replication and Quorum Commit for Synchronous Replication, PostgreSQL 10 specifically focused on enhancements for effectively distribute data across many nodes.
______________________
https://www.pgadmin.org/screenshots/#4
______________________
Hot Stand-By:
https://linuxconfig.org/how-to-create-a-hot-standby-with-postgresql
______________________
https://linuxconfig.org/postgresql-performance-tuning-for-faster-query-execution
_________________________
https://crunchydata.github.io/crunchy-containers/
The Crunchy Container Suite provides Docker containers that enable rapid deployment of PostgreSQL, including administration and monitoring tools. Multiple styles of deploying PostgreSQL clusters are supported.
_______________________
Postgresql Replication with Repmgr and Pgbouncer:
https://www.youtube.com/watch?v=wgp_7hzelEc
____________________________
Calculo de ruedas óptimas con PostGis (Postgresql)
https://www.unigis.es/estructuras-de-grafo-para-el-enrutamiento-en-postgis/
_________________________
https://www.postgresql.org/docs/current/static/sepgsql.html
________________________
Security: 
Extracted from https://www.postgresql.org/docs/current/static/sepgsql.html#SEPGSQL-LIMITATIONS
PostgreSQL supports row-level access, but sepgsql does not.
________________________
https://www.postgresql.org/docs/8.1/static/sql-createrole.html
CREATE ROLE adds a new role to a PostgreSQL database cluster. A role is an entity that can own database objects and have database privileges; a role can be considered a "user", a "group", or both depending on how it is used. Refer to Chapter 18 and Chapter 20 for information about managing users and authentication. You must have CREATEROLE privilege or be a database superuser to use this command.
________________________
https://www.percona.com/blog/2018/09/21/securing-postgresql-as-an-enterprise-grade-environment/

Securing PostgreSQL as an Enterprise-Grade Environment
By Avinash Vallarapu Events and Announcements, Insight for DBAs, Insight for Developers, PostgreSQL, Security Database security, enterprise-grade security 1 Comment

In this post, we review how you can build an enhanced and secure PostgreSQL database environment using community software. We look at the features that are available in PostgreSQL that, when implemented, provide improved security.

As discussed in the introductory blog post of this series, in our webinar of October 10, 2018 we highlight important aspects an enterprise should consider for their PostgreSQL environments. This series of blogs addressing particular aspects of the enterprise-grade postgres environment complements the webinar. This post addresses security.
Authentication Layer
Client connections to PostgreSQL Server using host based authentication

PostgreSQL uses a host based authentication file (pg_hba.conf) to authorize incoming connections. This file contains entries with a combination of 5 categories: type, database, user, address, and method. A client is allowed to connect to a database only when the combination of username, database and the hostname of the client matches an entry in the pg_hba.conf file.

Consider the following entry in pg_hba.conf file :
Shell
# TYPE DATABASE USER ADDRESS METHOD
host percona pguser 192.168.0.14/32 md5
1
2
3
    
# TYPE DATABASE USER ADDRESS METHOD
 
host percona pguser 192.168.0.14/32 md5

This entry says that connections from server 192.168.0.14 are only allowed from user pguser and only to the database percona. The method md5 forces password authentication.

The order of the entries in the pg_hba.conf file matters. If you have an entry that rejects connections from a given server followed by another that allows connections from it, the first entry in the order is considered. So, in this case, the connection is rejected.

This is the first layer of protection in authentication. If this criteria is not satisfied in this Access Control List (ACL), PostgreSQL will discard the request without considering even the server authentication.
Server Authentication

Historically, PostgreSQL uses MD5 digest as a password hash by default. The problem with pure MD5 hashing is that this function will always return the same hash for a given password, which renders a MD5 digest more susceptible for password cracking. Newer versions of PostgreSQL implement SCRAM Authentication (Simple Authentication and Secured Layer) that stores passwords in salted and iterated hash formats to strengthen PostgreSQL against offline attacks. SCRAM-SHA-256 support was introduced in PostgreSQL 10. What matters most in terms of “enterprise-grade” security is that PostgreSQL supports industry-standard authentication methods out of the box, like SSL certificates, PAM/LDAP, Kerberos, etc.
Authorization Layer
User management through roles and privileges

It is always recommended to implement segregation of users through roles and privileges. There may be several user accounts in your PostgreSQL server. Only a few of them may be application accounts while the rest are developers or admin accounts. In such cases, PostgreSQL allows you to create multiple roles. Those can be assigned with a set of privileges. Thus, instead of managing user privileges individually, standard roles can be maintained and the appropriate role from the list can be assigned to a user. Through roles, database access can be standardized, which helps in user management and avoids granting too much or too little privilege to a given user.

For example, we might have six roles:
Shell
app_read_write
app_read_only
dev_read_write
dev_read_only
admin_read_write
admin_read_only
1
2
3
4
5
6
    
app_read_write
app_read_only
dev_read_write
dev_read_only
admin_read_write
admin_read_only

Now, if you need to create a new dev user who can only have read access, grant one among the appropriate roles, such as dev_read_only:
Shell
GRANT dev_read_only to avi_im_developer;
1
    
GRANT dev_read_only to avi_im_developer;

Row level Security

Starting with version 9.5, PostgreSQL implements row level security, which can limit access to only a subset of records/rows in a table. Usually a user is granted a mix of SELECT, INSERT, DELETE and UPDATE privileges on a given table, which allows access to all records in the table. Through row level security, however, such privileges can be restricted to a subset of the records by means of a policy, which in turn can be  assigned to a role.

In the next example, we create an employee table and two manager accounts. We then enable row level security on the table and create a policy that allows the managers to only view/modify their own subordinates’ records:
Shell
CREATE TABLE scott.employee (id INT, first_name VARCHAR(20), last_name VARCHAR(20), manager VARCHAR(20));
INSERT INTO scott.employee VALUES (1,'avinash','vallarapu','carina');
INSERT INTO scott.employee VALUES (2,'jobin','augustine','stuart');
INSERT INTO scott.employee VALUES (3,'fernando','laudares','carina');
CREATE USER carina WITH ENCRYPTED PASSWORD 'carina';
CREATE USER stuart WITH ENCRYPTED PASSWORD 'stuart';
CREATE ROLE managers;
GRANT managers TO carina, stuart;
GRANT SELECT, INSERT, UPDATE, DELETE ON scott.employee TO managers;
GRANT USAGE ON SCHEMA scott TO managers;
ALTER TABLE scott.employee ENABLE ROW LEVEL SECURITY;
CREATE POLICY employee_managers ON scott.employee TO managers USING (manager = current_user);
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
    
CREATE TABLE scott.employee (id INT, first_name VARCHAR(20), last_name VARCHAR(20), manager VARCHAR(20));
 
INSERT INTO scott.employee VALUES (1,'avinash','vallarapu','carina');
INSERT INTO scott.employee VALUES (2,'jobin','augustine','stuart');
INSERT INTO scott.employee VALUES (3,'fernando','laudares','carina');
 
CREATE USER carina WITH ENCRYPTED PASSWORD 'carina';
CREATE USER stuart WITH ENCRYPTED PASSWORD 'stuart';
 
CREATE ROLE managers;
GRANT managers TO carina, stuart;
GRANT SELECT, INSERT, UPDATE, DELETE ON scott.employee TO managers;
GRANT USAGE ON SCHEMA scott TO managers;
 
ALTER TABLE scott.employee ENABLE ROW LEVEL SECURITY;
CREATE POLICY employee_managers ON scott.employee TO managers USING (manager = current_user);

In the log we can see that only certain records are visible to each manager:
Shell
$ psql -d percona -U carina
psql (10.5)
Type "help" for help.
percona=> select * from scott.employee ;
id | first_name | last_name | manager
----+------------+-----------+---------
 1 | avinash    | vallarapu | carina
 3 | fernando   | laudares | carina
(2 rows)
$ psql -d percona -U stuart
psql (10.5)
Type "help" for help.
percona=> select * from scott.employee ;
id | first_name | last_name | manager
----+------------+-----------+---------
 2 | jobin      | augustine | stuart
(1 row)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
    
$ psql -d percona -U carina
psql (10.5)
Type "help" for help.
 
percona=> select * from scott.employee ;
id | first_name | last_name | manager
----+------------+-----------+---------
 1 | avinash    | vallarapu | carina
 3 | fernando   | laudares | carina
(2 rows)
 
$ psql -d percona -U stuart
psql (10.5)
Type "help" for help.
 
percona=> select * from scott.employee ;
id | first_name | last_name | manager
----+------------+-----------+---------
 2 | jobin      | augustine | stuart
(1 row)

You can read more about row level security in the manual page.
Data Security
1. Encryption of data over the wire using SSL

PostgreSQL allows you to use SSL to enable encryption of data in motion. In addition, you may enable certification based authentication to ensure that the communication is happening between trusted parties. SSL is implemented by OpenSSL and thus it requires the OpenSSL package to be installed in your PostgreSQL server and PostgreSQL to be built –with-openssl support.

The following entry in a pg_hba.conf file says that connections to any database and from any user are allowed from server 192.68.0.13 as long as the communication is encrypted over SSL. Also, the connection is only established when a valid client certificate is provided:
Shell
# TYPE DATABASE USER ADDRESS METHOD
hostssl all all 192.168.0.13/32 md5
1
2
3
    
# TYPE DATABASE USER ADDRESS METHOD
 
hostssl all all 192.168.0.13/32 md5

Optionally, you may also use Client Certificate Authentication using the following method:
Shell
# TYPE DATABASE USER ADDRESS METHOD
hostssl all all 192.168.0.13/32 cert clientcert=1
1
2
3
    
# TYPE DATABASE USER ADDRESS METHOD
 
hostssl all all 192.168.0.13/32 cert clientcert=1

2. Encryption at Rest – pgcrypto

The pgcrypto module provides cryptographic functions for PostgreSQL, allowing certain fields to be stored encrypted. pgcrypto implements PGP encryption, which is part of the OpenPGP (RFC 4880) standard. It supports both symmetric-key and public-key encryption. Besides the advanced features offered by PGP for encryption, pgcrypto also offers functions for running simple encryption based on ciphers. These functions only run a cipher over data.
Accounting and Auditing
Logging in PostgreSQL

PostgreSQL allows you to log either all of the statements or a few statements based on parameter settings. You can log all the DDLs or DMLs or any statement running for more than a certain duration to the log file when logging_collector is enabled. To avoid write overload to the data directory, you may also move your log_directory to a different location. Here’s a few important parameters you should review when logging activities in your PostgreSQL server:
Shell





1 log_connections
2 log_disconnections
3 log_lock_waits
4 log_statement
5 log_min_duration_statement
    
log_connections
log_disconnections
log_lock_waits
log_statement
log_min_duration_statement

Please note that detailed logging takes additional disk space and may impose an important overhead in terms of write IO depending on the activity in your PostgreSQL server. You should be careful when enabling logging and should only do so after understanding the overhead and performance degradation it may cause to your workload.
Auditing – pgaudit and set_user

Some essential auditing features in PostgreSQL are implemented as extensions, which can be enabled at will on highly secured environments with regulatory requirements.

pgaudit helps to audit the activities happening in the database. If any unauthorized user has intentionally obfuscated the DDL or DML, the statement the user has passed and the sub-statement that was actually executed in the database will be logged in the PostgreSQL log file.

set_user  provides a method of privilege escalations. If properly implemented, it provides the highest level of auditing, which allows the monitoring of even SUPERUSER actions.

You can read more about pgaudit here.
Security Bug Fixes

PostgreSQL Global Development Group (PGDG) considers security bugs seriously. Any security vulnerabilities can be reported directly to security@postgresql.org. The list of security issues fixed for all the supported PostgreSQL versions can be found here. Security fixes to PostgreSQL are made available through minor version upgrades. This is the main reason why it is advised to always maintain PostgreSQL servers upgraded to the latest minor version.
If you liked this post…

Please join Percona’s PostgreSQL Support Technical Lead,  Avinash Vallarapu; Senior Support Engineer, Fernando Laudares; and Senior Support Engineer, Jobin Augustine, on Wednesday, October 10, 2018 at 7:00 AM PDT (UTC-7) / 10:00 AM EDT (UTC-4), as they demonstrate an enterprise-grade PostgreSQL® environment built using a combination of open source tools and extensions.
____________________________
https://www.percona.com/blog/2018/09/28/high-availability-for-enterprise-grade-postgresql-environments/
______________________________
https://www.percona.com/blog/2018/10/02/scaling-postgresql-using-connection-poolers-and-load-balancers-for-an-enterprise-grade-environment/
___________________________
https://www.percona.com/blog/2018/10/08/detailed-logging-for-enterprise-grade-postresql/
___________________________
https://www.percona.com/blog/2018/10/05/postgresql-extensions-for-an-enterprise-grade-system/
PostgreSQL Extensions for an Enterprise-Grade System
________________________________
Expanding with PostgreSQL Extensions

We previously blogged about a couple of FDW extensions (mysql_fdw and postgres_fdw ) which will allow PostgreSQL databases to talk to remote homogeneous/heterogeneous databases like PostgreSQL and MySQL, MongoDB, etc. We will now cover a few other additional extensions that can expand your PostgreSQL server capabilities.

pg_stat_statements: provides a means for tracking execution statistics of all
SQL statements executed by a server. The statistics gathered by the module
are made available via a view named pg_stat_statements.

pg_repack: Tables may end up with fragmentation.
      pg_repack is the most popular way to address this problem pgaudit

pgaudit: caters with major compliance requirement for many security standards. 
 providing detailed session and/or object audit logging via the standard PostgreSQL
 logging facility.


pldebugger: ¡¡must-have!! extension for developers who work on stored functions
written in PL/pgSQL. This extension is well integrated with GUI tools like pgadmin,
which allows developers to step through their code and debug it. 

plprofiler: wonderful extension for finding out where the code is slowing down. 
particularly during complex migrations from proprietary databases,

PostGIS: arguably the most versatile implementation of the specifications
 of the Open Geospatial Consortium. 

cstore_fdw: columnar store extension for PostgreSQL.
 Columnar stores provide notable benefits for analytics use cases where data
 is loaded in batches. Cstore_fdw’s columnar nature delivers performance by
  only reading relevant data from disk. It may compress data by 6 to 10 times
 to reduce space requirements for data archive. 

HypoPG: extension for adding support for hypothetical indexes 
– that is, without actually adding the index. This helps us to answer questions
 such as “how will the execution plan change if there is an index on column X?”. 


mongo_fdw: presents collections from mongodb as tables in PostgreSQL.
 This is a case where the NoSQL world meets the SQL world and features combine.

tds_fdw: Another important FDW (foreign data wrapper) extension 
   Both Microsoft SQL Server and Sybase uses TDS (Tabular Data Stream) format.

orafce: there are lot of migrations underway from Oracle to PostgreSQL. 
Incompatible functions in PostgreSQL are often painful 
 The “orafce” project implements some of the functions from the Oracle database. 
The functionality was verified on Oracle 10g and the module is useful for
 production work. 


TimescaleDB: In this new world of IOT and connected devices, there is a
growing need of time-series data. Timescale can convert PostgreSQL into a
scalable time-series data store. 

pg_bulkload: Is loading a large volume of data into database in a very efficient
and faster way a challenge for you? 

pg_partman: PostgreSQL 10 introduced declarative partitions. But creating new 
partitions and maintaining existing ones, including purging unwanted partitions,
requires a good dose of manual effort. If you are looking to automate part
 of this maintenance you should have a look at what pg_partman offers. 

wal2json: PostgreSQL has feature related to logical replication built-in. 
Extra information is recorded in WALs which will facilitate logical decoding. 
wal2json is a popular output plugin for logical decoding. 
This can be utilized for different purposes including change data capture.
____________________________________
https://www.infoq.com/news/2018/10/space-time-series-data
he European Space Agency Science Data Center (<a href="https://www.cosmos.esa.int/web/esdc" target="_blank">ESDC</a>) <a href="https://blog.timescale.com/european-space-agency-postgresql-geospatial-time-series-9cced899c41d" target="_blank">switched to PostgreSQL</a> with the TimescaleDB extension for their data storage. ESDC’s diverse data includes structured, unstructured and time series metrics running to hundred of terabytes, and querying requirements across datasets with open source tools.</p>

<p>ESDC collects massive amounts of data - <a href="https://www.nature.com/articles/d41586-018-05484-4" target="_blank">terabytes per day</a> - from each of their space missions and makes it available to various teams <a href="https://esdcnews.esac.esa.int/news/2018-05/" target="_blank">including the general public</a>. This data - consisting of metadata about missions and satellites and data generated during the missions - can be both structured and unstructured. Generated data includes geo-spatial and time series data. Cross referencing of datasets was a requirement while choosing a data storage solution, as was the need to be able to use readily available, open source tools to analyze the data. The team wanted to move away from legacy systems like Oracle and Sybase.</p>

<p>ESDC's team settled on <a href="https://www.infoq.com/postgres/" target="_blank">PostgreSQL</a> for its maturity and support for various data types and unstructured data. In addition to these routine requirements, ESDC also needed to store and process geo-spatial and time series data. Geospatial data is data which has location information attached to them such as a planet's position in the sky. This had to be done without using different data stores for different types or sources of data. The decision to move to PostgreSQL was motivated by the availability of an <a href="https://www.postgresql.org/docs/current/static/contrib.html" target="_blank">extension mechanism</a> which allows for such processing. PostgreSQL has native support for JSON and full text search. The <a href="http://postgis.net/" target="_blank">PostGIS</a>, <a href="https://pgsphere.github.io/" target="_blank">pg_sphere</a> and <a href="https://github.com/segasai/q3c" target="_blank">q3c</a> extension allowed ESDC to use normal SQL to run location based queries and more specialized analyses.</p>

<p>PostgreSQL also had to efficiently and scalably store time series data generated from missions such as the <a href="https://www.esa.int/Our_Activities/Space_Science/Solar_Orbiter" target="_blank">Solar Orbiter project</a>. This had low write speed requirements as the collected data is <a href="https://directory.eoportal.org/web/eoportal/satellite-missions/s/solar-orbiter-mission" target="_blank">stored locally</a> in the satellite "for later downlink during daily ground station passes" and inserted into the database in batches. However, queries against this db had to support structured data types, ad-hoc matching between datasets and large datasets of up to hundreds of TBs. It's unclear which specific time series databases were evaluated, but the team did not opt for any of them as they had standardized on SQL as the query language of choice, and PostgreSQL as the platform since it satisfied their other requirements. There <a href="https://grisha.org/blog/2015/09/23/storing-time-series-in-postgresql-efficiently/" target="_blank">have</a> been <a href="https://www.citusdata.com/blog/2018/01/24/citus-and-pg-partman-creating-a-scalable-time-series-database-on-PostgreSQL/" target="_blank">past</a> <a href="https://blog.2ndquadrant.com/scaling-iot-time-series-data-postgres-bdr/" target="_blank">approaches</a> to store time series data on PostgreSQL. Its recent <a href="https://www.infoq.com/news/2017/12/Postgres-10-Features-Developers" target="_blank">partitioning</a> feature attempts to solve the problem of keeping large table indexes in memory and writing them to disk on every update by splitting tables into smaller partitions. Partitioning can also be used to store time series data when the partitioning is done by time, followed by indices on those partitions. ESDC's efforts to store time series data ran into performance issues, and they switched to an extension called <a href="https://www.timescale.com/" target="_blank">TimescaleDB</a>.</p>

<p><img _href="img://1timescale-hypertable-1539442589763.png" alt="" data-src="news/2018/10/space-time-series-data/en/resources/1timescale-hypertable-1539442589763.png" style="width: 600px; height: 344px;" src="https://res.infoq.com/news/2018/10/space-time-series-data/en/resources/1timescale-hypertable-1539442589763.png" rel="share"></p>

<p>Image courtesy: <a href="https://blog.timescale.com/when-boring-is-awesome-building-a-scalable-time-series-database-on-postgresql-2900ea453ee2" target="_blank">https://blog.timescale.com/when-boring-is-awesome-building-a-scalable-time-series-database-on-postgresql-2900ea453ee2</a></p>

<p>TimescaleDB uses an abstraction called a <a href="https://docs.timescale.com/v1.0/introduction/architecture" target="_blank">hypertable</a> to hide partitioning across multiple dimensions like time and space. Each hypertable is split into "chunks", and each chunk corresponds to a specific time interval. Chunks are sized so that all of the B-tree structures for a table's indices can reside in memory during inserts, similar to how PostgreSQL does partitioning. Indices are auto-created on time and the partitioning key. Queries can be run against arbitrary "<a href="https://news.ycombinator.com/item?id=14041467" target="_blank">dimensions</a>", just like other time series databases allow querying <a href="https://docs.influxdata.com/influxdb/v1.6/concepts/glossary/#tag" target="_blank">against</a> <a href="http://opentsdb.net/docs/build/html/user_guide/query/timeseries.html#tags-vs-metrics" target="_blank">tags</a>. One of the differences between TimescaleDB and other partitioning tools like <a href="https://github.com/pgpartman/pg_partman" target="_blank">pg_partman</a> is <a href="https://news.ycombinator.com/item?id=14036252" target="_blank">support for auto-sizing</a> of partitions. Although TimescaleDB has <a href="https://blog.timescale.com/time-series-data-postgresql-10-vs-timescaledb-816ee808bac5" target="_blank">reported</a> higher performance benchmarks compared to PostgreSQL 10 partitioning based solutions and <a href="https://blog.timescale.com/timescaledb-vs-influxdb-for-time-series-data-timescale-influx-sql-nosql-36489299877" target="_blank">InfluxDB</a>, there have been <a href="https://news.ycombinator.com/item?id=14041870" target="_blank">concerns about maintainability</a>. Clustered deployments of TimescaleDB is still under development at the time of this writing.</p>

<p>TimescaleDB is open source and <a href="https://github.com/timescale/timescaledb" target="_blank">hosted on Github</a>.</p>

____________________________
https://www.timescale.com/
"
Optimized query engine

We modified the PostgreSQL insert path, execution engine, and query planner to intelligently process queries across chunks. We continue to leverage PostgreSQL’s battle-tested storage layer for the reliability you need to support mission critical workloads.
"
____________________________
systemctl status output:
/system.slice/postgresql.service CGroup
  ├─7117 /usr/bin/postgres -D /var/lib/pgsql/data -p 5432
  ├─7118 postgres: logger process
  ├─7120 postgres: checkpointer process
  ├─7121 postgres: writer process
  ├─7122 postgres: wal writer process
  ├─7123 postgres: autovacuum launcher process
  └─7124 postgres: stats collector process

$ ps hf -u postgresq -o cmd
/usr/pgsql-9.4/bin/postgres -D /var/lib/pgsql/9.4/data
\_ postgres: logger       process
\_ postgres: checkpointer process
\_ postgres: writer       process
\_ postgres: wal writer   process
\_ postgres: autovacuum launcher  process
\_ postgres: stats collector  process
\_ postgres: postgres pgbench [local] idle in transaction
\_ postgres: postgres pgbench [local] idle
\_ postgres: postgres pgbench [local] UPDATE
\_ postgres: postgres pgbench [local] UPDATE waiting
\_ postgres: postgres pgbench [local] UPDATE

________________________
https://www.slideshare.net/alexeylesovsky/deep-dive-into-postgresql-statistics-54594192
________________________

https://www.slideshare.net/alexeylesovsky/deep-dive-into-postgresql-statistics-54594192



       Client Backends              |                                                  |Postmaster
      [pg_stat_activity]            |                                                  |[pg_stat_database]
    -----------------------         |                                                  |----------------
       Query Planning               |  Shared Buffers                                  |Background Workers
                                    |  [pg_buffercache]                                |
    -----------------------         |                                                  |------------------
    Query Execution                 |                                                  |Autovacuum Launcher
                                    |                                                  |                   
    =======================         |                                                  |-------------------
    Indexes IO  | Tables IO         |                                                  |Autovacuum Workers
                |                   |                                                  |[pg_stat_activity]                   
                |                   |                                                  |[pg_stat_user_tables]
    -----------------------         |                                                  |
        Buffers IO                  |                                                  |
    [pg_stat_database]              |                                                  |
    [pg_statio_all_indexes]         |                                                  |
    [pg_statio_all_tables]          |                                                  |
    =============================================================================================================
             Write Ahead Log                                      
        [pg_current_xlog_location]
        [pg_xlog_location_diff]
    ============================================================================================================
                                                      |                                              
    Logger Process                                    |                        Stats Collector       
                                                      |                                              
    ============================================================================================================
    Logical               | WAL Sender           |Archiver           | Background         |  Checkpointer
    Replication           |   Process            | Process           |  Writer            |   Process
    [pr_replication_slots]| [pg_stat_replication]|[pg_stat_archiver] | [pg_stat_bgwriter] |  [pg_stat_database]
                          |                      |                   |                    |  [pg_stat_bgwriter]
    ============================================================================================================
                                                     |           
                  NETWORK                            |               STORAGE
                                                     |         [pg_stat_kcache]               
        (nicstat, iostat, ...)                       |         (iostat, ...)                  
    ============================================================================================================
         WAL Receiver Process     |                           |  Tables/Indexes Data Files
                                  |                           |  [pg_index_size]     [pgstattupple]
    ------------------------------|                           |  [pg_table_size]
            Recovery Process      |                           |  [pg_database_size]
     [pg_stat_database_conflicts] |                           |
    ==============================+                           +=================================================

____________________________
https://github.com/dataegret/pg-utils
__________________________________
The Internals of PostgreSQL!!!
http://www.interdb.jp/pg/index.html
__________________________________
https://www.postgresql.org/docs/10/static/pgbench.html
https://www.cri.ensmp.fr/~coelho/cours/si/pg-latency-20170323-handout.pdf
___________________________
https://blog.yugabyte.com/introducing-ysql-a-postgresql-compatible-distributed-sql-api-for-yugabyte-db/
Introducing YSQL: A PostgreSQL Compatible Distributed SQL API for YugaByte DB
__________________________________
https://www.postgresql.org/docs/9.1/sql-notify.html
https://www.postgresql.org/docs/9.1/sql-listen.html
http://camel.apache.org/pgevent.html
__________________________________
https://wiki.postgresql.org/wiki/PostgreSQL_for_Oracle_DBAs
_________________________________
pgbench : Postgresql benchmark (Useful for tunning also)
https://blog.pgaddict.com/posts/postgresql-performance-on-ext4-and-xfs
_____________________
https://paquier.xyz/postgresql-2/tuning-disks-and-linux-for-postgres.markdown/
____________________________
https://www.percona.com/blog/2019/02/13/plprofiler-getting-a-handy-tool-for-profiling-your-pl-pgsql-code/
plprofiler – Getting a Handy Tool for Profiling Your PL/pgSQL Code
By Jobin Augustine Insight for DBAs, PostgreSQL database tools, PostgreSQL Performance Tuning 1 Comment

PostgreSQL is emerging as the standard destination for database migrations from proprietary databases. As a consequence, there is an increase in demand for database side code migration and associated performance troubleshooting. One might be able to trace the latency to a plsql function, but explaining what happens within a function could be a difficult question. Things get messier when you know the function call is taking time, but within that function there are calls to other functions as part of its body. It is a very challenging question to identify which line inside a function—or block of code—is causing the slowness. In order to answer such questions, we need to know how much time an execution spends on each line or block of code. The plprofiler project provides great tooling and extensions to address such questions.
_________________________
https://www.2ndquadrant.com/en/resources/postgres-bdr-2ndquadrant/
Multimaster PostgreSQL (Commercial product)
_____________________________
https://postgreshelp.com/postgresql-kernel-parameters/
__________________________
https://www.infoq.com/news/2019/04/change-data-capture-debezium

Creating Events from Databases Using Change Data Capture: Gunnar Morling at MicroXchg Berlin

    When you store data in a database, you often also want to put the same data
in a cache and a search engine. The challenge then is how to keep all data in 
sync when you want to stay away from distributed transactions and dual writes. 

One solution is to use a change data capture (CDC) tool that captures and 
publishes changes in a database. In a presentation at MicroXchg Berlin, 
Gunnar Morling described Debezium, an implementation of CDC using Apache 
Kafka to publish changes as event streams.

    Morling, software engineer at Red Hat, describes Debezium as an open 
source CDC tool built on top of Kafka that reads the transaction logs in 
a database and creates streams of events. Other applications can then 
asynchronously consume these events in the correct order and update their 
own data storage according to their needs.

Transaction log files are append-only log files, used for rollback of 
transactions and replication, and for Morling they are an ideal source 
for capturing changes made in a database, since they contain all changes 
made and in the correct order. All databases have their own APIs for 
reading the log files, so Debezium comes with connectors for several 
databases. On the output side Debezium produces one generic and abstract 
event representation for Kafka.

Besides using CDC for updating caches, services and search engines, 
Morling notes some other use cases, including:

Data replication. Commonly used for replication of data into another type of 
database or data warehouse.  Auditing. By keeping the history of events, 
possibly after enriching each event with metadata, you will have an audit of 
all changes made. One example of metadata that may be interesting is the current user.

In an microservices based system, commonly services need data from other 
services, but Morling points out that we should to stay away from shared 
databases. Using REST calls will increase the coupling between services; 
instead, we can use CDC for such scenarios. By creating streams with changes, 
other services can subscribe to these changes and update their local databases
. This pipeline of events is asynchronous, which means services can fall 
behind in case of for instance network problems, but they will catch up 
eventually \u2014 the system is eventually consistent.

______________________________
TODO: (Video) InfoQ: Developing an Intelligent Analytics App with PostgreSQL
   
https://www.infoq.com/vendorcontent/show.action?vcr=4675

Azure Database for PostgreSQL brings together the community edition database engine and capabilities of a fully managed service—so you can focus on your apps instead of having to manage a database.
______________________________
Blockchain on Postgresql: (IBM Research)
https://arxiv.org/pdf/1903.01919.pdf
____________________________
The replication of transaction logs
and state among multiple database nodes is possible using master-slave [13, 21, 38, 42] and master-master [21, 40, 42] protocols
REF:
- P. A. Alsberg and J. D. Day. A principle for resilient
  sharing of distributed resources. In Proceedings of the 2Nd International Conference on Software Engineering
  ICSE ’76, pages 562–570, Los Alamitos, CA, USA, 1976. IEEE Computer Society Press.
-  E. Cecchet, G. Candea, and A. Ailamaki.
  Middleware-based database replication: The gaps between theory and practice. In Proceedings of the
  2008 ACM SIGMOD International Conference on Management of Data , SIGMOD ’08, pages 739–752,
  New York, NY, USA, 2008. ACM
- M. Stonebraker. Concurrency control and consistency of multiple copies of data in distributed ingres.
  IEEE Transactions on Software Engineering,
  SE-5(3):188–194, May 1979
- M. Wiesmann, F. Pedone, A. Schiper, B. Kemme, and G. Alonso. Understanding replication in databases
  and distributed systems. In Proceedings 20th IEEE International Conference on Distributed Computing
  Systems, pages 464–474, April 2000.
- . H. Thomas. A majority consensus approach to concurrency control for multiple copy databases.
  ACM Trans. Database Syst.  , 4(2):180–209, June 1979. H. Thomas. A majority consensus approach to
  concurrency control for multiple copy databases.  ACM Trans. Database Syst.  , 4(2):180–209, June 1979
- M. Wiesmann, F. Pedone, A. Schiper, B. Kemme, and G. Alonso. Understanding replication in databases
  and distributed systems. In Proceedings 20th IEEE International Conference on Distributed Computing
  Systems , pages 464–474, April 2000.
_______________________
(6) Serializability isolation, ACID:
Blockchain trans-
actions require serializable isolation, in which dirty read,
non-repeatable read, phantom reads, and serialization anoma-
lies are not possible. Transactions, when executed in paral-
lel, must follow the same serializable order across all nodes.
Further, transactions should be ACID [18] compliant. Seri-
alizable isolation can be achieved in databases by employing:
(i) strict 2-phase locking [32], (ii) optimistic concurrency
control [28], or (iii) SSI [19, 36]. This needs to be enhanced
to follow the block order as determined through consensus,
but can be leveraged to a large extent.
_____________________
PSQL Hackers list (Patches, dev. discussions, ...)
https://www.postgresql.org/list/pgsql-hackers/
________________________
https://github.com/gregs1104/pgtune
PostgreSQL configuration wizard
pgtune takes the wimpy default postgresql.conf and expands the database server to be as powerful as the hardware it's being deployed on.
There is no need to build/compile pgtune, it is a Python script
_______________________________
REF: Blockchain Meets Database: Design and Implementation
     of a Blockchain Relational Database
     ("Hacking and Morphing Postgresql into a blockchain)
     https://arxiv.org/pdf/1903.01919.pdf
______________
PostgreSQL [8] is the first open source database to implement the abort during commit SSI variant [36]
https://drkp.net/papers/ssi-vldb12.pdf
Serializable Snapshot Isolation in PostgreSQL
____________________________
PostgreSQL supports three isolation levels:
- read committed
- repeatable read (i.e., SI)
- and serializable (i.e., SI with detection and mitigation of anomalies).  


A snapshot comprises of a set of transaction IDs, which were committed as of 
the start of this transaction, whose effects are visible. Each row has two 
additional elements in the header, namely xmin and xmax , which are the IDs 
of the transactions that created and deleted the row, respectively. Note, 
every update to a row is a delete followed by an insert (both in the table 
and index). Deleted rows are flagged by setting xmax instead of being 
actually deleted. In other words, PostgreSQL maintains all versions of a row 
unlike other implementations such as Oracle that update rows in-place and 
keep a rollback log. This is ideal for our goal of building a blockchain that 
maintains all versions of data. For each row, a snapshot checks xmin and xmax 
to see which of these transactions’ ID are included in the snapshot to 
determine row visibility
______________________
https://www.howtoforge.com/how-to-setup-postgresql-streaming-replication-with-replication-slots-on-debian-10/
__________________
https://www.infoq.com/articles/postgres-handles-more-than-you-think/ !!!
______________________
https://searchdatamanagement.techtarget.com/news/252472281/PostgreSQL-12-boosts-open-source-database-performance
PostgreSQL 12 boosts open source database performance
____________________
https://www.percona.com/doc/percona-xtrabackup/2.4/release-notes/2.4/2.4.17.html?utm_campaign=2019%20Blog%20Q4&utm_content=108198646&utm_medium=social&utm_source=linkedin&hss_channel=lcp-421929
----------------
who is who:
Peter Zaitsev, Percona Founder.
 Driving Success with MySQL, MariaDB, MongoDB & PostgreSQL 
-->
