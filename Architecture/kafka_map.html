<!DOCTYPE html>
<html>
   <meta charset="UTF-8">
   <title>Kafka map (v0.1)</title>  <!-- ignore -->
<head>
<script type="module">import "/map_v1.js";</script>
<link rel="stylesheet" type="text/css" href="/map_v1.css" />
</head>

<body>
<div groupv>
    <span title>Summary</span><br/>
<pre zoom labels="resource">
<span xsmall>External Links</span>
- Documentation:
@[http://kafka.apache.org/documentation/]

- Clients: (Java,C/C--,Python,Go,...)
@[https://cwiki.apache.org/confluence/display/KAFKA/Clients]

- Official JAVA JavaDoc API:
 @[http://kafka.apache.org/11/javadoc/overview-summary.html]

- Papers, ppts, ecosystem, system tools, ...:
@[https://cwiki.apache.org/confluence/display/KAFKA/Index]

- Kafka Improvement Proposals (KIP)
@[https://cwiki.apache.org/confluence/display/KAFKA/Kafka+Improvement+Proposals]

- Data pipelines with Kotlin+Kafka+(async)Akka
@[../JAVA/kotlin_map.html?query=data+pipelines+kafka]

- Cheatsheets:
@[https://lzone.de/cheat-sheet/Kafka]
@[https://ronnieroller.com/kafka/cheat-sheet]
@[https://abdulhadoop.wordpress.com/2017/11/06/kafka-command-cheat-sheet/]
@[https://github.com/Landoop/kafka-cheat-sheet]
</pre>


<pre zoom labels="101,diagram">
<span xsmall>Global Diagram</span>
(one+ server in 1+ datacenters)

☞Bºbest practiceº: publishers must be unaware of underlying 
   LOG partitions and only specify a partition key:
   While LOG partitions are identifiable and can be sent 
   to directly, it is not recommended. Higher level 
   constructs are recommended.

┌────────┐              ┌──────────┐               ┌───────────┐
│PRODUCER│ 1 ←······→ 1 │TOPIC     │ 1 ←···→ 0...N │SUBSCRIBERS│
│        ··→ stream of··→(CATEGORY)│               │           │
└────────┘ OºRECORDsº   └──────────┘               └───────────┘
  ↑          ======       1                          1 ← Normally, 1←→1, but not necesarelly
  |          -key         ↑                          ↑       
  ·          -value       ·                          ·     
  ·          -timeStamp   ·                          ·
  ·                       ·                          ·  GºCONSUMER GROUPº: cluster of consumers
  ·                       ·                          ↓    |  (vs single process consumer) º*1º
(optionally) producers    ·                          1    ↓
can wait for ACK.         ↓                          CONSUMER   |    CONSUMER       ←───┐
  ·                       1                          GROUP A    |    GROUP B            │
  ·   ┌───┬────────────────────┬──────────────────┐  ---------  | --------------------   
  ·   │LOG│                    │  ordered records │  Cli1 Cli2  | Cli3 Cli4 Cli5 Cli6   │
  |   ├───┘                    │  ─────────────── │   |    |    |  |    |    |    |      
  └···→┌Partitionº0ºreplica ┌1 │  0 1 2 3 4 5     │  ←┤···········←┘    ·    ·    ·     │
      │└Partitionº0ºreplica │2┐│  0 1 2 3 4 5     │   ·    ·    |       ·    ·    ·      
      │┌Partition 1 replica ├1││  0 1 2 3         │   ·    ·    |       ·    ·    ·     │
      │└Partition 1 replica │2┤│  0 1 2 3         │  ·+···←┤    |       ·    ·    ·      
      │┌Partitionº2ºreplica ├1││  ...             │   ·    |    |       ·    ·    ·     │
      │└Partitionº2ºreplica │2┤│                  │   |···←┘    |       ·    ·    ·      
      │┌Partition 3 replica ├1││ - Partitions are │  ←┴················←┘    ·    ·     │
      │└Partition 3 replica │2┤│ independent and  │             |            ·    ·      
      │┌Partitionº4ºreplica ├1││ grow at differn. │             |            ·    ·     │
      │└Partitionº4ºreplica │2┤│ rates.           │             |            ·    ·      
      │┌Partition 5 replica ├1││                  │             |            ·    ·     │
      │└Partition 5 replica │2┤│ - Records expire │  ←······················←┘    ·      
      │┌Partitionº6ºreplica └1││ and can not be   │             |                 ·     │
      │└Partitionº6ºreplica  2┘│ deleted manually │  ←···························←┘      
      └──↑───────────────────^─┴──────────────────┘ * num. of group instances           │
                      │      │    ←── offset ─→       must be <= # partitions            
                      │      └────────────┐   record.offset (sequential id)             │
┌─────────────────────┴─────────────────┐ │   uniquelly indentifies the record           
─ Partitions serve several purposes:      │   within the partition.                     │
  ─ allow scaling beyond a single server. │                                              
  ─ act as the ☞BºUNIT OF PARALLELISMº.   │  - aGºCONSUMER GROUPºis a view (state,  ┐   │
─ Partition.ºRETENTION POLICYº indicates  │    position, or offset) of full LOG.    │
  how much time records will be available │  - consumer groups enable different     ├───┘
  for compsumption before being discarded.│    apps to have a different view of the │
-Bºnumber of partitions in an event hubº  │    LOG, and to read independently at    │
 Bºdirectly relates to the number of      │    their own pace and with their own    ┘
 Bºconcurrent readers expected.º          │    offsets:
-OºTOTAL ORDER  of events is just guaran-º│  ☞Bºin a stream processing Arch,º
 Oºteed inside a partition.º              │   Bºeach downstream applicationº 
   Messages sent by a producer to a given │   Bºequates to a consumer groupº
   partition are guaran. to be appended   │  
   in the order they were sent.           │  
          ┌───────────────────────────────┘           
- Messages sent by a producer to a particular topic partition are guaranteed
  to be appended in the order they are sent.
  ┌───────┴────────┐
  PARTITION REPLICA: (fault tolerance mechanism) 
  └ Kafka allows producers to wait on acknowledgement so that a write 
    isn't considered complete until it is fully replicated and guaranteed 
    to persist even if the server written to fails, allowing to balance 
    replica consistency vs performance.
  └ Each partition has one replica server acting as leader" and 0+ 
    replica servers "followers". The leader handles all read 
    and write requests for the partition while the followers passively 
    replicate the leader. If the leader fails, one of the followers 
    replaces it.
  └ A server can be a leader for partition A and a follower for 
    partition B, providing better load balancing.
  └ For a topic with replication factor N, we will tolerate up to N-1 
    server failures without losing any records committed to the log.

º*1:º
 - Log partitions are (dnyamically) divided over consumer instances so 
   that each client instance is the exclusive consumer of a "fair share"
   of partitions at any point in time.
 - The consumer group generalizes the queue and publish-subscribe:
   - As with a queue the consumer group allows you to divide (scale) up
     processing over a collection of processes (the members of the consumer group).
   - As with publish-subscribe, Kafka allows you to broadcast messages to
     multiple consumer groups


</pre>

<span title>data-schema Support</span>
<pre zoom labels="TODO">
<span xsmall>Apache AVRO format</span>
Efficient compac way to store data in disk.
@[https://shravan-kuchkula.github.io/kafka-schemas/#understand-why-data-schemas-are-a-critical-part-of-a-real-world-stream-processing-application]
<hr/>
<span xsmall>Kafka Schema Registry</span>
BºDZone Intro Summaryº
@[https://dzone.com/articles/kafka-avro-serialization-and-the-schema-registry]
by Jean-Paul Azar

Confluent Schema Registry:
  - REST API for producers/consumers managing Avro Schemas:
    - store schemas for keys and values of Kafka records.
    - List schemas and schema-versions by subject.
    - Return schema by version or ID.
    - get the latest version of a schema.
    - Check if a given schema is compatible with a certain version. 
       - Compatibility level include:
         - backward: data written with old schema readable with new one.
         - forward : data written with new schema is readable with old one
         - full    : backward + forward
         - none    : Schema is stored by not schema validation is disabled
                   (Rºnot recommendedº).
       - configured Bºglobally or per subjectº.

  - Compatibility settings can be set to support Bºevolution of schemasº.

  - Kafka Avro serialization project provides serializers 
    taking schemas as input?
  - Producers send the schema (unique) ID and consumers fetch (and cache)
    the full schema from the Schema Registry.

  - Producer will create a new Avro record (schema ID, data). 
    Kafka Avro Serializer will register (and cache locally) the associated
    schema if needed, before serializing the record.

  - Schema Registry ensures that producer and consumer see compatible
    schemas and Bºautomatically "transform" between compatible schemas,º
  Bºtransforming payload via Avro Schema Evolutionº.
   

BºSCHEMA EVOLUTIONº
  Scenario:
  - Avro schema modified after data has already been written to store
    with old schema version.
 
OºIMPORTANTº: ☞ From Kafka perspective,Oºschema evolutionº happens only
              Oºduring deserialization at the consumer (read)º:
                If consumer’s schema is different from the producer’s schema,
                and they are compatible, the value or key is automatically 
                modified during deserialization to conform to the consumer's
                read schema if possible.

BºSchema Evolution: Allowed compatible Modification º
- change/add field's default value.
- add new         field with default value.
- remove existing field with default value.
- change field's order attribute.
- add/remove      field-alias (RºWARN:º can break consumers depending on the alias).
- change type → union-containing-original-type.

BºBest Patternº
- Provide a default value for fields in your schema.
- Never change a field's data type.
- Do NOT rename an existing field (use aliases instead).

Ex: Original schema v1: 

{
  "namespace": "com.cloudurable.phonebook",
  "type": "record",
  "name": "Employee",
  "doc" : "...",
  "fields": [
    {"name": "firstName", "type": "string"                            },
    {"name": "nickName" , "type": ["null", "string"] , "default" : null},
    {"name": "lastName" , "type": "string"                             },
    {"name": "age"      , "type": "int"              , "default": -1   },
    {"name": "emails"   , "type": {"type" : "array",
                                   "items": "string"}, "default":[]    },
    {"name": "phoneNum" , "type":
                                  [ "null",
                                    { "type": "record",
                                      "name": "PhoneNum",
                                      "fields": [
                                        {"name": "areaCode"   , "type": "string"},
                                        {"name": "countryCode", "type": "string", "default" : ""},
                                        {"name": "prefix"     , "type": "string"},
                                        {"name": "number"     , "type": "string"}
                                      ]
                                    }
                                  ]
    },
    {"name": "status"                                , "default" :"SALARY", 

                        , "type": {
                                    "type": "enum",
                                    "name": "Status",
                                    "symbols" : ["RETIRED", "SALARY",...]
                                  }                  
    }
  ]
}

- Schema Version 2: 
  "age" field, def. value -1, added


               | KAFKA LOG |
  Producer@v2 →|Employee@v2| ··→ consumer@v.1 ·····→ NoSQL Store
               |           |     ^                    ^
               |           |     age field removed    'age' missing
               |           |     @deserialization
               |           |     
               |           |     
               |           |     consumer@ver.2 ←..... NoSQL Store
               |           |     ^                     ^
               |           |     age set to -1         'age' missing


BºRegistry REST API Ussageº

  └ POST New Schema
$º$ curl -X POST -H "Content-Type: application/vnd.schemaregistry.v1+json" \º
$º$   --data '{"schema": "{\"type\": …}’ \                                  º
$º$   http://localhost:8081/subjects/Employee/versions                      º

  └ List all of the schemas:

$º$ curl -X GET http://localhost:8081/subjects                              º

    Using Java OkHttp client:

    package com.cloudurable.kafka.schema;
    import okhttp3.*;
    import java.io.IOException;
    public class SchemaMain {
        private final static 
        MediaType SCHEMA_CONTENT =
                MediaType.parse("application/vnd.schemaregistry.v1+json");
        private final static String EMPLOYEE_SCHEMA = "{ \"schema\": \"" ...;
        private final static String BASE_URL = "http://localhost:8081";
     
    
        private final OkHttpClient client = new OkHttpClient();

        private static newCall(Requeste request) {
            System.out.println(client.newCall(request).execute().body().string() );
        }
        private static putAndDumpBody (final String URL, RequestBody BODY) {
            newCall(new Request.Builder().put(BODY).url(URL).build());
        }
        private static postAndDumpBody(final String URL, RequestBody BODY) {
            newCall(new Request.Builder().post(BODY).url(URL).build());
        }
    
        private static getAndDumpBody(final String URL) {
            request = new Request.Builder() 
                    .url(URL).build();
            System.out.println(client.newCall(request).
                   execute().body().string());
        }
    
        public static void main(String... args) throws IOException {
            System.out.println(EMPLOYEE_SCHEMA);
    
            postAndDumpBody(                            // ← POST A NEW SCHEMA
              BASE_URL + "/subjects/Employee/versions",
              RequestBody.create( SCHEMA_CONTENT, EMPLOYEE_SCHEMA )
            );
    
            getAndDumpBody(BASE_URL + "/subjects");     // ← LIST ALL SCHEMAS
    
            getAndDumpBody(BASE_URL                     // ← SHOW ALL VERSIONS
                 + "/subjects/Employee/versions/");
    
            getAndDumpBody(BASE_URL                     // ← SHOW VERSION 2 OF EMPLOYEE
                 + "/subjects/Employee/versions/2");
    
            getAndDumpBody(BASE_URL                     // ← "SHOW SCHEMA WITH ID 3
                 + "/schemas/ids/3");
    
            getAndDumpBody(BASE_URL                     // ← SHOW LATEST VERSION
                 + "/subjects/Employee/versions/latest");
    
            postAndDumpBody(                            // ← SCHEMA IS REGISTERED?
              BASE_URL + "/subjects/Employee",
              RequestBody.create( SCHEMA_CONTENT, EMPLOYEE_SCHEMA )
            );
    
            postAndDumpBody(                            // ← //TEST COMPATIBILITY
              BASE_URL + "/compatibility/subjects/Employee/versions/latest",
              RequestBody.create( SCHEMA_CONTENT, EMPLOYEE_SCHEMA )
            );
    
            getAndDumpBody(BASE_URL                     // ← TOP LEVEL CONFIG
                 + "/config");
    
            putAndDumpBody(                            // ← SET TOP LEVEL CONFIG VALs
              BASE_URL + "/config",                    //   VALs :=none|backward|
              RequestBody.create(SCHEMA_CONTENT,                   forward|full
                 "{\"compatibility\": \"none\"}"
            );
    
            putAndDumpBody(                            // ← SET CONFIG FOR EMPLOYEE
              BASE_URL + "/config/Employee",          //
              RequestBody.create(SCHEMA_CONTENT,
                 "{\"compatibility\": \"backward\"}"
            );
        }
    }

BºRUNNING SCHEMA REGISTRYº
  $º$ CONFIG="etc/schema-registry/schema-registry.properties"º
  $º$ cat ${CONFIG}                                          º
  $ºlisteners=http://0.0.0.0:8081                            º
  $ºkafkastore.connection.url=localhost:2181                 º
  $ºkafkastore.topic=_schemas                                º
  $ºdebug=false                                              º

  $º$ .../bin/schema-registry-start ${CONFIG}                º
  
BºWriting Producers/Consumers with Avro Serializers/Sche.Regº

  └ start up the Sch.Reg. pointing to ZooKeeper(cluster).
  └ Configure gradle: 
    plugins {
      id "com.commercehub.gradle.plugin.avro" version "0.9.0"
    }     └─────────────┬──────────────────┘
    //    http://cloudurable.com/blog/avro/index.html
    //    transform Avro type → Java class
    //    Plugin supports:
    //    - Avro schema files (.avsc) ("Kafka")
    //    - Avro RPC IDL (.avdl)
    // $º$ gradle buildº ← generate java classesº

    group 'cloudurable'
    version '1.0-SNAPSHOT'
    apply plugin: 'java'
    sourceCompatibility = 1.8
    dependencies {
      testCompile 'junit:junit:4.11'
      compile 'org.apache.kafka:kafka-clients:0.10.2.0'  ← 
      compile "org.apache.avro:avro:1.8.1"               ← Avro lib
      compile 'io.confluent:kafka-avro-serializer:3.2.1' ← Avro Serializer
      compile 'com.squareup.okhttp3:okhttp:3.7.0'
    }
    repositories {
        jcenter()
        mavenCentral()
        maven { url "http://packages.confluent.io/maven/" }
    }
    avro {
        createSetters = false
        fieldVisibility = "PRIVATE"
    }
  └ Setup producer to use GºSchema Registryº and BºKafkaAvroSerializerº
    package com.cloudurable.kafka.schema;
    import com.cloudurable.phonebook.Employee;
    import com.cloudurable.phonebook.PhoneNum;
    import io.confluent.kafka.serializers.KafkaAvroSerializerConfig;
    import org.apache.kafka.clients.producer.KafkaProducer;
    import org.apache.kafka.clients.producer.Producer;
    import org.apache.kafka.clients.producer.ProducerConfig;
    import org.apache.kafka.clients.producer.ProducerRecord;
    import org.apache.kafka.common.serialization.LongSerializer;
    import io.confluent.kafka.serializers.KafkaAvroSerializer;
    import java.util.Properties;
    import java.util.stream.IntStream;
    
    public class AvroProducer {
        private static Producer˂Long, Employee˃ createProducer() {
            final String
              serClassName = LongSerializer.class.getName();
              KafkaAvroClN = Serializer    .class.getName();
              SCHEMA_REG_URL_CONFIG = KafkaAvroSerializerConfig.
                                      SCHEMA_REGISTRY_URL_CONFIG;
              VAL_SERI_CLASS_CONFIG = ProducerConfig.
                                      VALUE_SERIALIZER_CLASS_CONFIG
    
            final Properties props = new Properties();
            props.put(ProducerConfig.   BOOTSTRAP_SERVERS_CONFIG , "localhost:9092");
            props.put(ProducerConfig.           CLIENT_ID_CONFIG , "AvroProducer"  );
            props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG , serClassName    );
          Bºprops.put(                     VAL_SERI_CLASS_CONFIG , KafkaAvroClN    );º
          Bº//        └────────────────────────┬──────────────────────────────┘      º
          Bº//        CONFIGURE KafkaAvroSerializer.                                 º
          Gºprops.put(                     SCHEMA_REG_URL_CONFIG , º // ← Set Schema Reg.
          Gº                              "http://localhost:8081");º      URL
            return new KafkaProducer˂˃(props);
        }
    
        private final static String TOPIC = "new-employees";
    
        public static void main(String... args) {
            Producer˂Long, Employee˃ producer = createProducer();
            Employee bob = Employee.newBuilder().setAge(35)
                    .setFirstName("Bob").set...().build();
            IntStream.range(1, 100).forEach(index->{
                producer.send(new ProducerRecord˂˃(TOPIC, 1L * index, bob));
            });
            producer.flush();
            producer.close();
        }
    }
  └ Setup consumer to use GºSchema Registryº and BºKafkaAvroSerializerº
    <!-- @ma -->
    package com.cloudurable.kafka.schema;
    import com.cloudurable.phonebook.Employee;
    import io.confluent.kafka.serializers.KafkaAvroDeserializer;
    import io.confluent.kafka.serializers.KafkaAvroDeserializerConfig;
    import org.apache.kafka.clients.consumer.Consumer;
    import org.apache.kafka.clients.consumer.ConsumerConfig;
    import org.apache.kafka.clients.consumer.ConsumerRecords;
    import org.apache.kafka.clients.consumer.KafkaConsumer;
    import org.apache.kafka.common.serialization.LongDeserializer;
    import java.util.Collections;
    import java.util.Properties;
    import java.util.stream.IntStream;
    public class AvroConsumer { 
      private final static String BOOTSTRAP_SERVERS = "localhost:9092";
      private final static String TOPIC = "new-employees";
      private static Consumer<Long, Employee> createConsumer() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, BOOTSTRAP_SERVERS);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "KafkaExampleAvroConsumer");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,
                  LongDeserializer.class.getName());
        //USE Kafka Avro Deserializer.
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,
          KafkaAvroDeserializer.class.getName());
    
        //Use Specific Record or else you get Avro GenericRecord.
        props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, "true");
    
        //Schema registry location.
        props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG,
                "http://localhost:8081"); //<----- Run Schema Registry on 8081
        return new KafkaConsumer<>(props);
      }

      public static void main(String... args) {
          final Consumer<Long, Employee> consumer = createConsumer();
          consumer.subscribe(Collections.singletonList(TOPIC));
          IntStream.range(1, 100).forEach(index -> {
              final ConsumerRecords<Long, Employee> records =
                      consumer.poll(100);
              if (records.count() == 0) {
                  System.out.println("None found");
              } else records.forEach(record -> {
                  Employee employeeRecord = record.value();
                  System.out.printf("%s %d %d %s \n", record.topic(),
                          record.partition(), record.offset(), employeeRecord);
              });
          });
      }
    }

    Notice that just like with the producer, we have to tell the consumer where to find the Registry, and we have to configure the Kafka Avro Deserializer.

Configuring Schema Registry for the consumer:

//Use Kafka Avro Deserializer.

props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,

                KafkaAvroDeserializer.class.getName());  

//Use Specific Record or else you get Avro GenericRecord.

props.put(KafkaAvroDeserializerConfig.SPECIFIC_AVRO_READER_CONFIG, "true");

//Schema registry location.        props.put(KafkaAvroDeserializerConfig.SCHEMA_REGISTRY_URL_CONFIG,

                "http://localhost:8081"); //<----- Run Schema Registry on 8081

An additional step is that we have to tell it to use the generated version of the Employee object. If we did not, then it would use the Avro GenericRecord instead of our generated Employee object, which is a SpecificRecord. To learn more about using GenericRecord and generating code from Avro, read the Avro Kafka tutorial as it has examples of both.

To run the above example, you need to start up Kafka and ZooKeeper. To learn how to do this if you have not done it before, see this Kafka tutorial. Essentially, there is a startup script for Kafka and ZooKeeper like there was with the Schema Registry and there is default configuration, you pass the default configuration to the startup scripts, and Kafka is running locally on your machine.

Running ZooKeeper and Kafka:

kafka/bin/zookeeper-server-start.sh kafka/config/zookeeper.properties &

kafka/bin/kafka-server-start.sh kafka/config/server.properties

Conclusion

Confluent provides Schema Registry to manage Avro Schemas for Kafka consumers and producers. Avro provides schema migration, which is necessary for streaming and big data architectures.

Confluent uses schema compatibility checks to see if the producer’s schema and consumer’s schemas are compatible and to do schema evolution if needed. You use KafkaAvroSerializer from the producer and point to the Schema Registry. You use KafkaAvroDeserializer from the consumer and point to the Schema Registry.


https://docs.confluent.io/current/schema-registry/index.html
https://docs.confluent.io/current/schema-registry/schema_registry_tutorial.html
</pre>
</div>

<div groupv>
<span title>APIs</span>
<pre zoom labels="resource,comparative">
<span xsmall>API Summary</span>
@[https://medium.com/@stephane.maarek/the-kafka-api-battle-producer-vs-consumer-vs-kafka-connect-vs-kafka-streams-vs-ksql-ef584274c1e]
┌──────────────────────────────────────────────────────────────────┐
│   ┌───────────────┐                           ┌──────────────┐   │
│   │Schema Registry│                           │Control Center│   │
│   └───────────────┘                           └──────────────┘   │
│                                                                  │
│                                                                  │
│     Kafka 2 ←·· Replicator ┐    ┌.. REST                         │
│                            ·    ·   Proxy                        │
│  Event                     ·    ·                                │
│  Source                  ┌─v────v──┐                             │
│ (IoT,log, ─→ Producer ───→         ───→ Consumer ──→ "real time" │
│  ...)                    │ Kafka 1 │                  action     │
│                          │         │                             │
│                          │         │                             │
│                          │         │                             │
│  Data                    │         │                 Target DDBB,│
│  Source   ─→ Connect  ───→         ───→  Connect ──→ S3,HDFS, SQL│
│ (DDBB,       Source      │         │     Sink        MongoDB,... │
│  csv,...)                └─^────^──┘                             │
│                            │    │                                │
│                            │   ┌───────────┐                     │
│                        Streams │KSQL Server│                     │
│                        API     └───────────┘                     │
└──────────────────────────────────────────────────────────────────┘
  APIs            Ussage Context
  ──────────────  ───────────────────────────────────────────────────────
  Producer        Apps directly injecting data into Kafka
  ──────────────  ───────────────────────────────────────────────────────
  Connect Source  Apps inject data into CSV,DDBB,... Conn.Src API inject 
                  such data into Kafka.
  ──────────────  ───────────────────────────────────────────────────────
  Streams/KSQL    Apps consuming from Kafka topics and injecting back
                  into Kafka:
                  - KSQL   : SQL declarative syntax
                  - Streams: "Complex logic" in programmatic java/...
  ──────────────  ───────────────────────────────────────────────────────
  Consumer        Apps consuming a stream,  and perform "real-time" action
                  on it (e.g. send email...)
  ──────────────  ───────────────────────────────────────────────────────
  Connect Sink    Read a stream and store it into a target store 
  ──────────────  ───────────────────────────────────────────────────────

Producer API:
└ Bºextremely simple to useº: send data and Wait in callback.
└ RºLot of custom code for ETL alike appsº:
    - How to track the source offsets? 
      (how to properly resume your producer in case of errors)
    - How to distribute load for your ETL across many producers?
    ( Kafka Connect Source API recommended in those cases)

Connect Source API:
└ High level API built on top of the Producer API for:
  -  producer tasks Bºdistribution for parallel processingº
  -Bºeasy mechanism to resume producersº
└Bº"Lot" of available connectorsº out of the box (zero-code).

Consumer API:
└ BºKISS APIº: It uses Consumer Groups. Topics can be consumed in parallel.
             RºCare must be put in offset management and commits, as wellº
             Rºas rebalances and idempotence constraints, they’re really º
             Rºeasy to write.                                            º
             BºPerfect for stateless workloadsº (notifications,...)
└ RºLot of custom code for ETL alike appsº:

Connect Sink API:
└  built on top of the consumer API.
└Bº"Lot" of available connectorsº out of the box (zero-code).

Streams API:
└ Support for Java and Scala.
└ It enables to write either:
  -BºHigh Level DSLº(ApacheºSpark alikeº)
  -  Low Level API  (ApacheºStorm alikeº).
└ Complicated Coding is still required, but producers/consumers handling
  completely is hidden.
Bºfocussing on stream logicº
└BºSupport for 'joins', 'aggregations', 'exactly-once' semmantics.º
└Rºunit test can be difficultº (test-utils library to the rescue).
└ Use of state stores, when backed up by Kafka topics, will force
Rºprocessing a lot more messagesº, butBºadding support for resilient appsº.


KSQL :
└ ºwrapper on top of Kafka Streamsº.
└  It abstract away Stream coding complexity.
└RºNot support for complex transformations, "exploding" arrays,...º
   (as of 2019-11, gaps can be filled)
</pre>
<pre zoom labels="TODO">
<span xsmall>Producer</span>
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/clients/producer/KafkaProducer.html]
- send streams of data to topics in the cluster
<hr/>
<span xsmall>Config</span>
See full list @ @[http://kafka.apache.org/documentation/#producerconfigs]
-ºcleanup.policyº      : "delete"* or "compact" retention policy for old log segments. 
-ºcompression.typeº    : 'gzip', 'snappy', lz4, 'uncompressed' 
-ºindex.interval.bytesº: how frequently Kafka adds an index entry to it's offset index.
-ºleader.replication.throttled.replicasº:  Ex:
                         [PartitionId]º:[BrokerId],[PartitionId]:[BrokerId]:... or
                         wildcard '*' can be used to throttle all replicas for this topic.                            
-ºmax.message.bytesº   : largest record batch size allowed
-ºmessage.format.versionº: valid ApiVersion ( 0.8.2, 0.9.0.0, 0.10.0, ...)
-ºmessage.timestamp º  : max dif allowed between the timestamp when broker receives
 º.difference.max.msº    a message and timestamp specified in message.
                         
-ºmessage.timestamp.typeº: "CreateTime"|"LogAppendTime"
-ºmin.insync.replicasº : all|-1|"N": number of replicas that must acknowledge a write
                         for it to be considered  successful.
-ºretention.msº        : time we will retain a log before old log segments are discarded.
                       BºSLA on how soon consumers must read their dataº.
</pre>


<pre zoom labels="TODO">
<span xsmall>Consumer</span>
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html]
<hr/>
<span xsmall>Config</span>
@[http://kafka.apache.org/documentation/#consumerconfigs]
@[http://kafka.apache.org/documentation/#newconsumerconfigs]
@[http://kafka.apache.org/documentation/#oldconsumerconfigs]
</pre>

<pre zoom labels="TODO">
<span xsmall>Connect</span>
@[http://kafka.apache.org/11/javadoc/index.html?overview-summary.html]
(<a TODO href="http://kafka.apache.org/documentation.html#connect">more info</a>)
- allows reusable producers or consumers that connect Kafka
  topics to existing applications or data systems.
<hr/>
<span xsmall>Connectors List</span>
@[https://docs.confluent.io/current/connect/connectors.html]
@[https://docs.confluent.io/current/connect/managing/connectors.html]
- Kafka connectors@github
  @[https://github.com/search?q=kafka+connector]
<hr/>
-BºHTTP Sinkº
-BºFileStreamºs (Development and Testing)
-BºGitHub Sourceº
-BºJDBC (Source and Sink)º
-  PostgresSQL Source (Debezium)
-  SQL Server Source (Debezium)
-BºSyslog Sourceº
- AWS|Azure|GCD|Salesforce "*"
- ...
<hr/>
<span xsmall>Config</span>
@[http://kafka.apache.org/documentation/#connectconfigs]
</pre>

<pre zoom labels="api,adminClient,TODO">
<span xsmall>AdminClient</span>
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/clients/admin/AdminClient.html]

- administrative client for Kafka, which supports managing and inspecting
  topics, brokers, configurations and ACLs.
<hr/>
<span xsmall>Config</span>
@[http://kafka.apache.org/documentation/#adminclientconfigs]
</pre>

<pre zoom labels="streams,snapshot,TODO">
<span xsmall>Streams</span>
@[http://kafka.apache.org/25/documentation/streams/]
See also:
@[http://kafka.apache.org/11/javadoc/index.html?org/apache/kafka/streams/KafkaStreams.html]
- Built on top o producer/consumer API.

- simple lightweight embedableBºclient libraryº, with support for
  real-time querying of app state with low level Processor API
  primitives plus high-level DSL.

- transparent load balancing of multiple instances of an application.
  using Kafka partitioning model to horizontally scale processing
  while maintaining strong ordering guarantees.

- Supports fault-tolerant local state, which enables very fast and 
 efficient stateful operations likeºwindowed joins and aggregationsº.

- Supports exactly-once processing semantics when there is a 
  client|Kafka failure.

- Employs one-record-at-a-time processing to achieve millisecond 
  processing latency, and supports event-time based windowing 
  operations with out-of-order arrival of records.

  import org.apache.kafka.common.serialization.Serdes;
  import org.apache.kafka.common.utils.Bytes;
  import org.apache.kafka.streams.KafkaStreams;
  import org.apache.kafka.streams.StreamsBuilder;
  import org.apache.kafka.streams.StreamsConfig;
  import org.apache.kafka.streams.kstream.KStream;
  import org.apache.kafka.streams.kstream.KTable;
  import org.apache.kafka.streams.kstream.Materialized;
  import org.apache.kafka.streams.kstream.Produced;
  import org.apache.kafka.streams.state.KeyValueStore;
   
  import java.util.Arrays;
  import java.util.Properties;
   
  public class WordCountApplication {
   
    public static void main(final String[] args) throws Exception {
      final Properties props = new Properties();
      props.put(StreamsConfig.APPLICATION_ID_CONFIG    ,
                "wordcount-app");
      props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG ,
                "kafka-broker1:9092");
      props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, 
                Serdes.String().getClass());
      props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG,
                Serdes.String().getClass());
  
      final StreamsBuilder builder = new StreamsBuilder();
      KStream˂String, String˃ txtLineStream 
        = builder.stream("TextLinesTopic");
      KTable˂String, Long˃ wordCounts = txtLineStream
          .flatMapValues(
                          textLine -˃
                          Arrays.asList(
                             textLine.toLowerCase().split("\\W+")
                          )
                        )
          .groupBy      ( (key, word) -˃ word )
          .count        ( Materialized.
                          ˂  String, Long, KeyValueStore˂Bytes, byte[]˃ ˃
                          as("counts-store"));
      wordCounts.
        toStream().
        to("WordsWithCountsTopic", 
           Produced.with(Serdes.String(), Serdes.Long()));
  
      KafkaStreams streams = new KafkaStreams(builder.build(), props);
      streams.start();
    }
  }

BºKafka Streams CORE CONCEPTSº
@[http://kafka.apache.org/25/documentation/streams/core-concepts]

  └ºStreamº   : graph of stream processors (nodes) that
   ºProcessorº  connected by streams (edges).
   ºtopologyº   

  └ºStreamº   : It represents an data set unbounded in size / time, 
                ordered, replayable and fault-tolerant inmmutable 
                record set.
 
  └ºStreamº   : node processing an step to transform data
   ºprocessorº  in streams.
                - It receives one input record at a time from
                  upstream processors and produces 1+ output records.

                - Special processors:
                  - Source Processor: NO upstream processors,
                    just one or multiple Kafka topics as input.

                  - Sink Processor: no down-stream processors. 
                    Outpus goes to Kafka topic/external system.

  └two ways to define the stream processing topology:
   - Streams DSL  : map, filter, join and aggregations
   - Processor API: (low-level) Custom processors. It also allows
                    to interact with state stores.

  └ºTime modelº: operations like windowing are defined based 
                 on time boundaries.  Common notions of time in
                streams are:
    - Event      time: 
    - Ingestion  time: time it's stored into a topic partition.
    - Processing time: It may be (milli)seconds for real time or
                       hours for batch time, after event time.
                       real event time.

    - Kafka 0.10.x+ automatically embeds (event or ingestion) time.
      Event of ingestion choose can be done at Kafka or topic level 
      in configuration.
    - Kafka Streams assigns a TS to every data record via the 
      TimestampExtractor interface allowing to describe the "progress"
      of a stream with regards to time and are leveraged by 
      time-dependent operations such as window operations. 

    - time only "advances" when a new record arrives at the 
      processor.  Concrete implementations of the TimestampExtractor
      interface will provide different semantics to the
      stream time definition.

    - Finally, Kafka Streams sinks will also assign timestamps
      in a way that depends on the context:
      - When output is generated from some input record,
        for example, context.forward(), TS is  inherited from input.
      - When new output record is generated via periodic
        functions such as Punctuator#punctuate(), TS is defined
        as current node internal time (context.timestamp()) .
      - For aggregations, result update record TS is the max.
        TS of all input records.
      NOTE: default behavior can be changed in the Processor API
            by assigning timestamps to output records explicitly 
            in "#forward()".

 
  └ºAggregationº: 
   ºOperation  º  
          INPUT                      OUTPUT
          --------                   ------
          KStream   → Aggregation →  KTable
       or KTable         
          ^             ^              ^
          DSL         Ex: count/sum  DSL object:
                                     - new value is considered
                                       to overwrite the old value
                                      ºwith the same keyºin next
                                       steps.

  └ºWindowingº:  - trackedºper record keyº.
                 - Available in ºStreams DSLº. 
                 - window.ºgrace_periodº controls 
                  ºhow long Streams clients will wait forº
                  ºout-of-order data records.º
                 - Records arriving "later" are discarded.
                   "late" == record.timestamp dictates it belongs 
                             to a window, but current stream time
                             is greater than the end of the window 
                             plus the grace period.

  └ºStates   º:  - Needed by some streams. 
                 -Bºstate storesº in Stream APIs allows apps to
                   store and query data, needed by stateful operations.
                 - Every task in Kafka Streams embeds 1+ state stores
                   that can be accessed via APIs to store and query
                   data required for processing. They can be:
                   -ºpersistent key-value storeº:
                   -ºIn-memory hashmapº
                   - "another convenient data structure".
                 - Kafka Streams offers fault-tolerance and automatic
                   recovery for local state-stores.
                 - directºread-only queriesºof the state stores
                   is provided to methods, threads, processes or
                   applications external to the stream processing
                   app through BºInteractive Queriesº, exposing the
                   underlying implementation of state-store read 
                   methods.

  └ºProcessingº: - at-least-once delivery 
   ºGuaranteesº    (processing.guarantee=exactly_once  in config)
                 - exactly-once processing semantics (Kafka 0.11+)
                                                      ^^^^^^^^^^
                Kafka 0.11.0+ allows producers to send messages to
                different topic partitions in transactional and
                idempotent manner.
                More specifically,Streams client APIguarantees that 
                for any record read from the source Kafka topics,
                its processing results will be reflected exactly once
                in the output Kafka topic as well as in the 
                state stores for stateful operations.
                (KIP-129 lecture recomended)


  └ºOut-of-Orderº:
   ºHandlingº
      - Within topic-partition:
        - records with larger timestamps but smaller offsets
          are processed earlier.
      - Within stream task processing "N" topic-partitions:
        - If app is Rºnot configured to wait for all partitionsº
        Rºto contain some buffered dataº and pick from the
          partition with the smallest timestamp to process
          the next record, timestamps may be smaller in 
          following records for different partitions.
          "FIX": Allows applications to wait for longer time
                  while bookkeeping their states during the wait time.
                  i.e. making trade-off decisions between latency,
                 cost, and correctness.
                 In particular, increase windows grace time.
           Rº As for Joins some "out-of-order" data cannot be handled
              by increasing on latency and cost in Streams yet:

<hr/>
<span xsmall>Config</span>
See details: @[http://kafka.apache.org/documentation/#streamsconfigs]
-ºCore config:º
  -ºapplication.id    º: string unique within the Kafka cluster
  -ºbootstrap.servers º: host1:port1,host2:port2
                         ^^^^^^^^^^^^^^^^^^^^^^^
                         No need to add all host. Just a few ones to start sync
  -ºreplication.factorº: int, Default:1
  -ºstate.dir         º: string, Default: /tmp/kafka-streams
-ºOther params:º
  - cache.max.bytes.buffering: long, def: 10485760 (max bytes for buffering ºacross all threadsº)
  - client.id          : ID prefix string used for the client IDs of internal consumer,
                         producer and restore-consumer, with pattern '-StreamThread--'.
                         Default: ""
  - default.deserialization.exception.handler  
  - default.key  .serde                   : Default serializer / deserializer class 
  - default.value.serde
  - default.production.exception.handler: Exception handling class 
  - default.timestamp.extractor
  - max.task.idle.ms : long, Maximum amount of time a stream task will stay idle 
                       when not all of its partition buffers contain records, 
                       to avoid potential out-of-order record processing 
                       across multiple input streams. 
  - num.standby.replicas: int (default to 0)
  - num.stream.threads : 
  - processing.guarantee: at_least_once (default) | exactly_once.
                                                    ^^^^^^^^^^^^
                         - It requires 3+ brokers  in production
                         - for development it can be changed by
                           tunning broker setting 
                           - transaction.state.log.replication.factor
                           - transaction.state.log.min.isr.
  - security.protocol   : PLAINTEXT, SSL, SASL_PLAINTEXT, SASL_SSL.
  - topology.optimization: [none*, all] Set wheher Kafka Streams should optimize the topology
  - application.server  : Default: "", endpoint used for state store discovery and
                          interactive queries on this KafkaStreams instance.
  - buffered.records.per.partition: int, default to 1000 
  - built.in.metrics.version
  - commit.interval.ms   : frequency to save the position of the processor.
                          (default to 100 for exactly_once or 30000 otherwise).
  - connections.max.idle.ms: Default:    540000
  - metadata.max.age.ms: 
  - metric.reporters : 
  - metrics.num.samples: 
  - metrics.recording.level: [INFO, DEBUG]
  - metrics.sample.window.ms
  - poll.ms  : Amount of time in milliseconds to block waiting for input.
  - receive.buffer.bytes: int, def: 32768, size of the TCP receive buffer (SO_RCVBUF) to use
                          when reading  data. Set to -1 to use OS default.
  - send.buffer.bytes   : in, def: 131072, size of the TCP send   buffer (SO_SNDBUF ) to use
                           when sending data. Set to -1 to use OS default.
  - reconnect.backoff.max.ms: 
  - reconnect.backoff.ms
  - request.timeout.ms
  - retries             : Setting a value greater than zero will cause the client to 
                          resend any request that fails with a potentially transient error.
  - retry.backoff.ms    :
  - rocksdb.config.setter:
  - state.cleanup.delay.ms:
  - upgrade.from : 
  - windowstore.changelog.additional.retention.ms:
<hr/>
<span xsmall>Examples</span>
@[https://github.com/confluentinc/kafka-streams-examples]
  - Examples: Runnable Applications
  - Examples: Unit Tests
  - Examples: Integration Tests
  - Docker Example: Kafka Music demo application
    Example ºdocker-compose.ymlº:
             └───────┬────────┘
             Services launched:
             zookeeper:
             kafka:
             schema-registry:
             kafka-create-topics:
                 ...  kafka-topics --create --topic play-events ...
                 ...  kafka-topics --create --topic song-feed   ... 
      
             kafka-music-data-generator:  ← producer
             kafka-music-application:     ← consumer
    
  - Examples: Event Streaming Platform
</pre>

<pre zoom labels="TODO">
<span bgorange xsmall>KSQL(ksqlDB)</span>
@[https://www.confluent.io/]
@[https://www.confluent.io/blog/ksql-open-source-streaming-sql-for-apache-kafka/]

See also:
- Pull Queries and Connector Management Added to ksqlDB (KSQL) Event Streaming Database for Kafka (2019-12)
@[https://www.infoq.com/news/2019/12/ksql-ksqldb-streaming-database/]
  - pull queries to allow for data to be read at a specific point in 
    time using a SQL syntax, and Connector management that enables direct 
    control and execution of connectors built to work with Kafka Connect. 
  - Until now KSQL has only been able to query continuous streams 
    ("push queries"). Now it can also read current state of a materialized 
    view using pull queries:
    These new queries can run with predictable low latency since the
    materialized views are updated incrementally as soon as new
    messages arrive.
  - With the new connector management and its built-in support for a 
    range of connectors, it’s now possible to directly control and 
    execute these connectors with ksqlDB, instead of creating separate 
    solutions using Kafka, Connect, and KSQL to connect to external data 
    sources. 
    The motive for this feature is that the development team 
    believes building applications around event streams Rºwas too complexº. 
  BºInstead, they want to achieve the same simplicity as when buildingº
  Bºapplications using relational databases.                          º

  - Internally, ksqlDB architecture is based on a distributed commit 
    log used to synchronize the data across nodes. To manage state, 
    RocksDB is used to provide a local queryable storage on disk. A 
    commit log is used to update the state in sequences and to provide 
    failover across instances for high availability.
</pre>
</div>

<div groupv>
<span title>Developer Tools</span>
<pre zoom labels="tool,low_code,docker,">
<span XSMALl>Docker Img For Developers</span>
@[https://github.com/lensesio/fast-data-dev]
- Apache Kafka docker image for developers; with Lenses (lensesio/box) 
  or Lenses.io's open source UI tools (lensesio/fast-data-dev). Have a 
  full fledged Kafka installation up and running in seconds and top it 
  off with a modern streaming platform (only for kafka-lenses-dev), 
  intuitive UIs and extra goodies. Also includes Kafka Connect, Schema 
  Registry, Lenses.io's Stream Reactor 25+ Connectors and more.

  Ex SystemD integration file:
  /etc/systemd/system/kafkaDev.service
  | #!/bin/bash
  | 
  | # Visit http://localhost:3030 to get into the fast-data-dev environment
  | 
  | [Unit]
  | Description=Kafka Lensesio
  | After=docker.service
  | Wants=network-online.target docker.socket
  | Requires=docker.socket
  | 
  | [Service]
  | Restart=always
  | # Create container if it doesn't exists with container inspect
  | ExecStartPre=/bin/bash -c "/usr/bin/docker container inspect lensesio/fast-data-dev 2> /dev/null || /usr/bin/docker run -d --name kafkaDev --net=host -v /var/backups/DOCKER_VOLUMES_HOME/kafkaDev:/data lensesio/fast-data-dev" 
  | ExecStart=/usr/bin/docker start -a    kafkaDev 
  | ExecStop=/usr/bin/docker   stop -t 20 kafkaDev 
  | 
  | [Install]
  | WantedBy=multi-user.target
</pre>

<span title>DevOps</span>
<pre zoom labels="TODO,ansible">
<span xsmall>Ansible Install</span>
@[https://github.com/confluentinc/cp-ansible]
@[https://docs.confluent.io/current/installation/cp-ansible/index.html]
- Installs Confluent Platform packages.
  - ZooKeeper
  - Kafka
  - Schema Registry
  - REST Proxy
  - Confluent Control Center
  - Kafka Connect (distributed mode)
  - KSQL Server

- systemd Integration

- Configuration options for:
   plaintext, SSL, SASL_SSL, and Kerberos.



</pre>

<pre zoom>
<span xsmall>pom.xml</span>

ºProducer, consumer, AdminClient APIsº   ºStreams APIº
˂dependency˃                             ˂dependency˃
  ˂groupId˃org.apache.kafka˂/groupId˃      ˂groupId˃org.apache.kafka˂/groupId˃
  ˂artifactId˃kafka-clients˂/artifactId˃   ˂artifactId˃kafka-streams˂/artifactId˃
  ˂version˃1.1.0˂/version˃                 ˂version˃1.1.0˂/version˃
˂/dependency˃                            ˂/dependency˃
</pre>
<pre zoom>
<span xsmall>Quick start</span>
@[http://kafka.apache.org/documentation.html#quickstart]
BºPRE-SETUPº
  Download tarball from mirror @[https://kafka.apache.org/downloads],
  then untar like:
  $º$ tar -xzf - kafka_2.13-2.5.0.tgz ; cd kafka_2.13-2.5.0º

  - Start the server
  $º$ ~ bin/zookeeper-server-start.sh \                    º ← Start Zookeeper
  $º  config/zookeeper.properties 1˃zookeeper.log 2˃⅋1 ⅋   º

  $º$ ~ bin/kafka-server-start.sh  \                       º ← Start Kafka Server
  $º  config/server.properties 1˃kafka.log  2˃⅋1 ⅋ \       º

  $º$ bin/kafka-topics.sh --create --zookeeper \           º ← Create BºTESTº topic
  $º  localhost:2181  --replication-factor 1 \             º (Alt.brokers can be
  $º  --partitions 1 --topic TEST                          º  configured to  auto-create 
                              └──┘                            them when publishing to
                                                              non-existent ones)

  $º$ bin/kafka-topics.sh --list --zookeeper localhost:2181º ← Check topic
  $º$ TEST                                                 º ← Expected output           
      └──┘

  $º$ bin/kafka-console-producer.sh --broker-list          º ← Send some messages
  $º  localhost:9092 --topic TEST                          º
  $ºThis is a message                                      º
  $ºThis is another message                                º
  $ºCtrl+V                                                 º

  $º$ bin/kafka-console-consumer.sh --bootstrap-server \   º ← Start a consumer
  $ºlocalhost:9092 --topic Bºtestº --from-beginning        º
  $ºThis is a message                                      º
  $ºThis is another message                                º


  ********************************
  * CREATING A 3 BROKERS CLUSTER *
  ********************************
  $º$ cp config/server.properties config/server-1.propertiesº ← Add 2 new broker configs.
  $º$ cp config/server.properties config/server-2.propertiesº
                                  └───────────┬────────────┘ 
               ┌──────────────────────────────┤
   ┌───────────┴────────────┐      ┌──────────┴─────────────┐  
  ºconfig/server-1.properties:º   ºconfig/server-2.properties:º
   broker.id=1                     broker.id=2                 ← unique id 
   listeners=PLAINTEXT://:9093     listeners=PLAINTEXT://:9094
   log.dir=/tmp/kafka-logs-1       log.dir=/tmp/kafka-logs-2   ← avoid overwrite


  $º$ bin/kafka-server-start.sh config/server-1.properties ...º ← Start 2nd cluser
  $º$ bin/kafka-server-start.sh config/server-2.properties ...º ← Start 3rd cluser

  $º$ bin/kafka-topics.sh --create --zookeeper localhost:2181\º ← Create new topic with
  $º  --replication-factor 3 --partitions 1                   º Bºreplication factor of 3º
  $º  --topic  topic02                                        º

  $º$ bin/kafka-topics.sh --describe \                        º ← Check know which broker
  $º  --zookeeper localhost:2181 --topic topic02              º   is doing what
   (output will be similar to)
   Topic: topic02  PartitionCount:1  ReplicationFactor:3 Configs:         ← summary 
       Topic: topic02 Partition:0 Leader:1  Replicas: 1,2,0 Isr: 1,2,0 ← Partition 0
                                                            └┬┘
                                            set of "in-sync" replicas. 
                                           (subset of "replicas" currently
                                            alive and in sync with "leader")


  *****************************************
  * Using Kafka Connect to import/export  *
  * data using simple connectors          *
  *****************************************

BºPRE-SETUPº
  $º$ echo -e "foo\nbar" &gt; test.txt       º ← Prepare (input)test data

  - SETUP source/sink Connectors:
    config/connect-file-srcs.properties ← unique_connector_id, connector class,
    config/connect-file-sink.properties   ...

  $º$ bin/connect-standalone.sh \            º ← Start Bºtwo connectorsº running in 
  $º  \                                      º   standalone mode (dedicated process)
  $º  config/connect-standalone.properties \ º ← 1st param is common Kafka-Connect config 
  $º  \                                      º   (brokers, serialization format ,...)
  $º  config/connect-file-srcs.properties  \ º 
  $º  config/connect-file-sink.properties    º 
    └─────────────────┬────────────────────┘
    examples in kafka distribution set the "pipeline" like:
    "test.txt" → connect-test  → sink connector → "test.sink.txt"


</pre>

<span title>Configuration</span>

<pre zoom labels="devops,TODO">
<span TODO xsmall>Broker Config</span>
Broker config: @[http://kafka.apache.org/documentation/#configuration]
  Main params:
broker.id
log.dirs
zookeeper.connect
</pre>

<pre zoom labels="TODO">
<span xsmall>Topics Conf</span>
Topics config: @[http://kafka.apache.org/documentation/#topicconfigs"
</pre>


<pre zoom labels="devops,performance,TODO">
<span xsmall>Compaction</span>
http://kafka.apache.org/documentation.html#compaction
</pre>
</div>

<div groupv>
<span title>Kafka+Kubernetes</span>
<pre zoom labels="TODO">
<span xsmall>Strimzi (Images+Operators)</span>
@[https://developers.redhat.com/blog/2019/06/06/accessing-apache-kafka-in-strimzi-part-1-introduction/]
@[https://developers.redhat.com/blog/2019/06/07/accessing-apache-kafka-in-strimzi-part-2-node-ports/]
@[https://developers.redhat.com/blog/2019/06/10/accessing-apache-kafka-in-strimzi-part-3-red-hat-openshift-routes/]
@[https://developers.redhat.com/blog/2019/06/11/accessing-apache-kafka-in-strimzi-part-4-load-balancers/]
@[https://developers.redhat.com/blog/2019/06/12/accessing-apache-kafka-in-strimzi-part-5-ingress/]

- Strimzi: open source project that provides container images and 
Bºoperatorsº for running Apache Kafka on Kubernetes and Red Hat 
  OpenShift. Scalability is one of the flagship features of Apache 
  Kafka. It is achieved by partitioning the data and distributing them 
  across multiple brokers. Such data sharding has also a big impact on 
  how Kafka clients connect to the brokers. This is especially visible 
  when Kafka is running within a platform like Kubernetes but is 
  accessed from outside of that platform.
</pre>
</div>

<br/>

<div group>
    <span title>Unordered</span><br/>
<pre zoom labels="architecture,TODO">
<span xsmall>Kafka Design</span>
For details about the Kafka's commit log storage and replication design:
https://kafka.apache.org/documentation/#design
</pre>

<pre zoom labels="security,TODO">
<span xsmall>Security</span>
  <a href="https://kafka.apache.org/documentation/#security">Security Docs</a>
</pre>

<pre zoom labels="devops,monitoring,TODO">
<span xsmall>Burrow Monit</span>
@[https://dzone.com/articles/kafka-monitoring-with-burrow]
</pre>

<pre zoom labels="TODO">
<span xsmall>Best Pracites</span>
@[https://www.infoq.com/articles/apache-kafka-best-practices-to-optimize-your-deployment]
</pre>

<pre zoom labels="TODO,ha">
<span xsmall>Mirror Maker (geo-replica)</span>
@[https://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330]
fka's mirroring feature makes it possible to maintain a replica of an 
existing Kafka cluster. The following diagram shows how to use the 
MirrorMaker tool to mirror a source Kafka cluster into a target 
(mirror) Kafka cluster. The tool uses a Kafka consumer to consume 
messages from the source cluster, and re-publishes those messages to 
the local (target) cluster using an embedded Kafka producer.
</pre>


<pre zoom labels="TODO">
<span xsmall>Akka+Kotlin:Data Pipes</span>
@[https://www.kotlindevelopment.com/data-pipelines-kotlin-akka-kafka/]
- Objective: 
  Create a "GitHub monitor" that build an analytics component that
  polls one of your services (GitHub), writes data into a message queue
  for later analysis, then, after post-processing, updates statistics in
  a SQL database.

- Tooling: Akka Streams + Alpakka connector collection

- "Architecture":
  1) polls GitHub Event API for kotlin activity
  2) writes all events into a Kafka topic for later use
  3) reads events from Kafka and filters out PushEvents
  4) updates a Postgres database with:
     - who pushed changes
     - when
     - into which repository.

- Akka summary: data is moving from Sources to Sinks.
  (Observable and Sink in RxJava)

ºENTRY POINTº
(standard main function)
  fun main(vararg args: String) {
    val system = ActorSystem.create()                          // "boilerplate" for using Akka and Akka Streams
    val materializer = ActorMaterializer.create(system)
    val gitHubClient = GitHubClient(system, materializer)      // instance used to poll the GitHub events API
    val eventsProducer = EventsProducer(system, materializer)  // instance used to write events into Kafka
    val eventsConsumer = EventsConsumer(system)                // instance used to read events from Kafka
    val pushEventProcessor = PushEventProcessor(materializer)  // instance used to filter PushEvents and update the database
                                                               
    eventsProducer.write(gitHubClient.events())                // put things in motion.
    pushEventProcessor.run(eventsConsumer.read())
  }

  Each time we receive a response from GitHub, we parse it and send individual events downstream.
  fun events(): Source˂JsonNode, NotUsed˃ =
    poll().flatMapConcat { response -˃
      response.nodesOpt
        .map { nodes -˃ Source.from(nodes) }
        .orElse(Source.empty())
    }

ºEventsProducer and EventsConsumerº
(the "power" of Akka Streams and Alpakka)

- Akka-Streams-Kafka greatly reduces the amount of code
  that we have to write for integrating with Kafka.
  Publishing events into a Kafka topic look like:

  fun write(events: Source˂JsonNode, NotUsed˃)
      : CompletionStage˂Done˃
        =  events.map {                                        // ← maps GitHub event to(Kafka)ProducerRecord 
             node -˃ ProducerRecord˂ByteArray, String˃
                        ("kotlin-events",
                         objectMapper.writeValueAsString(node) // ← serialize JsonNode as a String
                        ) 
      }.runWith(                        // ← connects Source Sink  
          Producer.plainSink(settings), materializer)                                         
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        - defined@Akka Streams Kafka
        - takes care of communicating with Kafka

- The other way around, reading from Kafka, is also super simple.

  fun read(): Source˂JsonNode, NotUsed˃ =
    Consumer.plainSource(
        settings,
        Subscriptions.assignmentWithOffset(
            TopicPartition("kotlin-events", 0), 0L))
       .map { record -˃ objectMapper.readTree(record.value()) }
       .mapMaterializedValue { NotUsed.getInstance() }

  At this point, we have a copy of GitHub's events feed for github.com/Kotlin stored in Kafka,
so we can time travel and run different analytics jobs on our local dataset.

ºPushEventProcessorº
 we want to filter out PushEvents from the stream
 and update a Postgres database with the results.
 - Alpakka Slick (JDBC) Connector is used to connect to PostgreSQL.
   fun createTableIfNotExists(): Source˂Int, NotUsed˃ {
     val ddl =
       """
         |CREATE TABLE IF NOT EXISTS kotlin_push_events(
         |  id         BIGINT    NOT NULL,
         |  name       VARCHAR   NOT NULL,
         |  timestamp  TIMESTAMP NOT NULL,
         |  repository VARCHAR   NOT NULL,
         |  branch     VARCHAR   NOT NULL,
         |  commits    INTEGER   NOT NULL
         |);
         |CREATE UNIQUE INDEX IF NOT EXISTS id_index ON kotlin_push_events (id);
       """.trimMargin()
     return Slick.source(session, ddl, { _ -˃ 0 })
   }

 - Similarly, the function to update the database looks like this.

   fun Source˂PushEvent, NotUsed˃.updateDatabase() :
       CompletionStage˂Done˃ =
           createTableIfNotExists().flatMapConcat { this }
       .runWith(Slick.sink˂PushEvent˃(session, 20, { event -˃
         """
           |INSERT INTO kotlin_push_events
           |(id, name, timestamp, repository, branch, commits)
           |VALUES (
           |  ${event.id},
           |  '${event.actor.login}',
           |  '${Timestamp.valueOf(event.created_at)}',
           |  '${event.repo.name}',
           |  '${event.payload.ref}',
           |  ${event.payload.distinct_size}
           |)
           |ON CONFLICT DO NOTHING
         """.trimMargin()
       }), materializer)
 
   We are almost done, what's left is filtering and mapping from JsonNode to
   PushEvent and composing the methods together.

   fun Source˂JsonNode, NotUsed˃.filterPushEvents(): Source˂PushEvent, NotUsed˃ =
     filter { node -˃ node["type"].asText() == "PushEvent" }
       .map { node -˃ objectMapper.convertValue(node, PushEvent::class.java) }

   And finally, all the functions composed together look like this.
   fun run(events: Source˂JsonNode, NotUsed˃): CompletionStage˂Done˃ =
     events
       .filterPushEvents()
       .updateDatabase()

  This is why we've used the extension methods above, so we can describe 
  transformations like this, simply chained together. 
  That's it, after running the app for a while (gradle app:run) we can see 
  the activities around different Kotlin repositories. 


You can find the complete source code on GitHub.

    A very nice property of using Akka Streams and Alpakka is that it makes 
  really easy to migrate/reuse your code, e.g. in case you want to store data 
  in Cassandra later on instead of Postgres. All you would have to do is define 
  a different Sink with CassandraSink.create. Or if GitHub events would be 
  dumped in a file located in AWS S3 instead of published to Kafka, all you 
  would have to do is create a Source with S3Client.download(bucket, key). The 
  current list of available connectors is located here, and the list is growing.
</pre>

<pre zoom labels="TODO,low_code,">
<span xsmall>Pixy</span>
@[https://github.com/mailgun/kafka-pixy]
Kafka-Pixy is a dual API (gRPC and REST) proxy for Kafka with 
automatic consumer group control. It is designed to hide the 
complexity of the Kafka client protocol and provide a stupid simple 
API that is trivial to implement in any language.
</pre>

<pre zoom labels="pulsar,comparative,TODO">
<span xsmall>Pulsar vs Kafka</span>
- Pulsar is a younger project Bºinspired and informed by Kafkaº. 
  Kafka on the other side has a bigger community. 

BºPulsas "PRO"s over Kafkaº
Bº========================º
@[https://kafkaesque.io/7-reasons-we-choose-apache-pulsar-over-apache-kafka/]
@[https://kesque.com/5-more-reasons-to-choose-apache-pulsar-over-kafka/]

1. Streaming and queuing Come together:
   Pulsar supports standard message queuing patterns, such as
   competing consumers, fail-over subscriptions, and easy message 
   fan out keeping track of the client read position in the topic 
   and stores that information in its high-performance distributed 
   ledger, Apache BookKeeper, handling many of the use cases of a 
   traditional queuing system, like RabbitMQ.
 
2. Simpler ussage:
   If you don't need partition you don't have to worry about them.
   "If you just need a topic, then use a topic". Do not worry about
   how many consumers the topic might have. 
   Pulsar subscriptions allow you to add as many consumers as you want 
   on a topic with Pulsar keeping track of it all. If your consuming 
   application can’t keep up, you just use a shared subscription to 
   distribute the load between multiple consumers.
   Pulsar hasºpartitioned topicsºif you need them, but 
  ºonly if you need themº.

3. Fitting a log on a single server becomes a challenge (Disk full,
   remote copy o large logs can take a long time.
   More info at "Adding a New Broker Results in Terrible Performance"
 @[https://www.confluent.io/blog/stories-front-lessons-learned-supporting-apache-kafka/]
   Apache Pulsar breaks logs into segments and distributes them across
   multiple servers while the data is being written by using BookKeeper
   as its storage layer. 
   This means that the Bºlog is never stored on a single serverº, so a
   single server is never a bottleneck. Failure scenarios are easier to
   deal with and Bºscaling out is a snap: Just add another server.º
 BºNo rebalancing needed.º

4. Stateless Brokers:
   In Kafka each broker contains the complete log for each of
   its partitions. If load gets too high, you can't simply add
   another broker. Brokers must synchronize state from other 
   brokers that contain replicas of its partitions.
   In Pulsar brokers accept data from producers and send data
   to consumers, but the data is stored in Apache BookKeeper.
   If load gets high, just add another broker.
   It starts up quickly and gets to work right away.

5. Geo-replication is a first-class feature in Pulsar.
   (vs proprietary add-on).
 BºConfiguring it is easy and it just works. No PhD needed.º
   
6. Consistently Faster
 BºPulsar delivers higher throughput along with lower and moreº
 Bºconsistent latency.º

7. All Apache Open Source
   input and output connectors (Pulsar IO), SQL-based topic queries 
   (Pulsar SQL), schema registry,...
   (vs Kafka open-source features controlled by a commercial entity)

8. Pulsar can have multiple tenants and those tenants can have 
   multiple namespaces to keep things all organized. Add to that
   access controls, quotas, and rate-limiting for each namespace 
   and you can imagine a future where we can all get along using
   just this one cluster.
   (WiP or Kafka in KIP-37).

9. Replication
   You want to make sure your messages never get lost. In Kakfa
   you configure 2 or 3 replicas of each message in case 
   something goes wrong.
   In Kafka the leader stores the message and the followers make
   a copy of it. Once enough followers acknowledge they’ve got it,
   "Kafka is happy".
   Pulsar uses a quorum model: It sends the message out to a 
   bunch of nodes, and once enough of them acknowledge they've
   got it, "Pulsar is happy". Majority always wins, and all votes 
   are equal giving more consistent latency behavior over time.
 @[https://kafkaesque.io/performance-comparison-between-apache-pulsar-and-kafka-latency/]
   (Kafka quorum is also a WiP in KIP-250)

10.Tiered storage.
   What if you like to store messages forever (event-sourcing)?
   It can get expensive on main high-performance SSDs.
 BºWith Pulsar tiered storage you can automatically push old messages   º
 Bºinto practically infinite, cheap cloud storage (S3) and retrieve themº
 Bºjust like you do those newer, fresh-as-a-daisy messages.º
   (Kafka describes this feature in KIP-405).

11.End-to-end encryption
   Producer Java client can encrypt message using shared keys with
   the consumer. (hidden info to broker).
   Kafka is in feature-request state (KIP-317).

12.When an (stateless) Pulsar broker gets overloaded, pulsar rebalance
   request from clients automatically.
   It monitors the broker ussage of CPU, memory, and network (vs disk, since 
   brokers is stateless) to take the decision to balance.
   No need to add a new broker until all brokers are at full.
   In Kafka load-balancing is done by  installing another package 
   such as LinkedIn's Cruise Control or paying for Confluent's rebalancer
   tool.

</pre>

<pre zoom labels="qa,troubleshooting,TODO">
<span xsmall>Leasson Learned</span>
@[https://www.confluent.io/blog/stories-front-lessons-learned-supporting-apache-kafka/]
- Under-replicated Partitions Continue to Grow Inexplicably
- Kafka Liveness Check and Automation Causes Full Cluster Down
- Adding a New Broker Results in Terrible Performance
</pre>

<pre zoom labels="backups,troubleshooting,tool,TODO">
<span xsmall>Kafka Backup</span>
@[https://github.com/itadventurer/kafka-backup]
- Kafka Backup is a tool to back up and restore your Kafka data 
  including all (configurable) topic data and especially also consumer 
  group offsets. To the best of our knowledge, Kafka Backup is the only 
  viable solution to take a cold backup of your Kafka data and restore 
  it correctly.
</pre>

</div>
</body>
</html>
<!--
-->
